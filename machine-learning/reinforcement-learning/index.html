<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>强化学习 &amp; 模仿学习基础知识 - Rui&#039;s Blog</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Rui&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Rui&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta description="总是会看到强化学习及模仿学习的内容，每次看完都会忘记，此处把之前的小笔记总结一下。"><meta property="og:type" content="blog"><meta property="og:title" content="强化学习 &amp; 模仿学习基础知识"><meta property="og:url" content="https://superuier.github.io/machine-learning/reinforcement-learning/"><meta property="og:site_name" content="Rui&#039;s Blog"><meta property="og:description" content="总是会看到强化学习及模仿学习的内容，每次看完都会忘记，此处把之前的小笔记总结一下。"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://superuier.github.io/machine-learning/reinforcement-learning/MDP.png"><meta property="og:image" content="https://superuier.github.io/machine-learning/reinforcement-learning/taxonomy.png"><meta property="og:image" content="https://superuier.github.io/machine-learning/reinforcement-learning/TD_MC_DP_backups.png"><meta property="og:image" content="https://superuier.github.io/machine-learning/reinforcement-learning/a3c.png"><meta property="og:image" content="https://superuier.github.io/machine-learning/reinforcement-learning/IL_types_1.png"><meta property="og:image" content="https://superuier.github.io/machine-learning/reinforcement-learning/IL_types_2.png"><meta property="og:image" content="https://superuier.github.io/machine-learning/reinforcement-learning/BC.png"><meta property="og:image" content="https://superuier.github.io/machine-learning/reinforcement-learning/DPL_example.png"><meta property="og:image" content="https://superuier.github.io/machine-learning/reinforcement-learning/IL_sequential.png"><meta property="og:image" content="https://superuier.github.io/machine-learning/reinforcement-learning/RL_IRL.png"><meta property="og:image" content="https://superuier.github.io/machine-learning/reinforcement-learning/InverseRL.png"><meta property="article:published_time" content="2021-06-24T05:00:00.000Z"><meta property="article:modified_time" content="2021-06-24T05:00:00.000Z"><meta property="article:author" content="Superui"><meta property="article:tag" content="machine-learning"><meta property="article:tag" content="reinforcement-learning"><meta property="article:tag" content="imitation-learning"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/machine-learning/reinforcement-learning/MDP.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://superuier.github.io/machine-learning/reinforcement-learning/"},"headline":"Rui's Blog","image":["https://superuier.github.io/machine-learning/reinforcement-learning/MDP.png","https://superuier.github.io/machine-learning/reinforcement-learning/taxonomy.png","https://superuier.github.io/machine-learning/reinforcement-learning/TD_MC_DP_backups.png","https://superuier.github.io/machine-learning/reinforcement-learning/a3c.png","https://superuier.github.io/machine-learning/reinforcement-learning/IL_types_1.png","https://superuier.github.io/machine-learning/reinforcement-learning/IL_types_2.png","https://superuier.github.io/machine-learning/reinforcement-learning/BC.png","https://superuier.github.io/machine-learning/reinforcement-learning/DPL_example.png","https://superuier.github.io/machine-learning/reinforcement-learning/IL_sequential.png","https://superuier.github.io/machine-learning/reinforcement-learning/RL_IRL.png","https://superuier.github.io/machine-learning/reinforcement-learning/InverseRL.png"],"datePublished":"2021-06-24T05:00:00.000Z","dateModified":"2021-06-24T05:00:00.000Z","author":{"@type":"Person","name":"Superui"},"description":"总是会看到强化学习及模仿学习的内容，每次看完都会忘记，此处把之前的小笔记总结一下。"}</script><link rel="canonical" href="https://superuier.github.io/machine-learning/reinforcement-learning/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.2.0"><link rel="alternate" href="/atom.xml" title="Rui's Blog" type="application/atom+xml">
</head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/logo_superui1.svg" alt="Rui&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-6-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2021-06-24T05:00:00.000Z" title="2021-06-24T05:00:00.000Z">2021-06-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2021-06-24T05:00:00.000Z" title="2021-06-24T05:00:00.000Z">2021-06-24</time></span><span class="level-item"><a class="link-muted" href="/categories/Machine-Learning/">Machine Learning</a></span></div></div><h1 class="title is-3 is-size-4-mobile">强化学习 &amp; 模仿学习基础知识</h1><div class="content"><!-- omit in toc -->
<p>总是会看到强化学习及模仿学习的内容，每次看完都会忘记，此处把之前的小笔记总结一下。</p>
<a id="more"></a>
<h1 id="基本术语-Terminologies"><a href="#基本术语-Terminologies" class="headerlink" title="基本术语 Terminologies"></a>基本术语 Terminologies</h1><p>以下术语在强化学习和模仿学习中都经常见到。</p>
<ul>
<li>agent: the intelligent individual</li>
<li>environment: The agent is acting in an environment.  </li>
<li>state: Current condition. The agent can stay in one of many states of the environment</li>
<li>action: The agent chooses to take one of many actions under the certain states.</li>
<li>reward: Once an action is taken, the environment delivers a reward as feedback.</li>
<li>policy: Agents’ behavior.(s =&gt; a) The agent’s policy π provides the guideline on what is the optimal action to take in a certain state with the goal to maximize the total rewards.</li>
<li>value: (s =&gt; value) Each state is associated with a value function V(s) predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy. <ul>
<li>state-value of a state s is the expected return if we are in this state at time t.</li>
<li>action-value (“Q-value”; Q as “Quality” I believe?) of a state-action pair is expected return if we are in this state at time t and take action a.</li>
<li>A-value: The difference between action-value and state-value is the action advantage function (“A-value”):</li>
</ul>
</li>
<li>model: Transition and reward. (s,a =&gt; s’ &amp; r) How the environment reacts to certain actions (we may or may not know). </li>
</ul>
<h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><h2 id="1-马尔科夫决策过程"><a href="#1-马尔科夫决策过程" class="headerlink" title="1. 马尔科夫决策过程"></a>1. 马尔科夫决策过程</h2><p>In more formal terms, almost all the <strong>RL problems</strong> can be framed as <strong>Markov Decision Processes (MDPs)</strong>. 
All states in MDP has “Markov” property, referring to the fact that the future only depends on the current state, not the history.
The goal is to react on each state to <strong>maximize the total reward</strong>.</p>
<div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/MDP.png" class="" title="MDP"></div>

<p>如果所有 MDP 成分都已知，我们便可以较容易的训练出来一个 agent。
但是现实情况是很多时候，我们的 agent 对 transition function $P$ 和 reward function $R$ 一无所知，所有的信息都来自于同环境的交互。</p>
<h3 id="1-1-强化学习方法分类"><a href="#1-1-强化学习方法分类" class="headerlink" title="1.1. 强化学习方法分类"></a>1.1. 强化学习方法分类</h3><ol>
<li>以是否对环境建模分类:<ul>
<li>Doesn’t model the environment:<ul>
<li>Model-free RL: Doesn’t need to know the transition function (“model”), neither the real function nor a learned function.</li>
</ul>
</li>
<li>Model the environment:<ul>
<li>Model-based RL: Need to know the transition function (“model”), either the real function or a learned function.</li>
<li>Inverse reinforcement learning: Need to learn a value function for a state. (Imitation learning)</li>
</ul>
</li>
</ul>
</li>
<li>以行动策略和评估策略是否相同分类：<ul>
<li>On-policy: Use the deterministic outcomes or samples from the target policy to train the algorithm. 行动策略和评估策略相同</li>
<li>Off-policy: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy. 行动策略和评估策略不同</li>
</ul>
</li>
</ol>
<h3 id="1-2-对价值函数进行评估和分解"><a href="#1-2-对价值函数进行评估和分解" class="headerlink" title="1.2. 对价值函数进行评估和分解"></a>1.2. 对价值函数进行评估和分解</h3><p>强化学习的目标是可以使最终价值最大化，所以需要对其进行评估。</p>
<p>Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values.</p>
<script type="math/tex; mode=display">
\begin{aligned}
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \big) \\
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi} (s', a')
\end{aligned}</script><h2 id="2-Common-Approaches"><a href="#2-Common-Approaches" class="headerlink" title="2. Common Approaches"></a>2. Common Approaches</h2><div style="width:80%;margin:auto"><img src="/machine-learning/reinforcement-learning/taxonomy.png" class="" title="Taxonomy"></div>

<h3 id="2-1-Dynamic-Programming"><a href="#2-1-Dynamic-Programming" class="headerlink" title="2.1. Dynamic Programming"></a>2.1. Dynamic Programming</h3><p>When the model is fully known, following Bellman equations, we can use Dynamic Programming (DP) to iteratively evaluate value functions and improve policy. The policy would greedy based on the Q-value. Iteratively update the state value and the Q-value.</p>
<p>Generalized Policy Iteration (GPI) adaptive dynamic process</p>
<script type="math/tex; mode=display">
\pi_0 \xrightarrow[]{\text{evaluation}} V_{\pi_0} \xrightarrow[]{\text{improve}}
\pi_1 \xrightarrow[]{\text{evaluation}} V_{\pi_1} \xrightarrow[]{\text{improve}}
\pi_2 \xrightarrow[]{\text{evaluation}} \dots \xrightarrow[]{\text{improve}}
\pi_* \xrightarrow[]{\text{evaluation}} V_*</script><h3 id="2-2-Monte-Carlo-Methods"><a href="#2-2-Monte-Carlo-Methods" class="headerlink" title="2.2. Monte-Carlo Methods"></a>2.2. Monte-Carlo Methods</h3><p>Model-free method.
It learns from episodes of raw experience without modeling the environmental dynamics and computes the observed mean return as an approximation of the expected return</p>
<h3 id="2-3-Temporal-Difference-Learning"><a href="#2-3-Temporal-Difference-Learning" class="headerlink" title="2.3. Temporal-Difference Learning"></a>2.3. Temporal-Difference Learning</h3><p>TD Learning is model-free and learns from episodes of experience. 
However, TD learning can learn from <strong>incomplete</strong> episodes and hence we don’t need to track the episode up to termination.</p>
<p>主要思想是将效用估计朝着理想均衡方向调整:</p>
<ul>
<li>TD调整一个状态与已观察到的后继状态相一致</li>
<li>ADP调整一个状态与可能出现的的后继状态相一致</li>
<li>TD可视为对ADP的一个粗略有效的一阶近似</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
V(S_t) &\leftarrow (1- \alpha) V(S_t) + \alpha G_t \\
V(S_t) &\leftarrow V(S_t) + \alpha (G_t - V(S_t)) \\
V(S_t) &\leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t)) \\
Q(S_t, A_t) &\leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))
\end{aligned}</script><p>To learn optimal policy:</p>
<ul>
<li>SARSA: On-Policy TD control<ul>
<li>“SARSA” refers to the procedure for updating the Q-value.</li>
<li>Same routine of GPI.</li>
<li>In each step of SARSA, we need to choose the next action according to the current policy.</li>
</ul>
</li>
<li>Q-Learning: Off-policy TD control<ul>
<li>The key difference from SARSA is that Q-learning does not follow the current policy to pick the second action (off-policy).</li>
<li>Q-learning may suffer from instability and divergence when combined with an nonlinear Q-value function approximation</li>
</ul>
</li>
<li>Deep Q-Network<ul>
<li>It quickly becomes computationally infeasible to memorize Q-table when the state and action space are large. </li>
<li>Use functions (i.e. a machine learning model) to approximate Q values and this is called function approximation.</li>
<li>greatly improve and stabilize the training procedure of Q-learning by two innovative mechanisms:<ul>
<li>Experience Replay: improves data efficiency, removes correlations in the observation sequences, and smooths over changes in the data distribution.</li>
<li>Periodically Updated Target: only periodically updated, overcomes the short-term oscillations</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-4-Combining-TD-and-MC-Learning"><a href="#2-4-Combining-TD-and-MC-Learning" class="headerlink" title="2.4. Combining TD and MC Learning"></a>2.4. Combining TD and MC Learning</h3><p>In TD learning, we only trace one step further down the action chain when calculating the TD target. One can easily extend it to take multiple steps to estimate the return.</p>
<div style="width:90%;margin:auto"><img src="/machine-learning/reinforcement-learning/TD_MC_DP_backups.png" class=""></div>

<h3 id="2-5-Policy-Gradient"><a href="#2-5-Policy-Gradient" class="headerlink" title="2.5. Policy Gradient"></a>2.5. Policy Gradient</h3><p>All the methods we have introduced above aim to learn the state/action value function and then to select actions accordingly. Policy Gradient methods instead learn the policy directly with a parameterized function.</p>
<ul>
<li>Measure the quality of a policy with the policy score function.</li>
<li>Use policy gradient ascent to find the best parameter that improves the policy.</li>
</ul>
<h3 id="2-6-Asynchronous-Advantage-Actor-Critic-A3C"><a href="#2-6-Asynchronous-Advantage-Actor-Critic-A3C" class="headerlink" title="2.6. Asynchronous Advantage Actor-Critic (A3C)"></a>2.6. Asynchronous Advantage Actor-Critic (A3C)</h3><ul>
<li>Asynchronous: Several agents are trained in it’s own copy of the environment and the model form these agent’s are gathered in a master agent. The reason behind this idea, is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.</li>
<li>Advantage: Similarly to PG where the update rule used the dicounted returns from a set of experiences in order to tell the agent which actions were “good” or “bad”.</li>
<li>Actor-critic: combines the benefits of both approaches from policy-iteration method as PG and value-iteration method as Q-learning (See below). The network will estimate both a value function V(s) (how good a certain state is to be in) and a policy π(s). Agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.</li>
</ul>
<div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/a3c.png" class=""></div>

<h2 id="3-Known-Problems"><a href="#3-Known-Problems" class="headerlink" title="3. Known Problems"></a>3. Known Problems</h2><ul>
<li>Exploration-Exploitation Dilemma</li>
<li>Deadly Triad Issue: off-policy, nonlinear function approximation, and bootstrapping are combined in one RL algorithm, the training could be unstable and hard to converge.</li>
</ul>
<h1 id="模仿学习"><a href="#模仿学习" class="headerlink" title="模仿学习"></a>模仿学习</h1><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>Background:</p>
<ul>
<li>Given: demonstrations or demonstrator </li>
<li>Goal: train a policy to mimic demonstrations</li>
</ul>
<p>Intuition: </p>
<ul>
<li>人们并不总是知道执行某项任务所获得的报酬</li>
<li>但是人们可能会知道“做什么是正确的事情（最佳策略）</li>
</ul>
<p>Rollout: sequentially execute $\pi(s_0)$ on an initial state</p>
<ul>
<li>Produce trajectory $\mathcal{T}=(s_0,a_0,s_1,a_1,…)$</li>
</ul>
<h2 id="2-模仿学习分类"><a href="#2-模仿学习分类" class="headerlink" title="2. 模仿学习分类"></a>2. 模仿学习分类</h2><ul>
<li>Behavior cloning </li>
<li>Direct policy learning (multiple step BC)</li>
<li>Inverse reinforcement learning (assume learning R is statistically easier)</li>
</ul>
<div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/IL_types_1.png" class="" title="Imitation learning types 1"></div>
<div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/IL_types_2.png" class="" title="Imitation learning types 2"></div>

<h3 id="2-1-Behavioral-Cloning-simplest-Imitation-Learning-setting"><a href="#2-1-Behavioral-Cloning-simplest-Imitation-Learning-setting" class="headerlink" title="2.1. Behavioral Cloning (simplest Imitation Learning setting)"></a>2.1. Behavioral Cloning (simplest Imitation Learning setting)</h3><p>Treat experts’ states-actions pairs i.i.d and as training example use supervised learning (from state to action).
Distribution provided exogenously.</p>
<p>When to use BC?</p>
<div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/BC.png" class="" title="BC"></div>

<h3 id="2-2-Direct-Policy-Learning"><a href="#2-2-Direct-Policy-Learning" class="headerlink" title="2.2. Direct Policy Learning"></a>2.2. Direct Policy Learning</h3><p>Learning reduction: Reduce “harder” learning problem to “easier” one</p>
<p>Idea:</p>
<ul>
<li>Construct a sequence of distributions or sequence of supervised learning problems.</li>
<li>Query interactive oracle about the state and construct a loss function according to our action and expert action on this state.</li>
</ul>
<div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/DPL_example.png" class="" title="DPL_example"></div>
<div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/IL_sequential.png" class="" title="Sequential process"></div>

<h3 id="2-3-Inverse-reinforcement-learning"><a href="#2-3-Inverse-reinforcement-learning" class="headerlink" title="2.3. Inverse reinforcement learning"></a>2.3. Inverse reinforcement learning</h3><p>Inverse RL指我们需要对环境的reward进行建模。
RL与IRL的对比如下图所示：</p>
<div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/RL_IRL.png" class="" title="RL-IRL"></div>

<p>In a traditional RL setting, the goal is to learn a decision process to produce behavior that maximizes some predefined reward function. 
Inverse reinforcement learning (IRL), flips the problem and instead attempts to extract the reward function from the observed behavior of an agent.
IRL seeks the reward functions that ‘explains’ the demonstrations.</p>
<p>此时同样存在是否依赖 transition function 的情况</p>
<div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/InverseRL.png" class="" title="InverseRL"></div>

<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Reinforcement_learning">https://en.wikipedia.org/wiki/Reinforcement_learning</a></li>
<li><a target="_blank" rel="noopener" href="https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc">https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc</a></li>
<li><a target="_blank" rel="noopener" href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html</a></li>
<li>人工智能：一种现代的方法</li>
<li><a target="_blank" rel="noopener" href="https://sites.google.com/view/icml2018-imitation-learning/">https://sites.google.com/view/icml2018-imitation-learning/</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>强化学习 &amp; 模仿学习基础知识</p><p><a href="https://superuier.github.io/machine-learning/reinforcement-learning/">https://superuier.github.io/machine-learning/reinforcement-learning/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Superui</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2021-06-24</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2021-06-24</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/machine-learning/">machine-learning</a><a class="link-muted mr-2" rel="tag" href="/tags/reinforcement-learning/">reinforcement-learning</a><a class="link-muted mr-2" rel="tag" href="/tags/imitation-learning/">imitation-learning</a></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/machine-learning/active-reinforcement/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">主动学习与交互决策过程</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/economy-finance/didongsheng/zjqd/"><span class="level-item">翟东升政经启翟系列视频</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/avatar_rui_2.jpg" alt="Rui"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Rui</p><p class="is-size-6 is-block">PhD Student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shenzhen, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">50</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">40</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/SupeRuier" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/SupeRuier"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/atom.xml"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/Artificial-Intelligence/"><span class="level-start"><span class="level-item">Artificial Intelligence</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-science-engineering/"><span class="level-start"><span class="level-item">Computer Science and Engineering</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/economy-finance/"><span class="level-start"><span class="level-item">Economy &amp; Finance</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/Interesting-Stuff/"><span class="level-start"><span class="level-item">Interesting Stuff</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/knowledge-from-growth/"><span class="level-start"><span class="level-item">Knowledge from Growth</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/Machine-Learning/"><span class="level-start"><span class="level-item">Machine Learning</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li><li><a class="level is-mobile" href="/categories/paper-reading/"><span class="level-start"><span class="level-item">Paper Reading</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/programming/"><span class="level-start"><span class="level-item">Programming</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/reading-note/"><span class="level-start"><span class="level-item">Reading Note</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/research/"><span class="level-start"><span class="level-item">Research</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/software-tools/"><span class="level-start"><span class="level-item">Software Tools</span></span><span class="level-end"><span class="level-item tag">7</span></span></a></li></ul></div></div></div><div class="column-right-shadow is-hidden-widescreen is-sticky"></div></div><div class="column column-right is-4-tablet is-4-desktop is-3-widescreen is-hidden-touch is-hidden-desktop-only order-3 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#基本术语-Terminologies"><span class="level-left"><span class="level-item">基本术语 Terminologies</span></span></a></li><li><a class="level is-mobile" href="#强化学习"><span class="level-left"><span class="level-item">强化学习</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-马尔科夫决策过程"><span class="level-left"><span class="level-item">1. 马尔科夫决策过程</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-1-强化学习方法分类"><span class="level-left"><span class="level-item">1.1. 强化学习方法分类</span></span></a></li><li><a class="level is-mobile" href="#1-2-对价值函数进行评估和分解"><span class="level-left"><span class="level-item">1.2. 对价值函数进行评估和分解</span></span></a></li></ul></li><li><a class="level is-mobile" href="#2-Common-Approaches"><span class="level-left"><span class="level-item">2. Common Approaches</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-1-Dynamic-Programming"><span class="level-left"><span class="level-item">2.1. Dynamic Programming</span></span></a></li><li><a class="level is-mobile" href="#2-2-Monte-Carlo-Methods"><span class="level-left"><span class="level-item">2.2. Monte-Carlo Methods</span></span></a></li><li><a class="level is-mobile" href="#2-3-Temporal-Difference-Learning"><span class="level-left"><span class="level-item">2.3. Temporal-Difference Learning</span></span></a></li><li><a class="level is-mobile" href="#2-4-Combining-TD-and-MC-Learning"><span class="level-left"><span class="level-item">2.4. Combining TD and MC Learning</span></span></a></li><li><a class="level is-mobile" href="#2-5-Policy-Gradient"><span class="level-left"><span class="level-item">2.5. Policy Gradient</span></span></a></li><li><a class="level is-mobile" href="#2-6-Asynchronous-Advantage-Actor-Critic-A3C"><span class="level-left"><span class="level-item">2.6. Asynchronous Advantage Actor-Critic (A3C)</span></span></a></li></ul></li><li><a class="level is-mobile" href="#3-Known-Problems"><span class="level-left"><span class="level-item">3. Known Problems</span></span></a></li></ul></li><li><a class="level is-mobile" href="#模仿学习"><span class="level-left"><span class="level-item">模仿学习</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#1-背景"><span class="level-left"><span class="level-item">1. 背景</span></span></a></li><li><a class="level is-mobile" href="#2-模仿学习分类"><span class="level-left"><span class="level-item">2. 模仿学习分类</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#2-1-Behavioral-Cloning-simplest-Imitation-Learning-setting"><span class="level-left"><span class="level-item">2.1. Behavioral Cloning (simplest Imitation Learning setting)</span></span></a></li><li><a class="level is-mobile" href="#2-2-Direct-Policy-Learning"><span class="level-left"><span class="level-item">2.2. Direct Policy Learning</span></span></a></li><li><a class="level is-mobile" href="#2-3-Inverse-reinforcement-learning"><span class="level-left"><span class="level-item">2.3. Inverse reinforcement learning</span></span></a></li></ul></li></ul></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/AI/"><span class="tag">AI</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Conda/"><span class="tag">Conda</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cuda/"><span class="tag">Cuda</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Docker/"><span class="tag">Docker</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/EA/"><span class="tag">EA</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hackintosh/"><span class="tag">Hackintosh</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Icarus/"><span class="tag">Icarus</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/MacOS/"><span class="tag">MacOS</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Master-Ma/"><span class="tag">Master Ma</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Matplotlib/"><span class="tag">Matplotlib</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Python/"><span class="tag">Python</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Research/"><span class="tag">Research</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Vim/"><span class="tag">Vim</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/active-learning/"><span class="tag">active-learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algorithms/"><span class="tag">algorithms</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cloud/"><span class="tag">cloud</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/contrastive-learning/"><span class="tag">contrastive-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/cs-seminar/"><span class="tag">cs-seminar</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/few-shot-learning/"><span class="tag">few-shot-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/gaussian-process/"><span class="tag">gaussian-process</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/imitation-learning/"><span class="tag">imitation-learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/interesting/"><span class="tag">interesting</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/latex/"><span class="tag">latex</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/loss/"><span class="tag">loss</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine-learning</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/neural-network/"><span class="tag">neural-network</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/nlp/"><span class="tag">nlp</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/numpy/"><span class="tag">numpy</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement-learning</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/search/"><span class="tag">search</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/self-supervised-learning/"><span class="tag">self-supervised-learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/sorting/"><span class="tag">sorting</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/survey/"><span class="tag">survey</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%84%9F%E6%82%9F%E6%80%BB%E7%BB%93/"><span class="tag">感悟总结</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BF%9F%E4%B8%9C%E5%8D%87/"><span class="tag">翟东升</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AF%BB%E4%B9%A6/"><span class="tag">读书</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%BD%AC%E8%BD%BD/"><span class="tag">转载</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/logo_superui1.svg" alt="Rui&#039;s Blog" height="28"></a><p class="is-size-7"><span>&copy; 2021 Superui</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" async></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>