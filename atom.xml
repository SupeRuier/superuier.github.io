<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rui&#39;s Blog</title>
  
  
  <link href="https://superuier.github.io/atom.xml" rel="self"/>
  
  <link href="https://superuier.github.io/"/>
  <updated>2021-09-29T05:00:00.000Z</updated>
  <id>https://superuier.github.io/</id>
  
  <author>
    <name>Superui</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>人工智能的中的搜索</title>
    <link href="https://superuier.github.io/artificial-intelligence/ai-search/"/>
    <id>https://superuier.github.io/artificial-intelligence/ai-search/</id>
    <published>2021-09-27T08:00:00.000Z</published>
    <updated>2021-09-29T05:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>人工智能中关于搜索的一些最基础的知识。</p><a id="more"></a><h2 id="AI-中的搜索"><a href="#AI-中的搜索" class="headerlink" title="AI 中的搜索"></a>AI 中的搜索</h2><p>对于一个 goal-based agent，搜索是使其找到一个动作或者一系列动作来达到目标。有很多例子，比如走迷宫，寻路问题，8-queen 问题等。一般来说包含树搜索和图搜索等。</p><h2 id="对于考虑路径的搜索而言"><a href="#对于考虑路径的搜索而言" class="headerlink" title="对于考虑路径的搜索而言"></a>对于考虑路径的搜索而言</h2><p>评估策略需考虑以下四个维度：</p><ul><li>Completeness: Does it always find a solution if it exists?</li><li>Time complexity: # nodes generated/expanded.</li><li>Space complexity: maximum # nodes in memory.</li><li>Optimality: Does it always find the least-cost solution?</li></ul><p>存在两类搜索策略：</p><ul><li>Uniformed<ul><li>Breadth-first search (BFS): Expand shallowest node</li><li>Depth-first search (DFS): Expand deepest node</li><li>Depth-limited search (DLS): Depth first with depth limit</li><li>Iterative-deepening search (IDS): DLS with increasing limit</li><li>Uniform-cost search (UCS): Expand least cost node (the cost could be the length between nodes)</li></ul></li><li>Informed<ul><li>Greedy best-first search: Expand the node that appears to be closest to goal</li><li>A* search: Minimize the total estimated solution cost (to middle node + node to goal；f=g+h). BFS mode.</li><li>IDA<em>: IDS + A</em>. DLS mode. The cost of space is lower than A*.</li></ul></li></ul><p>对于 A* 中启发式策略而言（h）：</p><ul><li>A good heuristic must be admissible.</li><li>An admissible heuristic never overestimates the cost to reach the goal, that is it is optimistic</li><li>For admissible $h_1$ and $h_2$, if $h_1$(s) ≥ $h_2$(s) for ∀𝑠 ⇒ $h_1$ dominates $h_2$ and is more efficient for search.</li></ul><p>UCS vs Greedy Best First vs A*:</p><ul><li>UCS：f(n) = g(n) </li><li>Greedy Best First: f(n) = h(n) </li><li>A*: f(n)=g(n)+h(n)</li></ul><h2 id="对于不考虑路径的搜索"><a href="#对于不考虑路径的搜索" class="headerlink" title="对于不考虑路径的搜索"></a>对于不考虑路径的搜索</h2><p>Local search: the path doesn’t matter</p><ul><li>Hill climbing</li><li>Genetic algorithms</li><li>Simulated Annealing: Given a chance to jump out the local minimum</li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;人工智能中关于搜索的一些最基础的知识。&lt;/p&gt;</summary>
    
    
    
    <category term="Artificial Intelligence" scheme="https://superuier.github.io/categories/Artificial-Intelligence/"/>
    
    
    <category term="AI" scheme="https://superuier.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib 学习笔记</title>
    <link href="https://superuier.github.io/programming/matplotlib/"/>
    <id>https://superuier.github.io/programming/matplotlib/</id>
    <published>2021-09-24T07:00:03.000Z</published>
    <updated>2021-09-24T07:00:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>这里是个人的 Matplotlib 学习笔记。记录一些作图的过程。</p><a id="more"></a><h1 id="用法"><a href="#用法" class="headerlink" title="用法"></a>用法</h1><h2 id="减小空白边界"><a href="#减小空白边界" class="headerlink" title="减小空白边界"></a>减小空白边界</h2><p>由于使用 Latex 排版时需要严格控制图片位置、大小、排布方式，所以任何冗余的空间都是需要极力避免的。在做图的时候可以直接将图片空白边界抹去。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.savefig(<span class="string">&#x27;test.png&#x27;</span>, bbox_inches=<span class="string">&#x27;tight&#x27;</span>)</span><br></pre></td></tr></table></figure><p>但是这个方法有一个问题就是多个图片的留白可能不一致，导致如果多子图放置于 latex 文档中会出现排版无法对齐的情况。此时需要手动对边界进行设置。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.subplots_adjust(left=<span class="number">0.12</span>, right=<span class="number">0.97</span>, top=<span class="number">0.98</span>, bottom=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;这里是个人的 Matplotlib 学习笔记。
记录一些作图的过程。&lt;/p&gt;</summary>
    
    
    
    <category term="Programming" scheme="https://superuier.github.io/categories/programming/"/>
    
    
    <category term="Python" scheme="https://superuier.github.io/tags/Python/"/>
    
    <category term="Matplotlib" scheme="https://superuier.github.io/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>人工智能的基本概念</title>
    <link href="https://superuier.github.io/artificial-intelligence/ai-basics/"/>
    <id>https://superuier.github.io/artificial-intelligence/ai-basics/</id>
    <published>2021-09-07T09:00:00.000Z</published>
    <updated>2021-09-07T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>人工智能基础知识和一些概念。</p><a id="more"></a><h2 id="AI-是什么？"><a href="#AI-是什么？" class="headerlink" title="AI 是什么？"></a>AI 是什么？</h2><p>其实很多地方都给出了不同角度的定义，此处仅记录自己的看法。</p><blockquote><p>AI 是承载于机器或程序，人为设计或训练，并可以解决特定或一些列特定问题的能力。</p></blockquote><p>其涵盖多个计算机领域：机器学习/优化/搜索/etc.</p><h2 id="智能体"><a href="#智能体" class="headerlink" title="智能体"></a>智能体</h2><p>AI 承载于智能体（Agent），即我的定义中的机器或程序。</p><blockquote><p>Agent = Architecture + Program</p></blockquote><p>在解决问题的过程中，智能体通过传感器（Sensors）接受外界环境（Environment）的信号，并自我处理信息，之后作出相应行动（Action）。于是在对一个智能体进行定义时，通常要考虑 <strong>PEAS</strong>：</p><ul><li>Performance</li><li>Environment</li><li>Actuators</li><li>Senors</li></ul><h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><p>环境有很多重要的类别：</p><ul><li>Fully observable (vs. partially observable)</li><li>Deterministic (vs. stochastic)</li><li>Episodic (vs. sequential)</li><li>Static (vs. dynamic)</li><li>Discrete (vs. continuous)</li><li>Single agent (vs. multi-agent)</li><li>Known (vs. Unknown)</li></ul><p>下面是一些具体的例子。</p><div class="table-container"><table><thead><tr><th>Environment</th><th>Observable</th><th>Agents</th><th>Deterministic</th><th>Static</th><th>Discrete</th></tr></thead><tbody><tr><td>8-puzzle</td><td>Fully</td><td>Single</td><td>Deterministic</td><td>Static</td><td>Discrete</td></tr><tr><td>Chess</td><td>Fully</td><td>Multi</td><td>Deterministic</td><td>(Semi)Static</td><td>Discrete</td></tr><tr><td>Poker</td><td>Partially</td><td>Multi</td><td>Stochastic</td><td>Static</td><td>Discrete</td></tr><tr><td>Backgammon</td><td>Fully</td><td>Multi</td><td>Stochastic</td><td>Static</td><td>Discrete</td></tr><tr><td>Car</td><td>Partially</td><td>Multi</td><td>Stochastic</td><td>Dynamic</td><td>Continuous</td></tr><tr><td>Cleaner</td><td>partially</td><td>Single</td><td>Stochastic</td><td>Dynamic</td><td>Continuous</td></tr></tbody></table></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;人工智能基础知识和一些概念。&lt;/p&gt;</summary>
    
    
    
    <category term="Artificial Intelligence" scheme="https://superuier.github.io/categories/Artificial-Intelligence/"/>
    
    
    <category term="AI" scheme="https://superuier.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>有趣的事物</title>
    <link href="https://superuier.github.io/interesting-stuff/interesting-stuff/"/>
    <id>https://superuier.github.io/interesting-stuff/interesting-stuff/</id>
    <published>2021-09-07T06:50:00.000Z</published>
    <updated>2021-09-07T06:50:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>一些有趣的东西。</p><a id="more"></a><ul><li><a href="https://csunplugged.org/en/">Computer Science without a computer</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;一些有趣的东西。&lt;/p&gt;</summary>
    
    
    
    <category term="Interesting Stuff" scheme="https://superuier.github.io/categories/Interesting-Stuff/"/>
    
    
    <category term="interesting" scheme="https://superuier.github.io/tags/interesting/"/>
    
  </entry>
  
  <entry>
    <title>Batch Normalization</title>
    <link href="https://superuier.github.io/machine-learning/batch-normalization/"/>
    <id>https://superuier.github.io/machine-learning/batch-normalization/</id>
    <published>2021-09-03T07:00:00.000Z</published>
    <updated>2021-09-03T07:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>此处记录对于 Batch Normalization 的学习。</p><a id="more"></a><h2 id="神经网络中存在的-ICS-问题"><a href="#神经网络中存在的-ICS-问题" class="headerlink" title="神经网络中存在的 ICS 问题"></a>神经网络中存在的 ICS 问题</h2><p>深度学习中存在 Internal Covariate Shift (ICS) 的问题。类比迁移学习中的 Covariate Shift 指源领域和目标领域数据 marginal distribution 的偏移。这里 ICS 指神经网络中，由于参数的变化，引起的每一层输出分布的差异变化，换句话说之后下一层的输入在参数变化后，可能基于了另一个分布。所以下一层在训练过程中就需要不断的去适应这种变化。</p><p>于是带来了以下问题：</p><ul><li>后面一层的参数需要适应不断变化的分布，训练效率降低。</li><li>对于饱和非线性激活函数，例如 sigmoid 和 tanh，容易落入饱和区。饱和区是指由于输入值极度偏离0点，导致梯度计算接近于0，从而难以起到学习效果。</li></ul><h2 id="如何解决-ICS-问题"><a href="#如何解决-ICS-问题" class="headerlink" title="如何解决 ICS 问题"></a>如何解决 ICS 问题</h2><p>此类由于分布变化带来不良影响的问题，最 naive 的解决方式一般来说都是对分布进行限制。就比如在迁移学习中，会将源域和目标域的样本来做一个统一（减小分布差异）。在 ICS 问题中，也可以对分布进行限制。</p><p>最初，白化（whitening）被提出，一般来说采用 <a href="http://ufldl.stanford.edu/tutorial/unsupervised/PCAWhitening/">PCA</a> 或者 ZCA 的方法，使所有特征分布均值为0，方差为1（PCA）或相同（ZCA）。白话的目的是去除数据特征之间相关性（独立），同时使其具有相同均值和方差（同分布）。这样每一层网络的输入分布被固定，加速网络收敛。</p><p>但是白化也存在一些问题：</p><ul><li>计算成本高</li><li>改变数据表达能力，一些参数信息会被丢失</li><li>不可微，难以通过反向传播训练</li></ul><p>所以说我们期望有一种计算代价低廉，且能使标准化的数据尽可能保有表达能力的方法。这就是 Batch Normalization 提出的背景。</p><h2 id="什么是-Batch-Normalization"><a href="#什么是-Batch-Normalization" class="headerlink" title="什么是 Batch Normalization"></a>什么是 Batch Normalization</h2><p>主要思路：</p><ul><li>既然白化计算过程比较复杂，那我们就放松限制一点，尝试只单独对每个特征进行标准化，使其均值为0，方差为1。</li><li>既然类白化操作减弱了网络中每一层输入数据表达能力，那再加入线性变换操作，让这些数据再能够尽可能恢复本身的表达能力，使其不因规范化而下降。</li></ul><p>通用变换框架如下所示，包含两次平移和伸缩变换，在使用 BN 之后，每层神经元输入的样本的均值仅由 $\boldsymbol{b}$ 决定，而不像之前由前面一层神经网络复杂的参数决定：</p><script type="math/tex; mode=display">h=f\left(\boldsymbol{g}\cdot\frac{\boldsymbol{x}-\boldsymbol{\mu}}{\boldsymbol{\sigma}}+\boldsymbol{b}\right)\\</script><p>但是好像这个第二次的仿射变换在理论上是否有用，需不需要用还有争议，日后可以再仔细看一下。（挖坑）</p><p>同时 BN 有一些变体，在这里不展开了：</p><ul><li>纵向规范化（最基础）</li><li>横向规范化</li><li>参数规范化</li><li>余弦规范化</li></ul><h2 id="如何使用-Batch-Normalization"><a href="#如何使用-Batch-Normalization" class="headerlink" title="如何使用 Batch Normalization"></a>如何使用 Batch Normalization</h2><p>适用场景：</p><ul><li>每个 mini-batch 比较大，数据分布比较接近。</li><li>在进行训练之前，要做好充分的 shuffle， 否则效果会差很多。</li><li>因此不适用于 动态的网络结构 和 RNN 网络</li></ul><p>测试阶段：</p><ul><li>保留训练时每一个 batch 的统计量 $\mu_{batch}$ 和 $\sigma^2_{batch}$。</li><li>使用整个样本的统计量来对Test数据进行归一化，具体来说使用均值与方差的无偏估计。<ul><li>$\mu_{test}=\mathbb{E} (\mu_{batch})$</li><li>$\sigma^2_{test}=\frac{m}{m-1}\mathbb{E}(\sigma^2_{batch})$</li></ul></li></ul><p>构建阶段：</p><ul><li>置于 Conv 层或全连接层之后</li><li>对于饱和非线性激活函数而言，BN 层需要放到 activation 之前。Dropout 则应当置于 activation layer 之后.</li><li>对于 ReLU 而言，目前并没有定论，不管是实验还是理论争论都比较多，目前看来 BN 放在 ReLU 之后可能表现更好，但是放在 ReLU 前的可能更多一些（BN 原论文是放在了前面）。（<strong>大误</strong>）</li></ul><h2 id="在-Pytorch-中的-Batch-Normalization"><a href="#在-Pytorch-中的-Batch-Normalization" class="headerlink" title="在 Pytorch 中的 Batch Normalization"></a>在 Pytorch 中的 Batch Normalization</h2><p>在 Pytorch 的实现中，BN 也包含两次平移和伸缩变换，其中第二次变换可以通过调整仿射变换参数 <code>affine=True</code> 选择是否打开。这里可能也体现了这个仿射变换是否有必要的争议。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://zhuanlan.zhihu.com/p/33173246">详解深度学习中的Normalization，BN/LN/WN - Juliuszh的文章 - 知乎</a></li><li><a href="https://zhuanlan.zhihu.com/p/34879333">Batch Normalization原理与实战 - 天雨粟的文章 - 知乎</a></li><li><a href="https://www.zhihu.com/question/318354788/answer/640006790">Batch Normalization 和激活函数的使用顺序是什么，神经元的饱和指的又是什么？ - 无双谱的回答 - 知乎</a></li><li><a href="https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html">https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;此处记录对于 Batch Normalization 的学习。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://superuier.github.io/categories/Machine-Learning/"/>
    
    
    <category term="neural-network" scheme="https://superuier.github.io/tags/neural-network/"/>
    
    <category term="machine-learning" scheme="https://superuier.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>Latex 图片排版记录</title>
    <link href="https://superuier.github.io/programming/latex/"/>
    <id>https://superuier.github.io/programming/latex/</id>
    <published>2021-08-30T03:00:00.000Z</published>
    <updated>2021-09-24T08:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>此处记录一些 Latex 的用法。</p><a id="more"></a><h2 id="图片排版"><a href="#图片排版" class="headerlink" title="图片排版"></a>图片排版</h2><h3 id="子图排版"><a href="#子图排版" class="headerlink" title="子图排版"></a>子图排版</h3><p>涉及子图的图片排版可以参考<a href="https://blog.csdn.net/a6822342/article/details/80533135">此链接</a>。在此就不再赘述。</p><h3 id="将图片放置于表格中"><a href="#将图片放置于表格中" class="headerlink" title="将图片放置于表格中"></a>将图片放置于表格中</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;table&#125;</span><br><span class="line">   <span class="keyword">\centering</span></span><br><span class="line">   <span class="keyword">\caption</span>&#123;The table of figures&#125;</span><br><span class="line">   <span class="keyword">\hspace</span>*&#123;-0.15<span class="keyword">\linewidth</span>&#125;</span><br><span class="line">   <span class="keyword">\begin</span>&#123;tabular&#125;&#123;cM&#123;0.25<span class="keyword">\linewidth</span>&#125;M&#123;0.25<span class="keyword">\linewidth</span>&#125;M&#123;0.25<span class="keyword">\linewidth</span>&#125;M&#123;0.25<span class="keyword">\linewidth</span>&#125;&#125;</span><br><span class="line">      <span class="keyword">\toprule</span></span><br><span class="line">        <span class="built_in">&amp;</span> A                                              <span class="built_in">&amp;</span> B                                              <span class="built_in">&amp;</span> C                                              <span class="built_in">&amp;</span> D                                              <span class="keyword">\\</span></span><br><span class="line">      <span class="keyword">\midrule</span></span><br><span class="line">      K <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="keyword">\\</span></span><br><span class="line">      L <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="keyword">\\</span></span><br><span class="line">      M <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="keyword">\\</span></span><br><span class="line">      N <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="built_in">&amp;</span> <span class="keyword">\includegraphics</span>[width=<span class="keyword">\linewidth</span>]&#123;figure.png&#125; <span class="keyword">\\</span></span><br><span class="line">      <span class="keyword">\bottomrule</span></span><br><span class="line">   <span class="keyword">\end</span>&#123;tabular&#125;</span><br><span class="line">   <span class="keyword">\label</span>&#123;table:table-of-figures&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;table&#125;</span><br></pre></td></tr></table></figure><h3 id="子图超出行宽强制不换行"><a href="#子图超出行宽强制不换行" class="headerlink" title="子图超出行宽强制不换行"></a>子图超出行宽强制不换行</h3><p>这个需求是因为在写文章时有时模版会在子图间加距离，导致按比例设置大小的子图超出行距换行。另一种情况就是有时我们需要稍外超出一些行距来使图片更大一些。一般来说这里需要使用到 <code>\makebox</code> 命令。</p><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\begin</span>&#123;figure&#125;</span><br><span class="line">   <span class="keyword">\centering</span></span><br><span class="line">   <span class="keyword">\makebox</span>[<span class="keyword">\textwidth</span>][c]&#123;</span><br><span class="line">   <span class="keyword">\subfigure</span>[Caption A]&#123;</span><br><span class="line">      <span class="keyword">\begin</span>&#123;minipage&#125;[b]&#123;0.3<span class="keyword">\linewidth</span>&#125;</span><br><span class="line">         <span class="keyword">\includegraphics</span>[width=1<span class="keyword">\textwidth</span>]&#123;figure/digit<span class="built_in">_</span>best.png&#125;</span><br><span class="line">      <span class="keyword">\end</span>&#123;minipage&#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">\hspace</span>&#123;-0.05<span class="keyword">\linewidth</span>&#125;</span><br><span class="line">   <span class="keyword">\subfigure</span>[Caption B]&#123;</span><br><span class="line">      <span class="keyword">\begin</span>&#123;minipage&#125;[b]&#123;0.3<span class="keyword">\linewidth</span>&#125;</span><br><span class="line">         <span class="keyword">\includegraphics</span>[width=1<span class="keyword">\textwidth</span>]&#123;figure/amazon<span class="built_in">_</span>best.png&#125;</span><br><span class="line">      <span class="keyword">\end</span>&#123;minipage&#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">\hspace</span>&#123;-0.05<span class="keyword">\linewidth</span>&#125;</span><br><span class="line">   <span class="keyword">\subfigure</span>[Caption C]&#123;</span><br><span class="line">      <span class="keyword">\begin</span>&#123;minipage&#125;[b]&#123;0.3<span class="keyword">\linewidth</span>&#125;</span><br><span class="line">         <span class="keyword">\includegraphics</span>[width=1<span class="keyword">\textwidth</span>]&#123;figure/office<span class="built_in">_</span>best.png&#125;</span><br><span class="line">      <span class="keyword">\end</span>&#123;minipage&#125;</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">\hspace</span>&#123;-0.05<span class="keyword">\linewidth</span>&#125;</span><br><span class="line">   <span class="keyword">\subfigure</span>[Caption D]&#123;</span><br><span class="line">      <span class="keyword">\begin</span>&#123;minipage&#125;[b]&#123;0.3<span class="keyword">\linewidth</span>&#125;</span><br><span class="line">         <span class="keyword">\includegraphics</span>[width=1<span class="keyword">\textwidth</span>]&#123;figure/imageCLEF<span class="built_in">_</span>best.png&#125;</span><br><span class="line">      <span class="keyword">\end</span>&#123;minipage&#125;</span><br><span class="line">   &#125;&#125;</span><br><span class="line">   <span class="keyword">\caption</span>&#123;A box of figures.&#125;</span><br><span class="line">   <span class="keyword">\label</span>&#123;fig:box-figures&#125;</span><br><span class="line"><span class="keyword">\end</span>&#123;figure&#125;</span><br></pre></td></tr></table></figure><h2 id="表格排版"><a href="#表格排版" class="headerlink" title="表格排版"></a>表格排版</h2><h3 id="竖排表格中的文本"><a href="#竖排表格中的文本" class="headerlink" title="竖排表格中的文本"></a>竖排表格中的文本</h3><figure class="highlight latex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">\rotatebox</span>&#123;90&#125;&#123;some rotated text&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;此处记录一些 Latex 的用法。&lt;/p&gt;</summary>
    
    
    
    <category term="Programming" scheme="https://superuier.github.io/categories/programming/"/>
    
    
    <category term="latex" scheme="https://superuier.github.io/tags/latex/"/>
    
  </entry>
  
  <entry>
    <title>中国为什么有前途</title>
    <link href="https://superuier.github.io/reading-note/china-bold-future/"/>
    <id>https://superuier.github.io/reading-note/china-bold-future/</id>
    <published>2021-08-25T18:04:00.000Z</published>
    <updated>2021-08-25T18:04:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>《中国为什么有前途》 翟东升老师</p><a id="more"></a><hr><h2 id="极简简介"><a href="#极简简介" class="headerlink" title="极简简介"></a>极简简介</h2><p>此处以两版的前言的节选来作简介。</p><h3 id="第三版前言"><a href="#第三版前言" class="headerlink" title="第三版前言"></a>第三版前言</h3><p>本书有2009年开始做了第一版，迄今为止已有第三版（2019年）。本书的若干语言和预警都得以兑现：</p><ul><li>2010年第一版提出逆全球化的风险</li><li>2015年第二版（撰写于2014年）预警美元强势周期导致人民币汇率下跌和资本外逃的恶性循环</li></ul><h3 id="第二版前言"><a href="#第二版前言" class="headerlink" title="第二版前言"></a>第二版前言</h3><p>宏观来看，中国的国际地位提高。但是微观来看，生活中的困境压的普通人喘不过气来。看整体国势，中国好像很有前途；但是看自己身边，好像又问题很大。写这本书是为了告诉读者，做一个21世纪的中国人，是艰辛却又令人骄傲的。</p><p>本书向读者指出一些基本事实和历史轨迹，包括：</p><ul><li>世界历史演进的趋势</li><li>当代中国体制的特质</li><li>中国已经具备巨大潜能</li></ul><h2 id="个人看法"><a href="#个人看法" class="headerlink" title="个人看法"></a>个人看法</h2><h3 id="极简评价"><a href="#极简评价" class="headerlink" title="极简评价"></a>极简评价</h3><p>这是一本全面的书，其以中国改革开放以来的政治经济为主线，涵盖货币，安全，投资，援助等方面，对认识中国当下经济政治政策的形成过程可以起到帮助作用。</p><p>这同样也是一本乐观的书，乐观地总结了中国从上世纪70年代以来的经济货币等政策变动，并乐观的展望了未来中国发展的趋势。为何说这本书乐观？是因为其总能从正反两面考虑过去的政策，甚至是“不那么好的政策”，并提取出有价值的经验。同时又通过分析历史和总结，在考虑到很多政策潜在弊端的情况下，对未来政策提出相对优良一些的改进或优化。</p><h3 id="感想-看法"><a href="#感想-看法" class="headerlink" title="感想/看法"></a>感想/看法</h3><p>本人从这本书收益良多，翟老师在自己的知识框架下对中国过去的走势和未来的发展做出了分析总结。对我个人而言，由于之前知识体系尚未构建，所以对翟老师的观点以认同居多，或许将来在不断学习后，可以更加批判看待书中观点。</p><p>仅从书中总结的事实来看，我个人对中国近年来的发展持乐观态度。可以从书中清晰地看到国内政策的调整与转变，尽管政策或许尚有不足，但是不断完善，向科学完备发展。</p><p>有一个明显有趣的点，就是可以看出翟老师的一些“双标”，总的来说就是我们察觉到了美国的货币霸权，不公平的优先发展权等，对其进行批判，但是我们的目标也是获得这一些权利。还有就是一些政策在我们国家使用的时候可以明显看出利弊，但是宣传给其他国家使用相同政策时则强调好处。这其实无可厚非，“人不为己，天诛地灭”，当然我还是更愿意相信当中国作为一个体系更中心的国家时，能更多的承担一些国际责任，可能要好于多数的民主大国。</p><p>在对外投资和援助上，我以前其实一直没搞清楚必要性重要性和利弊。这里看来或许翟老师已经一一阐述，之后可以新开一个 post 来做一下总结。但是总的来说，能说和不能说的好处都是存在的。此时就有涉及到另一个也很有趣的话题——“师出有名”。为自己的政策和潜在利益作出令人无法反驳的解释，还是很重要的。</p><p>还有一点就是这本书其实强化了一个我的认识。就是金融的本质，或者说最重要的点，是为社会活动提供资金，尤其是优化资金配置。以前我对这一认识并不清晰，觉得金融可能主要在于套利，或者保证市场有效（当然这个也很重要），所以一度觉得金融很虚，甚至有时会存在对其嗤之以鼻但又羡慕人家利润的看法。尤其是之前觉得很多金融人士致力于杀猪盘割韭菜，更是让我这行业持轻视态度（可能是吃不到葡萄说葡萄酸）。但是这本书在国家投资，主权基金，国企民企的陈述中，反复展现了优化资金的配置对于行业，对于国家的重要性，让我强化了金融这个行业负担起优化资本配置责任和义务的认知。所以个人认为，金融，尤其是中国金融，脱虚向实还是很有必要的。当然要能干的了实事，也得能抵御国外的金融攻击。（外行人的见解。）</p><p>其实书中大部分内容，翟老师在政经启翟中都有提到。这里只是在中国发展的维度进行展开。但是总的来说跳不出“人本主义政治经济学”的框架。本人虽然见识的社会比较少，但是对“人才是最重要的财富”深以为然。最后以 Ray Dalio 最近说的一番话结束这番感想。</p><blockquote><p>I encourage you to look at the trends and not misunderstand and over-focus on the wiggles.To understand what’s going on you need to understand that China is a state capitalist system which means that the state runs capitalism to serve the interests of most people and that policy makers won’t let the sensitivities of those in the capital markets and rich capitalists stand in the way of doing what they believe is best for the most people of the country.</p></blockquote><hr><h1 id="按节讨论-摘录"><a href="#按节讨论-摘录" class="headerlink" title="按节讨论/摘录"></a>按节讨论/摘录</h1><h2 id="第一章：世界市场体系的中央与外围"><a href="#第一章：世界市场体系的中央与外围" class="headerlink" title="第一章：世界市场体系的中央与外围"></a>第一章：世界市场体系的中央与外围</h2><p>四类国家：</p><ul><li>中央国家：依靠信心生活，美国</li><li>准中央国家：依靠理性生存，欧日</li><li>外围工业国家：依靠勤劳谋生，中国，印度，东亚，中东欧</li><li>原料提供国家：依靠运气生存，中东，拉美，非洲，东欧，中亚</li></ul><p>四类国家间主要存在<strong>两种关系</strong>：</p><ul><li>贸易关系：材料和产品的流动</li><li>融资关系：美元霸权</li></ul><p>世界市场体系的<strong>基本特点</strong>：</p><ul><li>无远弗届：很少有国家可以孤立于世界市场之外。</li><li>非常不公平的体系：中央国家比起外围国家更富有和稳定。<ul><li>“中央国家掌握自己的货币政策，外围国家难以掌握自己的货币政策。”—索罗斯</li><li>铸币税：信用货币与真实价值；外围投资与美元贬值。</li></ul></li><li>要素流动的选择性：不同要素流动程度不同，信息观念到商品服务到技术到人力资源流动程度递减。</li><li>自我强化功能：中心外围固化，强者恒强，弱者恒弱。（中短期效应）</li><li>周期的自我更新：伊利比亚半岛国家（葡萄牙和西班牙），荷兰，英国，美国<ul><li>人口规模越来越大：当大家都掌握了新的财富与权势要素，规模（人口，幅员）重新变得重要。</li><li>短期内“劫贫济富”，长期“损有余而补不足”。</li></ul></li><li>结构的鲁棒性：<ul><li>利益分布不均衡但是博弈论意义均衡。</li></ul></li></ul><p><strong>中央国家收益、代价和条件</strong>：</p><ul><li>获得的<strong>收益</strong>：<ul><li>提高人均财富拥有量。</li><li>提供对全球经济波动节奏的掌握和调控能力。</li><li>融资和负担转嫁能力。</li><li>语言、人才、政治自信、号召力。</li></ul></li><li>获取和保有中央地位的<strong>条件</strong>：<ul><li>货币霸权（参考如何成功开办和经营一家商业银行。）</li><li>资本实力</li><li>控制商业活动以自己货币进行</li><li>维系重要国家的关系获得支持</li><li>发行金融产品满足外围国家偏好</li><li>存款通过投资转化为资本<ul><li>自己的跨国公司有明显经营优势</li><li>全球市场开放性</li></ul></li></ul></li><li>中央国家承担的<strong>成本</strong>：<ul><li>产业外移，长期逆差，去工业化</li><li>控制战略地区的安全成本的</li></ul></li></ul><p><strong>外围国家发展道路上的常见陷阱</strong>：</p><ul><li>资源诅咒陷阱</li><li>欲速不达的赶超陷阱</li><li>发展过程中的政治陷阱</li></ul><p>寥寥几个外围国家<strong>逆袭</strong>的成功者，<strong>都符合以下特征</strong>：</p><ul><li>极度缺乏自然资源</li><li>人口增速适中</li><li>紧紧依附西方市场</li><li>政府对权力控制牢固</li></ul><h2 id="第二章：中国在体系中的足迹"><a href="#第二章：中国在体系中的足迹" class="headerlink" title="第二章：中国在体系中的足迹"></a>第二章：中国在体系中的足迹</h2><p>从人均角度来看，中国的<strong>比较优势</strong>不在于原材料而在于<strong>劳动力</strong>。改革开放以来实现<strong>“三外路线”</strong>：对外贸易、引进外资（外国直接投资）、对外货币安排。20世纪80年代，中国由外围原料出口国演变为外围工业国。2019年中国即将从外围工业国转变为中央工业国的临界区域（地位等同欧日）。</p><p><strong>“三外路线”</strong>的政策组合，形成了官方巨大的外汇储备：</p><ul><li>人民币大幅贬值锚定美元</li><li>引进外资</li><li>鼓励出口</li><li>经常项目人民币可兑换</li><li>强制结售汇</li></ul><p>“三外路线”的<strong>利弊</strong>：</p><ul><li>“三外路线”的利：<ul><li>就业、工业化、资本积累、对改革的推动、和平。</li></ul></li><li>“三外路线”的弊：<ul><li>本土工业形成挤压、环境资源代价、对外部资本及市场依赖、地区部门阶层间分配失衡</li><li>对中央国家的实质性纳贡，外商利用外汇在中国收益10%以上的年华，而中国用外汇投资收益接近0。</li><li>不可持续性：中国生产美国消费，中国放贷美国借债的循环模式不可持续。</li></ul></li></ul><p>“三外路线”的<strong>修正</strong>：</p><ul><li>科学发展观：以福利而不是 GDP 衡量政策成就。</li><li>外商回归国民待遇，招商引资到招商选资。</li><li>人民币渐进升值过程。</li><li>强调自主创新，培育自身<a href="/economy-finance/seminar-talks/xiaogang/" title="多层次资本市场">多层次资本市场</a>。</li><li>启动内需，减轻外部依赖。</li><li>对国有企业的扶持。</li></ul><p>“三外路线”在中国<strong>何以实现</strong>：</p><ul><li>工业化和现代化的历史使命：发挥比较优势，发展市场上有竞争力的产业，才能顺利实现工业化。</li><li>体制根源：国家权力彻底渗入动员和改造了中国社会，中央地方关系也同样重要。</li><li>思潮和意识形态因素：引入西方经济学，“科学发展观”。</li><li>要素禀赋，人口与资源：规模是竞争优势，也有着巨大的内需市场潜力。</li></ul><p>“三外路线”兴衰<strong>对中国外交的影响</strong>：</p><ul><li>20世纪90年代低调对外，“韬光养晦”。</li><li>2004年后，对西方依赖程度在逐步减轻，西方却更依赖中国。</li><li>2005年来，在联合国使用或声称使用否决权来迫使议案调整。</li><li>2012年以来，走出韬晦。</li></ul><h2 id="第三章：“三外路线”下的对外贸易"><a href="#第三章：“三外路线”下的对外贸易" class="headerlink" title="第三章：“三外路线”下的对外贸易"></a>第三章：“三外路线”下的对外贸易</h2><blockquote><p>即将到来的中国对外产业转移，需要我们思考这样一个问题：如何发展出一种独特的产业链编辑能力，以便尽可能按照我们的政治经济利益塑造地区乃至全球性的地缘经济和地缘政治格局。</p></blockquote><p><strong>中国对外贸易发展的轨迹</strong>：</p><ul><li>外贸惊人增长：<ul><li>贸易收支平衡方面持续顺差</li><li>外贸总量大幅升高和出口总量全球第一</li></ul></li><li>外贸结构改善：<ul><li>商品：原材料到劳动/资源密集型工业制成品到高新技术出口品</li><li>贸易性质：加工贸易比例先上升后下降。2005年之后，落后外资撤出，加工贸易下降，长期来看仍会下降。</li><li>出口目的地：从出口中央国家到出口目的地多元化。对外围地区的出口商由中资企业为主。</li><li>出口企业性质：外资比例稳步回落。</li></ul></li></ul><p><strong>中国对外贸易发展的动能</strong>：</p><ul><li><strong>观念</strong>的变迁：<ul><li>从使用苏联模式到接受西方自由主义经济学并成为主流。</li><li>进口替代型发展战略到出口导向型（不强调建立自己的完整产业体系），但是未极端化而是渐进融合。</li><li>比较优势和“竞争优势”。</li></ul></li><li><strong>制度</strong>的变迁：<ul><li>外贸管理体制市场化、国际化、法律化。</li><li>企业内部治理模式改革。</li><li>外贸经营权放开和扩大。</li></ul></li><li>鼓励出口的政策组合：<ul><li>外汇留成制度和人民币贬值</li><li>出口退税政策促进出口</li></ul></li></ul><p><strong>国际体系环境</strong>：</p><ul><li>日本产业转移，四小龙诞生</li><li>美国限制进口增速</li><li>中国招商引资</li><li>所以对美出口绕行到中韩台地区。</li></ul><h3 id="贸易、产业与地缘政治经济"><a href="#贸易、产业与地缘政治经济" class="headerlink" title="贸易、产业与地缘政治经济"></a>贸易、产业与地缘政治经济</h3><p>回答此章开头的问题。<code>对外贸易是否可以塑造一个有力的地缘政治经济环境？</code>基本思路在于培育“产业链编辑能力”。</p><blockquote><p>所谓产业链编辑能力（capacity of industrial chain editing, CICE），是指大国依靠<strong>资深市场规模</strong>以及<strong>对某些关键性生产要素的掌控</strong>而获得的一种特殊能力，借此可以在一定程度上按照自身国家利益的需要来<strong>主动调整地区性的甚至全球性的产业地理分布</strong>，以便从国家间不对称的相互依赖中获得优势和权利。</p></blockquote><p>今天，经济成长，吐故纳新，顺比较优势，我们之前的劳动力成本和环境成本不应该是优势了。</p><blockquote><p>用国内消费取代外需和投资拉动经济成长的中国经济转型，既是应对全球金融危机和经济衰退的必要举措，也是建设国内和谐社会和可持续发展的内在要求。</p></blockquote><p>所以产能向哪里转移？怎么转移？</p><ol><li>可以推动他国市场内在趋势加速，但是不能改变内在趋势。（斯里兰卡、缅甸、巴基斯坦、孟加拉转移纺织化工。）</li><li>集中力量深度介入少数经济体的经济建设和国家发展。</li><li>对周边单个国家依赖最小化，并让他们对我们依赖最大化。</li><li>中国已有的确保有效的编辑产业链的资源：<ul><li>部分产业的控制权</li><li>技术资本积累</li><li>越来越大的本土市场规模</li><li>政府高调控管治能力</li><li>巨大国家资本积累</li><li>未来国际大国的国际信用</li></ul></li></ol><blockquote><p>新问题：当替他大国也拥有上述资源和条件时，并试图运用这些资源实现与我们目标抵牾的地缘经济政治构造时，会发生什么情况？如何应对？</p></blockquote><h2 id="第四章：外商直接投资与中国的经济安全"><a href="#第四章：外商直接投资与中国的经济安全" class="headerlink" title="第四章：外商直接投资与中国的经济安全"></a>第四章：外商直接投资与中国的经济安全</h2><p>招商引资与<strong>缺口论</strong>：中国为什么要吸引外资？</p><ul><li>资金缺口论：缺钱。这是没有解释力的，不论是外汇还是储蓄。</li><li>技术缺口论：资金本身不是资本，只有资金与特定行业的技术、管理、营销能力结合在一起的时候才成为资本。但是仍没有解释力，技术并未转移。</li><li>制度缺口轮：中国对民营资本的歧视，而对外资无影响，形成外资对内资明显的竞争优势。</li></ul><blockquote><p>中国吸引外资并非政策成就，而是中国经济体制缺陷的症状表现。</p></blockquote><p><strong>渐变的外资政策</strong>：</p><ul><li>地缘上：点到线到面</li><li>行业上：出口加工到一般制造到服务业</li><li>引资方式：合资、合作、外资企业等方式</li><li>引资政策背后导向：进口替代到鼓励出口到促进产业结构升级和调整</li></ul><p><strong>外资的贡献</strong>：</p><ol><li>协助中国转变在全球分工中的角色。</li><li>帮助缓解就业和大城市挑战。</li><li>帮助积累外汇。</li><li>提升中国的技术能力和产业层次。<ul><li>竞争效应</li><li>示范模仿效应</li><li>联系效应</li><li>培训效应</li></ul></li></ol><h3 id="关于外资的争论：“恐外症”“崇外症”及其本质"><a href="#关于外资的争论：“恐外症”“崇外症”及其本质" class="headerlink" title="关于外资的争论：“恐外症”“崇外症”及其本质"></a>关于外资的争论：“恐外症”“崇外症”及其本质</h3><p>本质是带有非理性的思维特征，对中国企业中国人的特质与潜能缺乏最起码的信心。谈论这个问题首先要搞清楚：“谁是外资”、“什么是国家经济安全”、“什么是垄断”。</p><p><strong>恐外症</strong>：</p><ul><li>中国外资依存度过高（考虑实际使用外资规模总额/GDP）<ul><li>我们只算了直接投资，算上间接投资的话，依赖程度并不高</li><li>压低人民币价格情况下计算的GDP是有问题的。</li><li>有根据的统计数据是外资的对外出口占中国出口量的比重。</li></ul></li><li>“斩首策略”：独资企业或者控股合资，进入市场时，将中方品牌束之高阁取而代之。<ul><li>市场的首席裁判员应该是价格而不是政治，被斩首说明产业出了问题。</li></ul></li><li>诉诸情感和道德而不是理性和法理。</li></ul><p><strong>崇外症</strong>：</p><ul><li>外资的优越性和价值被捧得太高，反过来支撑着这个优势。</li><li>对外资迷信的背后，是部分人不愿正视历史和现实，不愿向事实低头，不愿承认中国民营资本的实力和潜力。</li></ul><h3 id="外资政策的未来趋势及政治和战略潜力"><a href="#外资政策的未来趋势及政治和战略潜力" class="headerlink" title="外资政策的未来趋势及政治和战略潜力"></a>外资政策的未来趋势及政治和战略潜力</h3><p><strong>未来的调整方向</strong>：</p><ul><li>取消外资超国民待遇</li><li>将国内市场吸引力而不是廉价要素作为吸引外国直接投资的核心竞争力：中国从全球生产基地转变为一个全球最大市场。</li><li>积极参与甚至推动国际多边投资规则的制定。</li><li>平衡的吸收外国直接投资和间接投资资金。</li></ul><p><strong>产业升级</strong>：</p><ul><li>产业升级难以靠外资实现，高价值就业机会是世界各国政府努力竞争、想尽办法留在本国的东西，在国际市场上本来就稀缺。</li><li>政府及其资本触角在推动中国产业赶超和升级过程中扮演了很重要的角色，但是不同行业中政策绩效存在明显差别。</li></ul><p>案例中的<strong>教益</strong>：</p><ul><li>面向家庭个人的消费品，向民营资本开放越早越彻底，创新能力越强。国企改革适合扮演财务投资者角色，从管资产到管资本。</li><li>中国最大的优势在于中国的市场规模。我们应该把产业发展的希望主要寄托在别国国民和家庭（而非政府）能主导的行业中。</li><li>政府官员要相信本国私人企业家的潜能和潜力，要意识到自己能力有限。政府需要做的是意识到哪些产业出现问题，哪些地方资源配置效率低，并为生产力提升打开空间。<strong>一个好政府的主要任务是，确保没有人能不劳而获，让人民都劳有所得。</strong></li></ul><h2 id="第五章：人民币汇率于人民币国际化"><a href="#第五章：人民币汇率于人民币国际化" class="headerlink" title="第五章：人民币汇率于人民币国际化"></a>第五章：人民币汇率于人民币国际化</h2><h3 id="汇率的政治经济学"><a href="#汇率的政治经济学" class="headerlink" title="汇率的政治经济学"></a>汇率的政治经济学</h3><p>一般观点：<strong>货币即是权力</strong>，即使是和平时期，金融业也是受制于政治需要的。</p><p><strong>不可能三角</strong>：</p><ul><li>资本流动、固定汇率、货币政策独立性三者不能同时成立。</li><li>固定汇率制度下财政政策有效，浮动汇率政策下货币政策有效。</li></ul><p><strong>不同立场的汇率偏好</strong>：</p><ul><li>出口海外投资型偏好汇率稳定，依靠国内市场的则更希望货币政策独立。</li><li>生产贸易品部门偏好固定汇率，生产非贸易品（金融业）部门偏爱浮动汇率。</li><li>中右翼政党更厌恶通货膨胀，左翼更关心失业率。</li><li>执政稳定政党汇率偏向弹性和高估（长远经济目标低通胀），执政不稳政党偏向干涉本币维持低估。</li></ul><h3 id="人民币汇率波动后的政治"><a href="#人民币汇率波动后的政治" class="headerlink" title="人民币汇率波动后的政治"></a>人民币汇率波动后的政治</h3><p><strong>人民币汇率制度的改革</strong>：</p><ul><li>1981-1984：盯住一揽子货币，双轨制，汇率下跌，扶持出口。</li><li>1985-1993：双轨外汇制度，管理浮动，汇率持续下跌，恶化国内通胀。</li><li>1994-2005：盯住美元，汇率长期稳定，促进经济发展。但是前期合理后期僵化，入世后应做调整。</li><li>2005-2013：盯住一揽子货币浮动管理，目标维持出口和就业稳定减少顺差，对美元稳定升值，导致套利行为，助长本土资本泡沫。</li><li>2013之后：参照一揽子货币双向波动，从汇率干预中逐步淡出，目标从被全球流动性绑架的货币政策中解放出来，波动性扩大但是保持强势和稳定，有助于打击套利，热钱流入减少。</li></ul><p><strong>对应的政治</strong>：</p><ul><li>1972-1980：人民币高估不利于出口，贬值又会在非贸易项目吃亏，于是双轨制，贸易非贸易采用不同汇率。</li><li>1985之后：中央地方博弈，政府企业博弈，官方汇率和市场汇率并存。</li><li>1997 亚洲金融危机宣布不贬值，考虑国际形象和香港金融的稳定，同时当时外汇储备高，中央财政资源增强。这一行为奠定了中国在争取东亚地区领导地位的国家信用基础。</li><li>1998年之后盯住美元，不愿意恢复弹性，是在当下结构性失业背景下提出的。</li><li>2005年之后发现汇率低估不利于产业升级，所以开始稳步升值。</li></ul><h3 id="人民币国际化"><a href="#人民币国际化" class="headerlink" title="人民币国际化"></a>人民币国际化</h3><blockquote><p>一个经济强国的货币在实现了自由兑换后，被其他国家接受，成为国际支付和处置手段，我们称这种货币为国际货币。</p></blockquote><p>人民币国际化的<strong>利弊</strong>衡量：</p><ul><li>好处：铸币税、节约外汇储备成本、降低汇率风险、扩大贸易投资、可以对外转移宏观经济风险。</li><li>负面代价和风险：<ul><li>低端产业挤出效应。</li><li>经济泡沫化风险（吸引巨量游资）。</li><li>放大汇率波动（大量流入流出）。</li><li>货币政策会收到海外存量货币的影响。</li></ul></li></ul><p>人民币国际化并不难，向外“送钱”即可。难的是让它的<strong>负面代价最小化</strong>。日本国际化就是我们的反面教训。</p><p>人民币国际化的<strong>机遇</strong>：</p><ol><li>2008年之后全球货币结构的动荡。</li><li>中国积累了足够的经济实力，且有良好的工业基础。</li><li>美欧亚并驾齐驱，但是在亚洲日元难堪重任。</li></ol><p>人民币国际化的进展：</p><ul><li>2012年人民币国际化指数0.56到15年3.91，18年4.84，在震荡中增长。</li><li>政府管制方式重大变化，顺应市场需求政革放权。</li><li>2013开始跨境结算支付规模上升，2017年下降至2012年来最低水平。</li><li>涉外部门统计和管理中人民币计价。</li><li>海外人民币清算行。</li><li>人民币跨境支付系统 CIPS。</li></ul><h3 id="人民币国际化和资本项目放开的政策辩论"><a href="#人民币国际化和资本项目放开的政策辩论" class="headerlink" title="人民币国际化和资本项目放开的政策辩论"></a>人民币国际化和资本项目放开的政策辩论</h3><p>央行支持主张放开。一部分经济学家，包括社科院北大的学者提出批判质疑，反对放开。两者展开了辩论：</p><ol><li>从利弊上看：<ul><li>支持者认为放开的好处：<ul><li>更多融投资机会，优化资源配置</li><li>创造非政府部门资本流出途径，避免实体经济硬着陆</li><li>倒逼国内改革</li></ul></li><li>反对者的焦虑：<ul><li>资金外流对国内产业负面冲击</li></ul></li></ul></li><li>从开放和改革的顺序上看<ul><li>央行认为可以同步审慎进行</li><li>一部分学者认为先改革再开放</li></ul></li><li>开放倒逼改革的理念<ul><li>反对者认为以外促内风险性高，历史上不乏失败案例</li></ul></li><li>要不要公布开放时间表<ul><li>反对者认为给出开放时间表不如给出推动国内结构性改革时间表</li><li>支持者认为时间表有必要。因为开放的实际利益受损者是权力部门，让受损者制定开放的方案推动起来会很难</li></ul></li></ol><p>辩论的<strong>三个特点</strong>：</p><ol><li>对风险的认知和态度存在很大差异。</li><li>双方都是列举归纳，缺乏整体主义视角。不完全归纳无法说服对方。需要有整体视角，用演绎法而不是归纳法。</li><li>双方对“应然”和“实然”的侧重。</li></ol><h3 id="国际化的条件和战略"><a href="#国际化的条件和战略" class="headerlink" title="国际化的条件和战略"></a>国际化的条件和战略</h3><p>阶段：</p><ol><li>区域性国际化，东亚与部分发展中国家展开。</li><li>成为国际金融活动媒介和国际金融资产。可以使用美元当年“先挂钩后脱钩”的策略来获取信心。</li></ol><p>步骤：</p><ol><li>国内多层次资本市场和银行体系改革</li><li>升级产业结构和提高技术能力</li><li>全球资产投资和并购渠道的建设</li></ol><h2 id="第六章：外汇管理政策与外汇储备"><a href="#第六章：外汇管理政策与外汇储备" class="headerlink" title="第六章：外汇管理政策与外汇储备"></a>第六章：外汇管理政策与外汇储备</h2><p><strong>外汇储备实际上是替外资保管的金银细软</strong>。但是也是一种前所未有的对外政策工具，政治上实现了“金融恐怖平衡”，大大弥补了中国整体国力尤其是军事能力方面的弱势。</p><h3 id="外汇管理制度的改革与储备的形成"><a href="#外汇管理制度的改革与储备的形成" class="headerlink" title="外汇管理制度的改革与储备的形成"></a>外汇管理制度的改革与储备的形成</h3><blockquote><p>外汇储备：当局（不包含民间）能够有效控制并可随时动用的对外资产。</p></blockquote><p>中国的外汇储备是在“强制结售汇政策”，“银行外汇周转头寸限制”，“盯住美元汇率政策”三位一体的制度安排下形成的。以10%的复合收益率估算，我们外汇储备规模远远小于外资在中国经济体内部的资产积累。</p><p><strong>之后外资撤出大量挤兑时怎么办？</strong>市场有风险，买卖自愿，人民币可以接受一定程度的本币贬值。</p><h3 id="外汇储备的经济含义"><a href="#外汇储备的经济含义" class="headerlink" title="外汇储备的经济含义"></a>外汇储备的经济含义</h3><ol><li><strong>为什么</strong>要积累巨额外汇储备？<ul><li>底部贸易赤字、保证偿付的外汇需求、维护汇率稳定、灾难时的战略储备。</li></ul></li><li>到底应该<strong>如何看待</strong>中国的外汇储备？<ul><li>外汇储备的高速不平衡增长源于国内金融体系的功能缺陷，效率低下，无法将储蓄转化为有效投资。</li><li>只要我国国内金融体系与国际金融体系存在较大的效率差异，我国就只能通过这种“体外”资本循环方式支持国内经济增长。</li><li>导致了巨大的机会成本</li></ul></li><li>中国积累多少外汇储备合适？外储如何实现<strong>保值升值</strong>？<ul><li>维持正常的需求，7000亿之内。</li><li>国债投资转为优质企业股权投资（例如淡马锡、阿布扎比投资局、挪威养老基金）。</li></ul></li><li>如何<strong>控制</strong>外汇储备的过度增长？<ul><li>改善国内金融市场，民间金融合法化</li><li>拓宽对外直接投资的空间</li><li>加大人民币汇率弹性，减少游资套利空间</li></ul></li></ol><h3 id="外汇储备与“中国-美国”"><a href="#外汇储备与“中国-美国”" class="headerlink" title="外汇储备与“中国-美国”"></a>外汇储备与“中国-美国”</h3><p>中国明知持有的美元存在贬值倾向，但是不能卖出，因为如果卖出美元，那么美元将出现恐慌性贬值，导致自己手中剩余的美元资产缩水。中国所能做的就是与美国政府谈判，警告他们不要做出背叛的行为，否则两败俱伤。</p><blockquote><p><strong>金融恐怖平衡</strong>：我们（美国）依赖的是他国不对美国赤字融资所需偿付的代价，（也即）他国一旦停止融资需要偿付的代价（是如此之大），确保了他国将继续融资。</p></blockquote><p>中美之间数十年相对成熟稳定的主要原因是因为存在“核恐怖平衡”和“金融恐怖平衡”。平衡中中国处于相对弱势。随着中国新世纪出口多元化，对美依赖减轻，谈判地位强弱之势悄然异位。</p><p>中国意识到这种关系不可持续，所以逐步从“三外路线”循环中脱离。中美关系少了重要的稳定器，如何维持一个较低成本的可持续和平环境？并且全球经常项目和资本项目顺差在中国周边大规模聚集是存在安全和政治前提的，就是东亚和平与稳定。</p><blockquote><p>由于美国目前仍然主导着东亚地区的和平与安全格局，那就潜在的出现了一个巨大的套利机会：跳起某个地区性的冲突并放手升级之，结果将是东亚资本的极速外逃，美国的融资问题一夜间获得解决，尽管这是一种杀鸡取卵的不可持续的解决方法。</p></blockquote><h3 id="储备多元化及其政策后果"><a href="#储备多元化及其政策后果" class="headerlink" title="储备多元化及其政策后果"></a>储备多元化及其政策后果</h3><p>中国持有大量美元，但其实是美元的<strong>空头</strong>，而不是多头。华尔街跟庄，中国外汇储备增大时，他们就做空美元做多欧元，减小时就相反。那常识为什么错了？外管局结售汇过程中收入的资产重新配置为美元资产和非美元资产。就是买入外汇资金然后配置。买入端美元比例极高（例如80:20）（由于美元可获得性高），然后配置时比例（50:50），其实是在卖出美元，买入非美货币。所以当资产配置在美元上绝对数目多时，其实已经售出了大量的美元。</p><p>作为美元的空头，中国外汇储备面临何种风险？当然是美元上升周期。</p><ul><li>外汇储备名义价值下降，欧日元资产贬值。</li><li>金融系统投入矿业和原油的信贷将面临风险。</li><li>不能陷入被逼空的状态。</li></ul><p><strong>美元的上行一般由以下因素驱动</strong>：</p><ul><li>美国经济领先于日欧复苏</li><li>失业率下跌到6.5%以下</li><li>美国能源独立导致逆差减少</li><li>中国因素（空头力量其实在衰减）</li></ul><p>被<strong>全面逼空</strong>的前提条件：</p><ol><li>美元超级强势周期</li><li>巨额资本逃离</li><li>出现新制造业大国</li><li>国内通胀高企，人民币没有贬值空间</li><li>中国产业转型面临失败</li></ol><h2 id="第七章：中国对外资本输出"><a href="#第七章：中国对外资本输出" class="headerlink" title="第七章：中国对外资本输出"></a>第七章：中国对外资本输出</h2><p>中国正在从一个（产业）资本净输入国变成净输出国。</p><h3 id="中国企业“走出去”"><a href="#中国企业“走出去”" class="headerlink" title="中国企业“走出去”"></a>中国企业“走出去”</h3><p>中国企业“走出去”对提升巩固中国在世界市场体系中的地位具有重大意义。政府也改革投资体制，鼓励本地企业国际直接投资。</p><p>“走出去”的<strong>动机</strong>：</p><ul><li>获得市场</li><li>获得外部生产要素：能源、原材料、技术、研发能力、品牌</li><li>往返程投资：打扮成外资回归</li></ul><p>“走出去”的<strong>问题和挑战</strong>：</p><ul><li>国有企业的体制问题：监管不到位容易贪污和资本外逃</li><li>民营企业融资担保和保险问题</li><li>企业和个人要在行为方式上严格约束自己，要合规合法道德</li><li>企业对外直接投资的政策体系和服务体系不完善</li></ul><p>自主品牌国际推广过程为<strong>中国国际公关提供政策杠杆</strong>：成为西方媒体的大客户，则会拥有对他们施加压力的重要杠杆，他们在报道中国的时候会有所顾忌。</p><h3 id="中国资本输出的现状和未来"><a href="#中国资本输出的现状和未来" class="headerlink" title="中国资本输出的现状和未来"></a>中国资本输出的现状和未来</h3><p>输出的现状：</p><ul><li>1978-2000：建立经济特区吸引资本</li><li>2001-2012：加入 WTO 引入输出并重，央企海外并购</li><li>2013-至今：输出为主，兼顾引入</li></ul><p>输出的<strong>主体</strong>：</p><ul><li>国有企业为主：效率低下，动机模糊，意识形态上被怀疑</li><li>民营企业很少：天赋惊人，家族地缘为基础信用网络融资，但是目光短视</li></ul><p><strong>战略方向和地缘空间</strong>：</p><ul><li>金砖体系、一带一路</li><li>西南西北产业聚集带</li><li>西部大开发升级</li></ul><h2 id="第八章：国际投资规则的制定权争夺"><a href="#第八章：国际投资规则的制定权争夺" class="headerlink" title="第八章：国际投资规则的制定权争夺"></a>第八章：国际投资规则的制定权争夺</h2><h3 id="主权财富基金的发展及其投资规则之争"><a href="#主权财富基金的发展及其投资规则之争" class="headerlink" title="主权财富基金的发展及其投资规则之争"></a>主权财富基金的发展及其投资规则之争</h3><p>主权财富基金是一种政府金融投资工具。与外汇储备不同，它偏好更高收益率。从资金来源看，主权财富基金可以分为三类：</p><ul><li>贸易外汇盈余</li><li>资源出口外汇盈余</li><li>国际援助</li></ul><p>2007年开始，西方政界开始担心主权投资中钱的所有者之意图和潜能，担心出于政治和战略目的而不是商业盈利目的的大规模买卖。所以下方一直主张制定一些“国际制度”来约束我们。<strong>西方国家一方面想要这个资金，另一方面又害怕金融核武器。</strong>目前来看，国际新规者制定主导权基本掌握在美欧手中。但是其实美欧之间，各大经济体内部也存在分歧。但是仍有一些<strong>共同诉求</strong>：</p><ul><li>通过非正式国际立法来约束和规范主权基金，并将规则的制定权和监督过程掌握在美欧控制的 IMF 和 OECD 手中。</li><li>从道义上贬低、法律上禁止实践中防范各国主权基金出于政治与安全动机的投资行为。</li><li>要求主权基金提高透明度。</li><li>在此前提下，维持各国资本市场的开放。</li></ul><p>于是美欧采取了一些明确的策略和措施：</p><ol><li>以美欧主导的多边经济组织作为工具，制定规则，拉拢引诱世界各国接受作为未来谈判的基础。</li><li>制造国际舆论并炒作形成国际共识</li><li>分而治之，压迫小国主权基金满足规范，塑造国际惯例。</li></ol><blockquote><p>我们在贸易、裁军、气候变迁等各种全球治理问题谈判中，已经反复得到如下经验和教训：参与到国际规则的制定中是维护国家利益的第一步，也往往是最关键的一步。</p></blockquote><p><strong>中方立场</strong>：</p><ol><li>鼓励而不是禁止出于政治目的的投资</li><li>提高透明度应该是在坚持联系原则和自主原则两大基本前提下进行。</li><li>维持美欧资本市场开放下是对主权基金东道国的合理补偿，也是维持全球金融经济秩序稳定可持续的基本前提。</li></ol><p>当越来越多的资产和股权被划为非卖品的时候，他们所支撑的货币将会不可挽回的越来越疲软。</p><h3 id="当前谈判地位和可选策略"><a href="#当前谈判地位和可选策略" class="headerlink" title="当前谈判地位和可选策略"></a>当前谈判地位和可选策略</h3><p>当前逆来顺受，被动妥协。</p><p>措施和策略：</p><ol><li>搞一个功能性同盟，把有共同利益的国家拉到一起，成立一个新的投资者俱乐部。团结有大量资本的依附性小国。</li><li>扶持急需外来投资的中小国，塑造我们的国际惯例。</li><li>自主研讨会，探讨全球主权基金投资规则。</li><li>假如能团结几大主权基金，则在谈判桌上能制定有利于资方的规则。</li></ol><h3 id="国际投资法与中国的选择"><a href="#国际投资法与中国的选择" class="headerlink" title="国际投资法与中国的选择"></a>国际投资法与中国的选择</h3><p>世界范围<strong>投资法的发展</strong>：</p><ul><li>多边国际投资法尝试，但是普遍失败</li><li>双边投资条约的发展</li></ul><p>中国处于一个吸收投资和资本输出的双重身份中。但是大背景是投资规模会超过吸纳投资：所以中国新世纪以来的投资协定，采纳了类似美式范本的高标准投资保护机制。</p><h2 id="第九章：超越“能源安全”"><a href="#第九章：超越“能源安全”" class="headerlink" title="第九章：超越“能源安全”"></a>第九章：超越“能源安全”</h2><p>不论是从人均还是未来前景看，中国都缺能源。国际能源炒家以能源安全剥削中国。但是高油价也许并非坏事，中国进口量远小于体系中央国家。如果油价使中央财富流向边缘，那么可以通过对边缘国家的出口把钱赚回来。所以要跳出有人营造的恐慌情绪，跳出那些直接的局部的利益得失，才能冷静的全面辩证看待能源在中国国家战略中的意义。</p><h3 id="能源问题的若干基本常识"><a href="#能源问题的若干基本常识" class="headerlink" title="能源问题的若干基本常识"></a>能源问题的若干<strong>基本常识</strong></h3><ol><li>能源生产消费在地域分布上严重失衡。</li><li>能源结构走向多元化，化石能源仍是主体。</li><li>能源是世界上贸易规模最大的大宗商品。</li><li>能源问题被高度政治化。</li></ol><h3 id="中国的能源安全"><a href="#中国的能源安全" class="headerlink" title="中国的能源安全"></a>中国的能源安全</h3><p>什么是能源安全？狭义：可以安全供应，是可以充分稳定经济的获取发展所需要的能源。</p><p>中国的能源安全可以分解为<strong>四个环节</strong>：</p><ol><li>能源供给安全：有时是非卖品，所以需要同时使用市场和外交两种手段</li><li>能源价格安全：并不是越廉价越好</li><li>能源运输安全：防范被截断</li><li>能源消费安全：环境气候危害</li></ol><p>中国的能源缺口是常见的夸大之处，其实缺口不大，有时甚至供大于求。能源安全运输也被夸大，马六甲海峡其实可以替代。</p><h3 id="高油价利于中国崛起"><a href="#高油价利于中国崛起" class="headerlink" title="高油价利于中国崛起"></a>高油价利于中国崛起</h3><blockquote><p>为了中国整体国家利益的最大化，中国在适当的时候有必要托起石油价格。</p></blockquote><ol><li>较高油价可以维持有利于中国的全球战略平衡。<ul><li>我们希望确保美国在任何时候都无法集中足够的力量、意志和盟友资源对华实施战略摊牌。</li><li>能源价格越高，中东地区财政力量越强，越有能力为美国及其盟友制造麻烦。</li><li>如果能源价格暴跌，很多国家包括俄罗斯都会陷入财政困境。</li></ul></li><li>托举油价在经济上也是合算的<ul><li>油价维持较高位置，有利于中国扩大对外围体系国家出口投资，将托市成本赚回来。</li><li>适度托举油价有利于提升中国在国际能源市场上的定价权。</li><li>拥有巨额石油储备之后，可以避免逼空。</li></ul></li><li>熨平能源价格波动是大国的国际责任<ul><li>帮助发展中国家应对能源诅咒</li><li>我们的战略储备基地已经竣工</li></ul></li></ol><h2 id="第十章：原材料市场中国的定价权"><a href="#第十章：原材料市场中国的定价权" class="headerlink" title="第十章：原材料市场中国的定价权"></a>第十章：原材料市场中国的定价权</h2><h3 id="原材料市场的三个关键问题"><a href="#原材料市场的三个关键问题" class="headerlink" title="原材料市场的三个关键问题"></a>原材料市场的三个关键问题</h3><ol><li>市场结构与定价权。即便我们在生产规模和莫呕血消费规模上是最大的，但是如果市场分散无序，在国际市场上仍不会有定价权。所以对国内市场整合，用国内的联合协作对付国际上跨国公司的联合垄断，才能扭转地位。</li><li>资源配置到底应该以价格还是权力为杠杆？以价格为杠杆可以让整体福利和效率最大化。但是国际市场非充分开放。中国应该用现实主义确保自己的安全利益，再用理想主义去推动开放。</li><li>资源能源与金融和货币关系。一个国家在拥有国际影响力的大宗交易产品市场，那么该国在争夺相关商品全球定价权时便获得了不可小视的技术型便利。</li></ol><h3 id="开放经济环境下的中国粮食安全"><a href="#开放经济环境下的中国粮食安全" class="headerlink" title="开放经济环境下的中国粮食安全"></a>开放经济环境下的中国粮食安全</h3><p>粮食具有作为重要战略资源的政治经济特性。其需求刚性生产周期性，价格弹性小。</p><blockquote><p>当前分工：外围国家从美国及其盟友进口粮食，自己则专门生产经济作物。当年管仲所用的战略模式，今天在体系大国和外围小国之间悄悄重演。</p></blockquote><p>对中国人来说，中国人什么都吃，是这个民族历史上所受苦难的遗迹和明证。目前来看，从供求关系来看，中国粮食自给有余，基本平衡；从战略态势来看，中国是在勉强防守，态势堪忧。</p><p>当前的平衡能维持，是因为中央政府比较富裕。如果不能维持，则要内部深度挖掘潜力，等待人口总量下降。但是未来三十年，我们<strong>通过内部挖潜维持平衡的难度将越来越大</strong>：</p><ol><li>人民币汇率上涨，粮食美元计价成本高，维持粮食产量则需要增大补贴（杠杆成倍放大补贴）。</li><li>工业化城市化，精耕细作失去其劳动力基础。</li><li>水资源匮乏限制粮食增长。</li></ol><p><strong>对于全球粮食格局的三个基本判断</strong>：</p><ol><li>全球人口增加将为粮食安全带来巨大挑战，为美国国家权势反弹埋下伏笔。</li><li>世界不缺耕地，也不缺潜能，缺的是耕地与高科技农业劳动力的错配。</li><li>粮食安全不能单纯依赖市场，必须有大国出来平抑粮价。</li></ol><p>中国未来粮食安全的长期<strong>解决出路</strong>是大规模农业资本输出：</p><ol><li>政府出面包租购买土地。出资本管理技术，粮食按比例分成。</li><li>合同由政府签，经营由市场吸引国内农业企业承包。</li><li>控制中方农技人员数量与东道国结合。</li><li>中方企业粮食分成由中国官方投资机构保价收购，返销国内，或者在其他地区建立储备库用于平抑波动。</li><li>国内建立更大规模粮食储备能力。</li></ol><h3 id="疯狂的石头：铁矿石进口及其谈判"><a href="#疯狂的石头：铁矿石进口及其谈判" class="headerlink" title="疯狂的石头：铁矿石进口及其谈判"></a>疯狂的石头：铁矿石进口及其谈判</h3><p>乌克兰中国铁矿石储量大，但是贫矿多富矿少，含铁量低。从铁产量上来看，澳大利亚和巴西是最主要的两个铁矿石生产大国。世界最大的铁矿石厂商位于巴西的淡水河谷，86亿吨高品位铁矿石储量。铁矿石主要进口方是中日韩欧。</p><p>在铁矿石价格谈判中，中方2003年开始参与。宝钢作为代表参与谈判。宝钢在谈判中没有考虑到其他钢企的利益。2009年宝钢退出谈判，中钢协接手。他们站在维护国家整体利益的立场谈判，“宁可谈判破裂也不妥协”，但是对于企业实际需求和市场谈判经验不足。2010年，宝钢重新占据谈判主导权。2012年初，铁矿石现货交易平台启动。2012年以来进口地区经济增长放缓，铁矿石产量却屡创新高，价格走低，转变为买方市场。2014年，我国开始涉足铁矿石期货的金融服务，旨在推动人名币计价、清算和结算铁矿石，是金融市场的创新，也满足实体经济套期保值的需求。</p><p>中国决策者意识到美元存款的不可靠和矿藏价格低迷的机遇。于是鼓励国企央企走出国门收购海外矿山资源。2009中国铝业收购力拓赔了夫人又折兵。五矿收购 OZ Minerals 旗下核心资产则获得良好收益。</p><p>有专家认为，如果要改善目前处境则需要在内部贸易和公平性下功夫，改变国内钢企一盘散沙的局面。也有学者提出取消钢铁产品出口退税。</p><h3 id="稀土"><a href="#稀土" class="headerlink" title="稀土"></a>稀土</h3><p>我国稀土储量占全球71.1%，产量95%。却不能像石油和铁矿一样控制全球价格，这是因为企业互相杀价，导致价格低位运行。（其实国外也有大量的未开采稀土矿，因为其相对于中国稀土不经济所以仍未开采。）</p><p>中国政府试图实现稀土行业全行业整合。2009年对我国储量产量第一的三种矿保护性开采。并鼓励企业走出去收购国际重要稀土矿资源。对保护性开采，西方国家强烈反对。但是这并没有违背国际贸易准则，且符合国际惯例。</p><blockquote><p>联合国《建立新的国际秩序宣言》：每个国家对自己的自然资源和一切经济活动拥有充分的永久主权。为了保卫这些资源，每个国家都有权采取适合自己的手段，对本国资源及其开发实行有效控制……任何一国都不应遭受经济、政治或其他形式的协迫，以致不能自由地和充分地行使这一不容剥夺的权利。</p></blockquote><p>中国稀土政策的若干选项：</p><ol><li>提高关税，控制出口配额：有利于内外价差，和新能源产业的先手。</li><li>对矿企征收高额环境税和资源税：利益留在中国，抬高全球价格，但是没有保护国内科技企业利益。</li><li>稀土金融化、货币化、储备化（像黄金一样）：财富增值，但是降低其使用价值。</li></ol><h2 id="第十一章：中国的对外援助"><a href="#第十一章：中国的对外援助" class="headerlink" title="第十一章：中国的对外援助"></a>第十一章：中国的对外援助</h2><h3 id="中国援外的历史和现实"><a href="#中国援外的历史和现实" class="headerlink" title="中国援外的历史和现实"></a>中国援外的历史和现实</h3><ol><li>新中国成立后30年：<ul><li>中国对外经济技术援助八项原则</li><li>大力援外并不是过度承担国际义务，影响自身发展，而要与当时冷战的背景结合理解。（但是这里也没说怎么理解。）</li><li>第一代领导人，世界革命的背景</li><li>谋势，用较小成本获得国家声望</li></ul></li><li>“三外路线”下的援外<ul><li>随着人民币贬值，受援国感到援助力度减小</li><li>自身经济扩张，援助占本国财政份额下降</li><li>第二代领导人，优先本国经济发展</li><li>无偿援助变贴息贷款，大型项目变中小型项目等</li><li>谋利，不求改变体系而是融入体系</li></ul></li><li>援外工作新高潮<ul><li>为了确保能源原材料供给，重新活跃</li><li>与西方国家口惠而不实形成对比</li><li>强调双赢</li><li>美欧认为中国援助：<ul><li>涉及一些对专制政权的援助</li><li>不干涉内政不利于改善全球治理水平</li><li>利用援助做为争夺原材料的辅助手段</li><li>很少雇佣当地人，不利于当地经济发展</li></ul></li></ul></li></ol><h3 id="构建中国特色援外理论"><a href="#构建中国特色援外理论" class="headerlink" title="构建中国特色援外理论"></a>构建中国特色援外理论</h3><p>自利利人的<strong>审慎道德</strong>原则：重塑对外援助的政治伦理基础</p><ul><li>援助宣传中的道德主义是一种陷阱，大家的援助都被诟病的原因在于，大家普遍使用很高的道德标准来要求和评价援助行为。</li><li>援助和意识形态的捆绑，无益于国家间的稳定和良性互动。</li><li>国家的对外援助不是慈善活动，而是理性的广义的国家利益拓展的需要。</li><li>审慎性道德准则：用人的标准来要求而不是神的标准来要求人，不通过损害他人以自利。</li><li>审慎道德标准低调可行可持续。</li><li>中国援助行为的特点：心理上平等，事理上互利。</li><li>中国人明白：对于落后者而言，尊严往往比五斗米更值得珍惜。</li></ul><p>在符合审慎道德原则下，<strong>国际援助的必要性</strong>：</p><ol><li>增进国家人民间信任友谊和认同，利于体系持久和平。</li><li>有殖民原罪的国家，有义务提供援助：假如先辈犯罪而获得好处至今仍为当代人享用，那么赎罪是援助不单是消除负罪感，更是一种政治和道德的必须。</li><li>全球市场中心的大国，发达国家从风险收益不对称获利巨大，代价由外围国家承担，补贴外围国家是平衡体系内生的不公平。</li><li>贫困和混乱会溢出到其他国家和地区，影响富国的长期利益。</li></ol><p>干涉与良治：援助中的主权问题：</p><ul><li>用财力逼他人就范，实际也是一种专制</li><li>任何人手中都不拥有绝对真理</li><li>在一国经济技术水平和社会结构非常落后情况下，强求发展民主可能是拔苗助长的做法。</li><li>援助者不应随意对他国家庭内部事务发表见解。</li></ul><h3 id="改进中国的援外政策"><a href="#改进中国的援外政策" class="headerlink" title="改进中国的援外政策"></a>改进中国的援外政策</h3><ol><li>重视人的作用<ul><li>争取当地民心，不能只惠政府</li><li>重视人员交流，服务民生</li></ul></li><li>改革援外决策和管理机制<ul><li>缺乏协调统一的高于各部委的决策部门</li><li>随意性较大缺乏论证</li><li>权利竞争，责任推诿</li><li>未纳入法制化轨道</li><li>企业和非政府组织的高效对外援助还有很大空间</li><li>应该争取本国民众理解</li></ul></li><li>集中援助树“典型”<ul><li>“撒胡椒面”难以产生强大的初始动能</li><li>评估上来说如果该国更依赖中国市场而不是中国政府，则援助成功</li><li>对其他国家对华政策产生诱导作用</li><li>安哥拉模式就是很好的例子</li></ul></li><li>援外和资本输出结合<ul><li>形成合力，良性循环</li><li>滚动发展，使对外援助在中国内部具有经济效益上的可持续</li><li>这种援助某种意义是一种投资</li><li>在和美元脱钩后，我们需要使对外援助在中国对外经济关系中具有不可替代性，要让其真实成本降低。</li><li>援助形成的友好氛围，基础设施可以为投资创造条件和抬高收益率</li><li>土地矿权股权有获得资本增值机会</li><li>金融危机救援，救急不救穷，别人做不了的事情往往最有利可图</li></ul></li><li>改进对援外事物的宣传<ul><li>拿出少量美元储备以贷款方式援助是出于对国家利益的整体长远考虑</li><li>民众认为政府拿着本来属于自己的财富白送给别人，这反映出：<ol><li>货币知识的缺失：美元不能用在国内，不用掉只能躺在美元债券，不能支付国内贫困学生的学费或者为苦难群众造房子。</li><li>民生问题关注度高</li><li>宣传技巧有进一步提升空间</li></ol></li></ul></li></ol><h2 id="第十二章：结论：中国对外经济关系的潜能和风险"><a href="#第十二章：结论：中国对外经济关系的潜能和风险" class="headerlink" title="第十二章：结论：中国对外经济关系的潜能和风险"></a>第十二章：结论：中国对外经济关系的潜能和风险</h2><h3 id="关于两组关系的探讨"><a href="#关于两组关系的探讨" class="headerlink" title="关于两组关系的探讨"></a>关于两组关系的探讨</h3><ol><li>国家与市场间的关系<ul><li>国家很重要，但是如果光靠国家控制，没有市场来配置资源，那么经济必然缺乏活力，最终国家力量也会衰竭。市场的确很高效，但是离开了国家力量的规范和约束，市场秩序和市场结构难以自我维持。</li><li>为了在全球市场获取超额利润，内部非市场化往往是一种必须，有选择有分寸的搞些政府管制，尽管会不可避免的导致资源配置效率损失，但是由于这种管制获得了外部博弈的优势，那么超额利润可能足够弥补内部损失。</li></ul></li><li>中国与体系间的关系<ul><li>哪个国家越能够全面彻底有效地把人类整合到全球性的交易分工和要素流动中来，就越能得到“天命”的资助。（世界市场体系对各地区各文明的吞噬消化和整合。）</li><li>我们将用什么办法来实现比美国时代更加全面深入有效的整合全球的经济要素？</li><li>我们如何在主导权更替的过程中尽可能实现和平禅让而不是物理决斗？</li></ul></li></ol><h3 id="“三外路线”是否可以向外围国家推广"><a href="#“三外路线”是否可以向外围国家推广" class="headerlink" title="“三外路线”是否可以向外围国家推广"></a>“三外路线”是否可以向外围国家推广</h3><ol><li>“三外路线”的<strong>普适性</strong>：<ul><li>体系外围国家：资本缺乏，工业技术能力落后，层次较低，人才匮乏，制度落后，国内治理混乱。互为因果互相牵制。</li><li>三外路线使中国一下子摆脱了资本匮乏，也让其他方面逐步改善。同时快虚积累外汇储备，增强了抵御系统性风险的能力。</li><li>三位路线可以看成一种融资模式，向他国（产业资本）融资，向未来融资，从而在特定地域和时间点上集中足够密集的资本规模，实现发展的突破，开启发展的正循环。</li></ul></li><li>“三外路线”的<strong>推广</strong>：<ul><li>向一部分发展中国家推荐，对中国有以下好处：加速产业升级和对外转移趋势，减少美欧贸易顺差但是不会损减整体福利，出口数量损失换来的是质量提升，在国家间形成不对称依赖，符合人民币国际化内在需求。</li><li>美欧应该不会反对此路线</li><li>适用性前提：<ol><li>资源富集劳动力匮乏就不适用</li><li>要求政府对社会有很强的渗透控制能力</li><li>出口加工工业的地理要求</li><li>民族文化对勤劳节俭的鼓励（较容易改变，例如70年代被认为懒惰，90年代勤奋。）</li><li>竟然制度变迁可以大幅度改造中国民众的生活方式，为什么对非洲或者其他地区人民持悲观态度呢？</li><li>时机问题，目前中国资源密集型产业向外转移</li></ol></li></ul></li></ol><h3 id="中国对外经济关系战略潜能"><a href="#中国对外经济关系战略潜能" class="headerlink" title="中国对外经济关系战略潜能"></a>中国对外经济关系战略潜能</h3><blockquote><p>中国已积累了罕见的经济实力，但是执政者还不善于用这种实力寻求国际抱负。</p></blockquote><p><strong>对各章探讨的对外经济关系各个方面潜能加以总结</strong>：</p><ol><li>参与国际经济分工时，可以使用自身各方面优势塑造周边中小国对中国的不对称依赖。</li><li>集中了世界最大的外汇官方储备可以通过耐心大胆巧妙的长期投资把手中外汇变成世界主要跨国企业的控制权，形成网络化的非正式权力。</li><li>推动人民币国际化，修正货币格局，让东亚在世界经济体系获得更有利的位置，并对美国国际权势基础形成瓦解作用。</li><li>中国企业走出去创建品牌的时候，中国的外宣系统应当加以支持和利用，获得世界各地主流媒体的影响力。</li><li>借助自身资金和市场规模，应该有选择的调控全球基础原材料的价格，拥有部分定价权。</li><li>对外援助和资本输出结合起来，在体系外围国家扮演金融和货币危机救援者的角色，形成中国援助品牌。</li></ol><p><strong>这些构想的实现需要中国的决策和执行体系做必要的改革，尤其是将对外经济事务同涉外政治事务之间的制度性藩篱消除掉。</strong>比较现实的政策建议，通过边际性改革促进对外战略和对外经济政策之间实现若干功能性的结合。</p><h3 id="对外经济关系的风险"><a href="#对外经济关系的风险" class="headerlink" title="对外经济关系的风险"></a>对外经济关系的<strong>风险</strong></h3><p><strong>2010-2020:</strong></p><ol><li>货币政策与泡沫化风险：<ul><li>这里不是说价格高得让人难以理解就是泡沫，而是符合某种特点的价格运动过程。（自我增长自我毁灭的循环。）</li><li>外资的投资行为，造成了人民币升值的事实，又使外资“理性决策”，继续投资。</li></ul></li><li>全球化退潮风险</li><li>对外投资效率风险<ul><li>国有企业体制缺陷，对人激励约束不充分</li><li>民营企业规模小，资本有限</li></ul></li></ol><p><strong>2020-2030:</strong></p><ol><li>逆全球化纵深发展，中美经济持续脱钩，并各自组团将全球市场再次变成相互平行而竞争的两个体系。</li><li>美国国内的债务周期将迎来巨幅调整阶段。<ul><li>2021年前后巨额的债券到期，借新还旧不可持续成本提高，除非大幅降息。</li></ul></li><li>一带一路面临的挑战，没有很好的回答以下问题：<ul><li>发展中国家缺什么发展不起来？<ul><li>可能并不是钱，而是现代主权国家治理能力和治理体系。</li></ul></li><li>我们通过一带一路究竟在追求什么要素？<ul><li>不是能源资源，甚至也不是劳动力，而是<strong>巨量年轻消费者</strong>。</li></ul></li><li>当发展中国家还不起债的时候，我们如何追求回报？</li></ul></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;《中国为什么有前途》 翟东升老师&lt;/p&gt;</summary>
    
    
    
    <category term="Reading Note" scheme="https://superuier.github.io/categories/reading-note/"/>
    
    
    <category term="读书" scheme="https://superuier.github.io/tags/%E8%AF%BB%E4%B9%A6/"/>
    
    <category term="翟东升" scheme="https://superuier.github.io/tags/%E7%BF%9F%E4%B8%9C%E5%8D%87/"/>
    
  </entry>
  
  <entry>
    <title>代理模型辅助的演化计算</title>
    <link href="https://superuier.github.io/paper-reading/surrogate/"/>
    <id>https://superuier.github.io/paper-reading/surrogate/</id>
    <published>2021-08-19T12:00:00.000Z</published>
    <updated>2021-08-19T12:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文是对 Surrogate-assisted evolutionary computation: Recent advances and future challenges 这篇综述的阅读笔记。这是一篇发表于2011年的关于在演化计算中如何应用辅助模型的综述。</p><a id="more"></a><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>绝大部分的演化计算都假设存在一个可以为每一个个体提供 fitness value 的手段，或是模拟，或是实验，或是一个显式的 fitness function。但是有的时候，这个 fitness 的评估是非平凡的，例如当模拟或实验需要消耗大量成本。此时通过使用代理模型的演化计算方法来减少昂贵问题上使用演化计算时间成本。</p><p>代理模型往往和真实的 fitness function 一起使用，以防演化计算被误引入代理模型提供的错误最优。当问题越高维度，代理模型的构建难度就越大。</p><h2 id="代理模型的策略"><a href="#代理模型的策略" class="headerlink" title="代理模型的策略"></a>代理模型的策略</h2><blockquote><p>No analytical fitness function exists for accurately evaluating the fitness of a candidate solution. Instead, there are only more accurate and less accurate fitness estimation methods, which often trade off accuracy with computational costs.</p></blockquote><p>使用代理模型也需要权衡模型的效率和保真度。最初，一部分工作完全依赖代理模型进行演化搜索，但是代理模型引入的可能并不存在的最优会带来严重的问题。代理模型几乎可以用在所有的演化计算步骤中来剔除差的结果，并且减少随机性：population initialization, cross-over, mutation, local search and fitness evaluations。</p><p>根据代理模型的使用对象进行不同，可以将代理模型方法分类可以分为以下三类：</p><ul><li>Generalization based:<ul><li>surrogates are used for fitness evaluations in some of the generations, while in the rest of the generations, the real fitness function is used</li></ul></li><li>Individual based:<ul><li>the real-fitness function is used for fitness evaluations for some of the individuals in a generation</li></ul></li><li>Population based:<ul><li>more than one subpopulation co-evolves, each using its own surrogate for fitness evaluations. Migration of individuals from one sub-population to another is allowed.</li></ul></li></ul><hr><h2 id="单代理模型"><a href="#单代理模型" class="headerlink" title="单代理模型"></a>单代理模型</h2><p>我们假设与真实 fitness 函数交互费时，希望尽可能减少与真实函数交互。那么关键的问题就在于如何确定哪个个体是应该被重新评估的。我们需要考虑到以下三个方面。</p><h3 id="1-选取重评估样本的标准"><a href="#1-选取重评估样本的标准" class="headerlink" title="1. 选取重评估样本的标准"></a>1. 选取重评估样本的标准</h3><p>不得不说这些选取的方式和<strong>主动学习</strong>极度相似。选取具有以下特征的样本评估:</p><ul><li>potentially have a good fitness value</li><li>representative</li><li>large degree of uncertainty in approximation<ul><li>fitness landscape around these solutions has not been well explored</li><li>improve the approximation accuracy of the surrogate,</li></ul></li></ul><p>如何描述或者估计这种不确定性或者错误呢？</p><ul><li>uncertainty is roughly set to be inversely proportional to the average distance to the closest data samples</li><li>estimating the variance of the individual estimates given by an ensemble of surrogates</li></ul><h3 id="2-如何评估代理模型的好坏"><a href="#2-如何评估代理模型的好坏" class="headerlink" title="2. 如何评估代理模型的好坏"></a>2. 如何评估代理模型的好坏</h3><p>首先代理模型并不是需要严格和 fitness function 相同才可以发挥作用，在存在较大的预测错误时同样可以起到作用，如下图所示。</p><div style="width:90%;margin:auto"><img src="/paper-reading/surrogate/surrogate-error.png" class=""></div><p>一些常用的度量：</p><ul><li>mean squared error between the individual’s real fitness value and the predicted fitness</li><li>the number of individuals that have been selected correctly using the surrogate</li><li>the rank of the selected individuals, calculated based on the real fitness function.</li><li>rank correlation: measure for the monotonic relation between the ranks of two variables</li><li>continuous correlation between the surrogate and the original fitness function.</li></ul><h3 id="3-提升预测的准确性"><a href="#3-提升预测的准确性" class="headerlink" title="3. 提升预测的准确性"></a>3. 提升预测的准确性</h3><ul><li>神经网络的正则化</li><li>随着搜索更新模型</li><li>建立一个地位的搜索空间</li><li>评估时利用生成的中间数据</li></ul><hr><h2 id="多代理模型"><a href="#多代理模型" class="headerlink" title="多代理模型"></a>多代理模型</h2><p>模型类别可能不同，模型保真度（fidelity）可能不同。此处根据对 fidelity 的掌控把多代理模型分为两类，同质（homogeneous）和异质（heterogeneous）多代理模型。</p><blockquote><p>By homogeneous multiple surrogates, the fidelity of the surrogates are not explicitly controlled, even if different types of surrogates are used. </p><p>On the contrary, heterogeneous surrogates vary in their fidelity due to an explicit control in model complexity or training data.</p></blockquote><ul><li>同质（homogeneous）多代理模型<ul><li>多个模型可以提升预测质量，也可以帮助识别较大的预测错误。</li><li>其经验有效性在多篇工作中得到证实。</li></ul></li><li>异质（heterogeneous）多代理模型<ul><li>其实是组合了一些列不同粒度的代理模型。</li><li>这类工作提出的背景也是训练不同粒度的代理模型会有不同的成本，粒度越低成本越低。</li></ul></li></ul><hr><h2 id="在昂贵问题之外更多的考虑"><a href="#在昂贵问题之外更多的考虑" class="headerlink" title="在昂贵问题之外更多的考虑"></a>在昂贵问题之外更多的考虑</h2><ol><li>Surrogates in interactive evolutionary computation</li><li>Surrogated-assisted evolution for solving dynamic optimization</li><li>Surrogates for robust optimization</li><li>Surrogates for constrained optimization</li></ol><h2 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h2><p>代理模型的方法更多的是应用驱动。</p><blockquote><p>One intensively researched area is surrogate-assisted design optimization, such as turbine blades, airfoils, forging, vehicle crash tests, multi-processor systems-on-chip design and injection systems. Other applications include drug design, protein design, hydroinformatics and evolutionary robotics.</p></blockquote><h2 id="未来挑战"><a href="#未来挑战" class="headerlink" title="未来挑战"></a>未来挑战</h2><ol><li>Theoretic work</li><li>Multi-level, multi-fidelity heterogeneous surrogates</li><li>Surrogate-assisted combinatorial optimization</li><li>Surrogate-assisted dynamic optimization</li><li>Rigorous benchmarking and test problems</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文是对 Surrogate-assisted evolutionary computation: Recent advances and future challenges 这篇综述的阅读笔记。
这是一篇发表于2011年的关于在演化计算中如何应用辅助模型的综述。&lt;/p&gt;</summary>
    
    
    
    <category term="Paper Reading" scheme="https://superuier.github.io/categories/paper-reading/"/>
    
    
    <category term="EA" scheme="https://superuier.github.io/tags/EA/"/>
    
  </entry>
  
  <entry>
    <title>Vim 常用功能记录</title>
    <link href="https://superuier.github.io/programming/vim/"/>
    <id>https://superuier.github.io/programming/vim/</id>
    <published>2021-08-18T07:05:14.000Z</published>
    <updated>2021-08-18T07:05:14.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>Vim 是一款文本编辑器。说来惭愧，自己在日常使用中并不熟练，于是此处记录一些有需求的功能，便于查阅。</p><a id="more"></a><h2 id="四种模式"><a href="#四种模式" class="headerlink" title="四种模式"></a>四种模式</h2><ul><li>正常模式：<code>&lt;ESC&gt;</code> </li><li>命令模式：<code>:</code> or <code>/</code></li><li>插入模式：<code>i</code> or <code>a</code> or <code>o</code>（new line）</li><li>可视模式：<code>v, V, &lt;Ctrl&gt;+v</code></li></ul><h2 id="正常模式下常用命令"><a href="#正常模式下常用命令" class="headerlink" title="正常模式下常用命令"></a>正常模式下常用命令</h2><p>基本：</p><ul><li>保存退出：<code>:wq</code> or <code>ZZ</code></li><li>不保存退出：<code>:q!</code> or <code>:qa!</code></li><li>光标位置和文件信息：<code>&lt;Ctrl&gt;+g</code></li><li>显示及取消显示行号：<code>:set nu</code> and <code>:set nonu</code></li><li>定位到 n 行：<code>:n</code></li><li>翻页：<code>ctrl+f</code>（下一页） <code>ctrl+b</code>（上一页）</li></ul><p>搜索：</p><ul><li>搜索：<code>/+&lt;content&gt;</code></li><li>继续搜索：<code>n</code>（向下） or <code>N</code>（向上）</li><li>跳转至从哪里来：<code>&lt;Ctrl&gt;+o</code>（向后） or <code>&lt;Ctrl&gt;+i</code>（向前）</li><li>对应符号的跳转（例如括号）：<code>%</code></li></ul><h2 id="正常模式下编辑"><a href="#正常模式下编辑" class="headerlink" title="正常模式下编辑"></a>正常模式下编辑</h2><p>插入或更改：</p><ul><li>插入模式：<code>i</code> or <code>a</code> or <code>o</code>（新一行）</li><li>替换字符：<code>r+&lt;char&gt;</code> or <code>R+&lt;char&gt;</code>（多个字符）</li></ul><p>删除：</p><ul><li>删除当前字符：<code>x</code></li><li>删除当前单词：<code>dw</code></li><li>删除2个单词：<code>d2w</code></li><li>删除当前行：<code>dd</code></li><li>删除2行：<code>2dd</code> </li><li>删除到当前行尾：<code>d$</code></li></ul><p>撤销删除</p><ul><li>撤销：<code>u</code>（单次） or <code>U</code>（整行）</li><li>整行重做：<code>&lt;Ctrl&gt;+R</code></li></ul><p>复制粘贴：</p><ul><li>复制：<code>y</code>（在可视模式下选中）</li><li>粘贴内容：<code>p</code>（）</li></ul><p>替换文本：</p><ul><li>To substitute new for the first old in a line type    <code>:s/old/new</code></li><li>To substitute new for all ‘old’s on a line type       <code>:s/old/new/g</code></li><li>To substitute phrases between two line #’s type       <code>:#,#s/old/new/g</code></li><li>To substitute all occurrences in the file type        <code>:%s/old/new/g</code></li><li>To ask for confirmation each time add ‘c’             <code>:%s/old/new/gc</code></li></ul><h2 id="移动光标"><a href="#移动光标" class="headerlink" title="移动光标"></a>移动光标</h2><p>行内移动光标：</p><ul><li>向前移动两个单词至词首：<code>2w</code></li><li>向前移动两个单词至词尾：<code>2e</code></li><li>移动到行首：<code>0</code></li><li>移动到行尾：<code>$</code></li></ul><p>跨行移动图标（显示什么内容）：</p><ul><li>光标定位到第 n 行的行首 <code>nG</code></li><li>光标定位到第一行的行首 <code>gg</code></li><li>光标定位到最后一行的行首 <code>G</code></li><li>光标定位到当前屏幕的第一行行首 <code>H</code></li><li>光标移动到当前屏幕的中间 <code>M</code></li><li>光标移动到当前屏幕的尾部 <code>L</code></li><li>把当前行移动到当前屏幕的最上方，也就是第一行 <code>zt</code></li><li>把当前行移动到当前屏幕的中间 <code>zz</code></li><li>把当前行移动到当前屏幕的尾部 <code>zb</code></li></ul><h2 id="其他功能"><a href="#其他功能" class="headerlink" title="其他功能"></a>其他功能</h2><p>大小写转换</p><ul><li>将光标下的字母改变大小写 <code>~</code></li><li>将光标位置开始的3个字母改变其大小写 <code>3~</code></li><li>改变当前行字母的大小写 <code>g~~</code></li><li>将当前行的字母改成大写 <code>gUU</code></li><li>将当前行的字母全改成小写 <code>guu</code></li><li>将从光标开始到下面3行字母改成大写 <code>3gUU</code></li><li>将光标下的单词改成大写。 <code>gUw</code></li><li>将光标下的单词改成小写 <code>guw</code></li></ul>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;Vim 是一款文本编辑器。
说来惭愧，自己在日常使用中并不熟练，于是此处记录一些有需求的功能，便于查阅。&lt;/p&gt;</summary>
    
    
    
    <category term="Programming" scheme="https://superuier.github.io/categories/programming/"/>
    
    
    <category term="Linux" scheme="https://superuier.github.io/tags/Linux/"/>
    
    <category term="Vim" scheme="https://superuier.github.io/tags/Vim/"/>
    
  </entry>
  
  <entry>
    <title>深度主动学习批判文章阅读</title>
    <link href="https://superuier.github.io/paper-reading/deep-AL-criticism/"/>
    <id>https://superuier.github.io/paper-reading/deep-AL-criticism/</id>
    <published>2021-08-12T06:51:10.000Z</published>
    <updated>2021-08-12T06:51:10.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>目前出现一些对深度主动学习批判的文章，结合自己的实践，深以为然，此处将其整理一下。同时这些文章也收录进了本人 <code>awesome-active-learning</code> 的仓库，详见<a href="https://github.com/SupeRuier/awesome-active-learning/blob/master/subfields/deep_AL.md">此链接</a>。在本文末尾，本人也提出了一些自己的看法。</p><a id="more"></a><p>文章列表：</p><ul><li>Parting with Illusions about Deep Active Learning [2019, Arxiv]</li><li>Towards Robust and Reproducible Active Learning Using Neural Networks [2020, Arxiv]</li><li>Effective Evaluation of Deep Active Learning on Image Classification Tasks [2021, Open Review]</li></ul><hr><h1 id="文献概览"><a href="#文献概览" class="headerlink" title="文献概览"></a>文献概览</h1><h2 id="1-Parting-with-Illusions-about-Deep-Active-Learning-2019-Arxiv"><a href="#1-Parting-with-Illusions-about-Deep-Active-Learning-2019-Arxiv" class="headerlink" title="1. Parting with Illusions about Deep Active Learning [2019, Arxiv]"></a>1. Parting with Illusions about Deep Active Learning <a href="https://arxiv.org/abs/1912.05361">[2019, Arxiv]</a></h2><p>这篇文章指出当前主动学习没有考虑到半监督学习及数据增强等平行设定，于是他们在图像分类和语义分割上做了一个对比实验。实验中对比到的内容包括：多种 SOTA 主动学习策略、加入数据增强模块的主动学习策略、加入半监督学习方法的主动学习策略、半监督学习策略。</p><p>分类任务的结果:</p><blockquote><ul><li>主动学习加入数据增强可以带来提升，但是这会模糊不同主动学习策略间的区别，他们表现都大致相同。</li><li>主动学习与半监督学习结合可以带来比单独使用主动学习和半监督学习都要好的表现。</li><li>不同主动学习策略的优劣排序在不同数据集上不同。</li><li>在样本数量极少时，主动选取的样本表现差于随机选取，包括在半监督学习上的随机选取。</li></ul></blockquote><p>语义分割的结果：</p><blockquote><ul><li>半监督学习加随机选取表现最好。</li></ul></blockquote><p>总结:</p><ul><li>当前用于评估主动学习的模式并不好，会带来错误的结论。</li><li>半监督学习的加入会大幅提升主动学习表现。</li><li>很多 SOTA 的主动学习方法会比随机选取要差，尤其是当标记样本数很小时。</li></ul><h2 id="2-Towards-Robust-and-Reproducible-Active-Learning-Using-Neural-Networks-2020-Arxiv"><a href="#2-Towards-Robust-and-Reproducible-Active-Learning-Using-Neural-Networks-2020-Arxiv" class="headerlink" title="2. Towards Robust and Reproducible Active Learning Using Neural Networks [2020, Arxiv]"></a>2. Towards Robust and Reproducible Active Learning Using Neural Networks <a href="https://arxiv.org/abs/2002.09564">[2020, Arxiv]</a></h2><p>这篇文章指出在不同的主动学习文章中，作为基准的随机选取策略表现飘忽不定。为了提高主动学习工作的可复现度和鲁棒性，这里对当前方法做一个公平的对比。</p><p>同时本文指出，当前主动学习方法大多忽视了正则化对减小泛化误差的作用。所以作者也将不同的正则化项 (parameter norm penalty, random augmentation (RA), stochastic weighted averaging (SWA), and shake-shake (SS)) 加入了对比。</p><p>图像分类任务的结果:</p><blockquote><ul><li>相比各作者在原文中提到的结果，这里随机选取的表现都要比他们宣称的更好。同时没有策略可以明显超过随机选取。</li><li>使用不同的主动学习批选取数目，得到的表现不一致。</li><li>主动学习方法没有超越随机选取，且在类别不平衡设定上表现不鲁棒。</li><li>加入了 RA 和 SWA 的正则化，主动学习表现明显提升，同时不同策略间表现差异减小。</li><li>把选取的样本迁移到不同的模型结构时，不同策略表现不同，但是随机选取表现依然占优。</li></ul></blockquote><p>讨论：</p><blockquote><ul><li>基于这种相同 baseline 表现不一致的情况，强调主动学习的对比应当在一系列限定条件下施行。</li><li>不同于 <code>Parting with Illusions about Deep Active Learning</code> 这篇文章，作者认为<ul><li>主动学习的表现提升仅在有限的的实验条件下才会出现</li><li>这种基于随机选取的提升不具有统计学意义</li><li>在加入正则化之后，这些对于随机选取策略的提升消失。</li></ul></li><li>正则化在小样本情况下带来的提升可观</li><li>对于一些在模型中使用到未标记样本训练的主动学习策略，例如 <code>VAAL</code> 更应该使用半监督学习来作为基线方法。</li><li>相信当前主动学习的工作都是基于未正则化的模型，其表现提升不能说是由于样本选取的好。</li></ul></blockquote><p>提出了一个评估的标准：</p><blockquote><ul><li>策略要在不同优化器、模型结构、批选取大小等上保证鲁棒。</li><li>模型训练时应当加入 RA 或 SWA 这类的正则化。</li><li>迁移测试必须进行，用来保证选取的样本的确是高质量的。</li><li>实验应该使用统一的评估平台</li><li>实验过程的快照应该留存发布</li><li>训练、测试、标记、未标记、选取样本的编码应该被分享。</li></ul></blockquote><h2 id="3-Effective-Evaluation-of-Deep-Active-Learning-on-Image-Classification-Tasks-2021-Open-Review"><a href="#3-Effective-Evaluation-of-Deep-Active-Learning-on-Image-Classification-Tasks-2021-Open-Review" class="headerlink" title="3. Effective Evaluation of Deep Active Learning on Image Classification Tasks [2021, Open Review]"></a>3. Effective Evaluation of Deep Active Learning on Image Classification Tasks <a href="https://arxiv.org/abs/2106.15324">[2021, Open Review]</a></h2><p><strong>个人认为这篇文章十分全面且实验到位。</strong>本文指出了当前 AL 工作的一些问题：</p><blockquote><ol><li>不同 AL 策略在不同的工作中往往存在矛盾的表现</li><li>非故意的排除一些用于深度学习的重要一般化步骤，例如数据增强和 SGD 优化。</li><li>对于标记效率等评估方式缺乏认知</li><li>对 AL 在什么情形下能超过随机选取没有清晰的认知</li></ol></blockquote><p>作者展示了一个统一的对于 SOTA 主动学习策略在图像分类下的复现，且在几个不同方面做出了分析。他们指出了一些重要但是目前不清楚的细节：</p><blockquote><ol><li>模型在每个轮循环之后需要从头训练还是可以 fine-tuning？</li><li>使用精心选取的初始集对 AL 有何影响。</li><li>有没有一个完美的 AL batch size？或者这个 AL batch size 重要吗？</li><li>什么情形下 AL 可以超越随机选取？什么因素对其产生影响？</li><li>AL 的 scalability 如何？具体的，模型的训练和 AL 的选取各需要多少时间？</li></ol></blockquote><p>以下是他们的重要结论和实用的经验：</p><blockquote><ol><li>数据增强对测试集表现以及标记效率能带来提升。</li><li>SGD 表现要比 Adam 表现好。</li><li>在使用数据增强和 SGD 优化器时，Diversity 选取策略难以比简单的 Uncertainty 选取表现好。</li><li>在加入人工制造的数据冗余时，BADGE 开始表现好于 Uncertainty 选取。</li><li>每个类别中未标记样本的数量对于表现有很大影响：每个类别中样本数越少，对于 AL 的提升空间越小。</li><li>初始标记数据的选取在几轮循环后对 AL 表现将几乎没有影响。</li><li>AL batch size 同样对表现几乎没有影响。</li><li>在每轮循环中，重训练模型或者 fine-tune 模型，只影响最开始几轮选取的表现。</li><li>最费时的步骤是每轮循环中模型的重训练。</li></ol></blockquote><hr><h1 id="总结、讨论与个人看法"><a href="#总结、讨论与个人看法" class="headerlink" title="总结、讨论与个人看法"></a>总结、讨论与个人看法</h1><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这三篇文章都是对当前深度主动学习的批判与调研文，个人很钦佩这种刨根问底的态度。首先这三篇文章有一些共性的结论：</p><ol><li>数据增强可以带来性能提升 [1,3]</li><li>半监督学习，正则化可以带来性能提升 [1,2]（个人看法，某些类的半监督学习方法可以看作正则化项，于是这里放在一起）</li><li>在加入适用于深度学习的提升方法后，不同 AL 策略间的优劣程度变得模糊 [1,2,3]</li><li>[1,2] 认为应当将半监督学习作为基线方法。</li></ol><p>当然也有一些互相排斥的结论：</p><ol><li>[2] 认为加入正则化之后，主动学习相对于随机选取的提升消失，不具有统计意义，但是 [1,3] 中数据增强后仍相对随机选取有提升。</li></ol><h2 id="个人看法"><a href="#个人看法" class="headerlink" title="个人看法"></a>个人看法</h2><p>就这三篇文章而言，背景是当前一大部分的深度主动学习仅考虑了<strong>主动学习的范畴</strong>而没有更多考虑<strong>深度学习的问题</strong>。</p><p>仅从主动学习范畴而言，这二十多年来，Pool-Based AL 的架构，评估方式都没有发生过太大变化：</p><ul><li>全监督式学习</li><li>循环架构并每次从头训练</li><li>通过学习曲线评估不同策略</li><li>以随机选取样本的监督式学习模型作为最基础的基线</li></ul><p>如果从传统模型来考虑，这个主动学习的范畴其实没有太大问题。但是近十年来，深度学习兴起，其实为主动学习带来了新的实现环境。之前的有关深度主动学习的调研（见<a href="/paper-reading/deep-AL-survey/" title="这篇笔记">这篇笔记</a>），其实已经指出了很多在深度模型中使用主动学习面临的挑战。其中第一条就是主动学习期望选取少量重要标记样本，而深度学习期望大量训练样本，两者之间有着矛盾。所以在深度学习的框架下，不能仅仅考虑主动学习的范畴，也要结合深度学习的情况。</p><p>深度学习在这十年的发展中，其实也苦于标记样本数目稀少，但是在深度学习的社区中，采用了一些其他的方式来解决此问题，并取得了十分不错的效果：</p><ul><li>数据增强</li><li>利用未标记的样本<ul><li>半监督学习</li><li>自监督学习</li></ul></li><li>预训练</li></ul><p>所以目前来看，主动学习的范畴致力于用少量的标记样本达到与随机选取相同的表现，深度学习的这些方法致力于用已有的标记数据达到更好的表现。这两种角度其实是一件事情，放在主动学习评估的学习曲线中，就是横向比较和纵向比较。其实在评估时，绝大部分的主动学习工作，都是宣称前者但是采纳后者。换言之就是更侧重于纵向的比较（当然横向好就会带来纵向好毋庸置疑）。</p><p>在这种情况下，<strong>许多深度主动学习仍然使用传统主动学习范畴就很不公平了</strong>。根本原因在于比较的基线方法是一个明明能获取大量未标记样本却只在少量标记样本上训练的模型。换句话说，当前的很多深度主动学习方法还是在虐菜，虐那些没有使用先进的深度学习方法且仅在少量标记样本上训练的神经网络的菜。再打个比方说，主动学习可以看作一种样本维度的提升方式，当前的很大一部分深度主动学习是在对像刀剑一样的武器上进行磨刀抛光提升，表明我磨的比你好。殊不知现在神经网络经过多年社区的贡献已经变成了诸葛神弩或者是火枪。所以说现在仍基于对刀剑提升的作为主动学习的评估已经过时了，磨好的刀是比不过未经提升的火枪的。只有在火枪上进行提升，才可能是当前深度主动学习需要考虑的方向，才是能带来实用价值的方向。正式的说，就是<strong>如何在当前深度学习这些已经验证好用的通用方法下，进一步使用主动学习减少标记成本</strong>。</p><p>目前来看，在策略设计之外，我们需要对主动学习的框架来进行调整，这些调整和主动学习策略的设计可以是正交的，并不一定互相影响：</p><ol><li>首先比较的基线需要是随机选取样本训练的，纳入很多先进成熟深度学习方法的神经网络，包括但不限于：半监督，自监督，正则化，数据增强等方法。</li><li>模型的训练可能需要重训练和增强训练交替进行，而不是一味的重训练（考虑到相较于经典方法来说，神经网络训练时间较长。）</li><li>评估的方式不能单纯以学习曲线论，labeled efficiency 也是一个更直观的角度。</li></ol>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;目前出现一些对深度主动学习批判的文章，结合自己的实践，深以为然，此处将其整理一下。
同时这些文章也收录进了本人 &lt;code&gt;awesome-active-learning&lt;/code&gt; 的仓库，详见&lt;a href=&quot;https://github.com/SupeRuier/awesome-active-learning/blob/master/subfields/deep_AL.md&quot;&gt;此链接&lt;/a&gt;。
在本文末尾，本人也提出了一些自己的看法。&lt;/p&gt;</summary>
    
    
    
    <category term="Paper Reading" scheme="https://superuier.github.io/categories/paper-reading/"/>
    
    
    <category term="active-learning" scheme="https://superuier.github.io/tags/active-learning/"/>
    
  </entry>
  
  <entry>
    <title>Prompt 学习记录</title>
    <link href="https://superuier.github.io/machine-learning/prompt/"/>
    <id>https://superuier.github.io/machine-learning/prompt/</id>
    <published>2021-08-11T09:02:00.000Z</published>
    <updated>2021-08-11T09:02:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>在自然语言处理中有一个叫做 prompt 的新范式最近较火，其背景是在少标记的场景下学习。本文主要内容都是从 <a href="https://arxiv.org/pdf/2107.13586.pdf">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</a> 这篇综述中提取。仅涵盖本人认为最需要被科普的内容。</p><a id="more"></a><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>当前在自然语言处理中主要存在以下范式，其中由 a 到 d 基本按照时间顺序出现。总的来说，目前为止经历了两个 sea changes（重大变化）。</p><blockquote><div style="width:100%;margin:auto"><img src="/machine-learning/prompt/paradigm.png" class=""></div></blockquote><p>在2017年以前，主要以完全监督学习为主（a和b范式）。研究的主要内容在于特征提取（传统模型），结构构建（深度模型）。但是在2017年之后，经历了第一个重大变化，完全监督的旧范式的使用不断在缩小，预训练及微调（c范式）开始流行。在当前时间节点，2021年，正在经历第二个重大变化。当前对于下游任务学习并不是通过预训练及微调中的 objective engineering，而是通过 prompt（提示）来实现。</p><p>这里举一个 prompt 的例子。</p><blockquote><p>When recognizing the emotion of a social media post, “I missed the bus today.”, we may continue with a prompt “I felt so __”, and ask the LM to ﬁll the blank with an emotion-bearing word. </p><p>Or if we choose the prompt “English: I missed the bus today. French: __”), an LM may be able to ﬁll in the blank with a French translation.</p></blockquote><p>通过这种选取合适的 prompt 的方法，预训练的 language model（LM）可以用来预测合适的输出，有时甚至不需要 task-specific 的训练。</p><h1 id="Prompt-的正式表述"><a href="#Prompt-的正式表述" class="headerlink" title="Prompt 的正式表述"></a>Prompt 的正式表述</h1><p>基于 prompt 的方法尝试规避无法获得大规模数据的问题，直接对样本 $\boldsymbol{x}$ 的概率 $P(\boldsymbol{x};\theta)$ 进行建模，之后再用这个概率来预测$\boldsymbol{y}$。以下是一些基于 prompt 的方法中的术语。</p><blockquote><div style="width:100%;margin:auto"><img src="/machine-learning/prompt/terminology.png" class=""></div></blockquote><p>通常来说，prompt 的方法预测高质量的输出 $\hat{\boldsymbol{y}}$ 有三步。</p><ol><li>Prompt Addition：通过模版，将原始语句转化为 Prompt，含有空白等待填入。</li><li>Answer Search：在候选集$\mathcal{Z}$中，选取 answer prompt $\hat{\boldsymbol{z}}=\operatorname{search}_{\boldsymbol{z} \in \mathcal{Z}} P\left(f_{\mathrm{fill}}\left(\boldsymbol{x}^{\prime}, \boldsymbol{z}\right) ; \theta\right)$。</li><li>Answer Mapping：将高分的回答 $\boldsymbol{z}$ 和高分的输出 $\hat{\boldsymbol{y}}$ 对应起来。</li></ol><h2 id="设计-Prompt-时需要考虑的具体问题"><a href="#设计-Prompt-时需要考虑的具体问题" class="headerlink" title="设计 Prompt 时需要考虑的具体问题"></a>设计 Prompt 时需要考虑的具体问题</h2><p>这里一般来说存在以下5个需要具体考虑的问题：</p><ol><li>如何选择预训练模型</li><li>选择何种 Prompt 来作为 Prompting funtion（模版）。</li><li>设计候选集$\mathcal{Z}$，可能同时还需要考虑与输出的映射。</li><li>对简单框架的扩展以提高表现和适用性。</li><li>训练参数的策略</li></ol><p>此处不一一展开，可以到综述中寻找具体的部分。</p><h2 id="Prompt-的应用和挑战"><a href="#Prompt-的应用和挑战" class="headerlink" title="Prompt 的应用和挑战"></a>Prompt 的应用和挑战</h2><p>这篇综述也详尽的展开了当前 Prompt 的应用和挑战。这里只简单记录不做展开。</p><p>应用方面，几乎涉及了 NLP 的方方面面，集中于以下几大类：Knowledge Probing、Classiﬁcation-based Tasks、Information Extraction、“Reasoning” in NLP、Question Answering、Text Generation、Automatic Evaluation of Text Generation、Multi-modal Learning、Meta-Applications。</p><p>挑战方面，主要集中于以下几个大类：Prompt Design、Answer Engineering、Selection of Tuning Strategy、Multiple Prompt Learning、Selection of Pre-trained Models、Theoretical and Empirical Analysis of Prompting、Transferability of Prompts、Combination of Different Paradigms、Calibration of Prompting Methods。</p><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><ul><li><a href="http://pretrain.nlpedia.ai">Pretrain, Prompt, Predict: A New Paradigm for NLP</a></li><li><a href="https://arxiv.org/pdf/2107.13586.pdf">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</a> </li><li><a href="https://github.com/thunlp/PromptPapers">PromptPapers</a></li></ul><h1 id="对这个范式的看法"><a href="#对这个范式的看法" class="headerlink" title="对这个范式的看法"></a>对这个范式的看法</h1><p>Prompt 方法归根结底还是面向标记数据缺乏这一老生常谈的问题。</p><p>从二十多年前的经典方法开始，大家就在对这一问题展开研究。从模型角度入手似乎还是比较少见，主要还是在学习范式的角度上进行研究。从最开始的监督学习，到半监督学习，还有迁移学习，都是致力于使用有限的标记来最大化任务表现。近几年这种半监督，或者说自（无）监督的方法已经极大程度上融进了目前机器学习的框架中。有时作为一种特征提取器而存在，有时作为一种正则化而存在。从这一角度来看，当前的几种范式，都是在探讨如何最大化利用未标记样本，Prompt 也不例外。</p><h2 id="对于-Prompt-而言"><a href="#对于-Prompt-而言" class="headerlink" title="对于 Prompt 而言"></a>对于 Prompt 而言</h2><p>在我来看 Prompt 的定位应该是一种较为成熟的广泛适用于文本的自监督学习方法。自监督学习关键在于定义一种可以自己知道正确答案的任务，用此任务来训练，以得到对该任务的较好表现，同时也可以获取质量相对较好的特征以适用于下游任务。</p><p>当前的预训练模型大多都是由 mask 这类操作来自监督训练。所以在预训练模型上学习的填空能力是可以很好的用于 Prompt 定义的填空题。由于这种填词题在训练的时候已经见过，不像很多下游任务还需要知道之前没见过的标签，所以预训练模型在寻找 answered prompt 时可以 zero-shot 或者 few-shot。</p><p>总的来说，这种范式设计了一种能“引诱”模型输出之前在大量样本中学到的统计（/逻辑/推断）数据。个人觉得还是蛮有意思，或许可以用来对模型进行解释。</p><h2 id="对于自己主动学习的研究而言"><a href="#对于自己主动学习的研究而言" class="headerlink" title="对于自己主动学习的研究而言"></a>对于自己主动学习的研究而言</h2><p>主动学习和这些范式其实不太相同，上述半监督自监督学习主要考虑如何利用未标记样本，而主动学习是在探讨如何最大化利用有限的标记成本来选取最有价值的样本。其已假设更重要的样本能学出来相对随机选取的样本更好的模型。</p><p>在当前无监督自监督学习表现如此之好的情况下，单纯使用主动学习得到的标记样本意义似乎不大。因为单纯选取少量重要的标记样本可能仍旧难以与大量未标记样本上的自监督匹敌。这就是那两篇主动学习劝退文里面指出的问题。所以说个人认为主动学习中，尤其是模型的训练部分，为了最大化效用，应该是一定是要使用未标记样本的。</p><p>主动学习中存在标记，那么必然是一个下游任务。那么具体如何结合少量标记样本和大量未标记样本来训练，就是其他那几种范式。在其他范式上学到的特征提取器上对于下游进行微调，可能才是主动学习最好的实施方法。具体如何微调，又是一个研究了很久的问题。</p><p>所以这种无机的结合可能才是主动学习的实际场景。</p>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;在自然语言处理中有一个叫做 prompt 的新范式最近较火，其背景是在少标记的场景下学习。
本文主要内容都是从 &lt;a href=&quot;https://arxiv.org/pdf/2107.13586.pdf&quot;&gt;Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing&lt;/a&gt; 这篇综述中提取。
仅涵盖本人认为最需要被科普的内容。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://superuier.github.io/categories/Machine-Learning/"/>
    
    
    <category term="machine-learning" scheme="https://superuier.github.io/tags/machine-learning/"/>
    
    <category term="nlp" scheme="https://superuier.github.io/tags/nlp/"/>
    
    <category term="few-shot-learning" scheme="https://superuier.github.io/tags/few-shot-learning/"/>
    
  </entry>
  
  <entry>
    <title>Numpy 相关</title>
    <link href="https://superuier.github.io/programming/numpy/"/>
    <id>https://superuier.github.io/programming/numpy/</id>
    <published>2021-08-11T06:07:55.000Z</published>
    <updated>2021-08-11T06:07:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>一些在使用 Numpy 时需要注意的东西</p><a id="more"></a><h2 id="随机数生成"><a href="#随机数生成" class="headerlink" title="随机数生成"></a>随机数生成</h2><p>为保证实验结果可复现，一般我们使用 <code>np.random.seed(number)</code> 来固定随机种子，之后可保证调用随机数生成器产生的结果相同。</p><p>但是在项目规模较大，且需要导入其他包时，这种固定随机种子的办法可能会出现一定问题。原因在于其他的包中，可能同样会设定其他的全局随机种子 <code>np.random.seed(other_number)</code>。导致之后生成的样本不是按照自己设定的随机种子来生成。</p><blockquote><p>“The preferred best practice for getting reproducible pseudorandom numbers is to instantiate a generator object with a seed and pass it around” — Robert Kern, <a href="https://numpy.org/neps/nep-0019-rng-policy.html">NEP19</a>.</p></blockquote><p>解决方法是定义一个随机数生成器，并将其传送到需要使用随机数的地方。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rng = np.random.default_rng(<span class="number">2021</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rng.random(<span class="number">4</span>)</span><br><span class="line">array([<span class="number">0.75694783</span>, <span class="number">0.94138187</span>, <span class="number">0.59246304</span>, <span class="number">0.31884171</span>])</span><br></pre></td></tr></table></figure><p>Reference:</p><ul><li><a href="https://towardsdatascience.com/stop-using-numpy-random-seed-581a9972805f">Stop using numpy.random.seed()</a></li><li><a href="https://numpy.org/neps/nep-0019-rng-policy.html">NEP 19 — Random Number Generator Policy</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;一些在使用 Numpy 时需要注意的东西&lt;/p&gt;</summary>
    
    
    
    <category term="Programming" scheme="https://superuier.github.io/categories/programming/"/>
    
    
    <category term="Python" scheme="https://superuier.github.io/tags/Python/"/>
    
    <category term="numpy" scheme="https://superuier.github.io/tags/numpy/"/>
    
  </entry>
  
  <entry>
    <title>早停与验证集损失</title>
    <link href="https://superuier.github.io/programming/early-stopping/"/>
    <id>https://superuier.github.io/programming/early-stopping/</id>
    <published>2021-08-10T06:32:55.000Z</published>
    <updated>2021-08-10T06:32:55.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>一些有关神经网络训练早停及验证集损失的记录。实验中发现验证集损失与其准确度不严格单调负相关，所以搜索一下答案。</p><a id="more"></a><h2 id="早停"><a href="#早停" class="headerlink" title="早停"></a>早停</h2><p>训练神经网络时，随着训练迭代次数的增加，训练损失会下降。而验证集损失一般会经历一个先下降后上升的过程，上升时则为过拟合出现。在此情况下，一般选用早停来结束当前模型的训练。</p><p>对于某种评估指标，假设其越低越好（/越高越好），在训练过程中，如果在一定数量的 epoch 的容忍度之内，其没有比之前的最好评估要低（/高），则停止训练，并重载之前表现最好时的模型参数作为最终的模型输出。</p><p>一般的，我们使用在验证集上的损失来作为早停的依据，如果验证集损失不进一步下降，那么则停止训练。（这也是目前来看大多数人用的依据。）但是早停的依据到底是使用验证集上的 loss 还是 accuracy（或者其他相应的 metric）并没有看到一个统一的说法，互联网上两派的支持者有之。</p><p>对于这种情况，George 在其<a href="http://alexadam.ca/ml/2018/08/03/early-stopping.html">博文</a>中指出了早停的一些问题。其中最重要的一点是，验证集准确度并不随着验证集损失的减小而严格单调递增，如下图所示。</p><div style="width:70%;margin:auto"></div><p>所以之后 George 实现了一种双重标准的早停，只有当损失和准确率都变糟糕时才早停。</p><h2 id="验证集损失与准确率"><a href="#验证集损失与准确率" class="headerlink" title="验证集损失与准确率"></a>验证集损失与准确率</h2><p>一般来说，验证集损失与准确率呈负相关关系。但是有些时候，存在验证集损失上升，但是验证集准确性仍进一步提高的情况。</p><p>这出现于模型对于预测过于自信和极端的情况。换句话说，模型倾向于输出较为极端的预测值，这样使得在同样数量分错的情况下，少数预测错的样本主导了 loss，但是对整体准确率影响不大。</p><p>以下是一些可能出现的原因：</p><ul><li>训练集验证集数据分布不一致</li><li>训练集过小未包含验证集所有情况</li></ul><p>Github 上也有一个 <a href="https://github.com/thegregyang/LossUpAccUp">Repo</a> 来讨论此现象。对于解决方法来说似乎没有一个明确的结论。作者在尝试了较小模型和批正则化后仍然会出现此问题。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="http://alexadam.ca/ml/2018/08/03/early-stopping.html">Early Stopping and its Faults</a></li><li><a href="https://www.zhihu.com/question/318399418/answer/1202932315">验证集loss上升，准确率却上升该如何理解？ - 刘国洋的回答 - 知乎</a></li><li><a href="https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy">https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;一些有关神经网络训练早停及验证集损失的记录。
实验中发现验证集损失与其准确度不严格单调负相关，所以搜索一下答案。&lt;/p&gt;</summary>
    
    
    
    <category term="Programming" scheme="https://superuier.github.io/categories/programming/"/>
    
    
    <category term="neural-network" scheme="https://superuier.github.io/tags/neural-network/"/>
    
    <category term="machine-learning" scheme="https://superuier.github.io/tags/machine-learning/"/>
    
    <category term="loss" scheme="https://superuier.github.io/tags/loss/"/>
    
  </entry>
  
  <entry>
    <title>交叉熵小结</title>
    <link href="https://superuier.github.io/machine-learning/cross-entropy/"/>
    <id>https://superuier.github.io/machine-learning/cross-entropy/</id>
    <published>2021-08-09T13:54:55.000Z</published>
    <updated>2021-08-09T13:54:55.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>这里对机器学习中常用到的交叉熵做一个总结。</p><a id="more"></a><h2 id="信息论中的交叉熵"><a href="#信息论中的交叉熵" class="headerlink" title="信息论中的交叉熵"></a>信息论中的交叉熵</h2><p>首先我们要定义一下信息熵。</p><script type="math/tex; mode=display">H(X)=\mathbb{E}_{X}[I(x)]=\sum_{x \in \mathcal{X}} - p(x) \log _{2}\left({p(x)}\right)</script><p>其中$\mathcal{X}$为有限个事件$x$的集合，$X$是定义在$\mathcal{X}$上的随机变量。信息熵是随机事件不确定性的度量，也可以看作是信息量的期望。同时，信息熵是信源编码定理中，压缩率的下限。当我们用少于信息熵的资讯量做编码，那么我们一定有资讯的损失。</p><p>交叉熵也是信息论中的一个概念。典型情况下，$p$ 表示数据的真实分布，$q$ 表示数据的理论分布、估计的模型分布、或$p$的近似分布。在此基础上定义交叉熵，</p><script type="math/tex; mode=display">H(p, q)=\mathrm{E}_{p}[-\log q]=H(p)+D_{\mathrm{KL}}(p \| q)</script><p>其中 $H(p)$ 是 $p$ 的熵， $D_{\mathrm{KL}}(p | q)$ 是从 $p$ 与 $q$ 的KL散度(也被称为 $p$ 相对于 $q$ 的相对熵。</p><script type="math/tex; mode=display">D_{\mathrm{KL}}(p \| q)=-\sum_{x} p(x) \ln \frac{q(x)}{p(x)}</script><p>KL散度是两个概率分布 $p$ 和 $Q$ 差别的非对称性的度量。其用来度量使用基于 $Q$ 的分布来编码服从 $p$ 的分布的样本所需的额外的平均比特数。</p><p>所以对于离散分布 $p$ 和 $q$, 这意味着其交叉熵为：</p><script type="math/tex; mode=display">H(p, q)=-\sum_{x} p(x) \log q(x)</script><p>基于相同事件测度的两个概率分布 $p$ 和 $q$ 的交叉熵指，当基于一个“非自然”（相对于“真实”分布 $p$ 而言）的概率分布 $q$ 进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（bit）。(说人话就是使用基于$q$的分布来编码服从$p$的分布的样本所需的平均比特数。)</p><h2 id="机器学习中的交叉熵损失"><a href="#机器学习中的交叉熵损失" class="headerlink" title="机器学习中的交叉熵损失"></a>机器学习中的交叉熵损失</h2><p>机器学习中，常用交叉熵作为神经网络的损失函数。其形式为，</p><script type="math/tex; mode=display">loss = -\sum_{i=1}^{n} y \log(\hat{y}_{i})</script><p>其中$\hat{y}_{i}$是第 $i$ 个样本在不同类别上的概率，$y$ 是第 $i$ 个样本的真实标记（长度为 $c$ 的向量）。神经网络中这个概率常常使用 softmax 函数，或者 sigmoid 函数得到。</p><h2 id="Pytorch-中实现交叉熵损失"><a href="#Pytorch-中实现交叉熵损失" class="headerlink" title="Pytorch 中实现交叉熵损失"></a>Pytorch 中实现交叉熵损失</h2><p>在 pytorch 中有两种方式实现交叉熵损失。可以参考之前的<a href="/programming/neural-network-loss/" title="这篇笔记">这篇笔记</a>。可以调用<code>NLLLoss()</code>或<code>CrossEntropyLoss()</code>两个函数。两函数的不同点主要在于输入不同：</p><ul><li>最后一层全连接层的输出可以直接调用<code>CrossEntropyLoss()</code></li><li>对于<code>NLLLoss()</code>，要将最后一层全连接层的输出再通过一次<code>LogSoftmax()</code>计算才能调用。</li></ul><p>同时如果类别不平衡也可以在每一个类别上加上相应的权重。</p><h2 id="交叉熵的求导"><a href="#交叉熵的求导" class="headerlink" title="交叉熵的求导"></a>交叉熵的求导</h2><p>因为之前要的工作要手动计算损失函数反向传播时对最后一层参数的梯度，所以需要对交叉熵损失求导。此处我们只需要考虑对最后一层全连接层的输出求导即可，对参数可以之后再进一步求导。</p><p>先考虑一下正向的过程（在 n 个类别下使用 softmax 的情况），对于一个样本在最后一层全连接层的输出 $X = [x_1,…,x_n]$，我们考虑第 $i$ 个类，</p><script type="math/tex; mode=display">x_i\xrightarrow[i_{th} output]{LogSoftmax}- \ln (\frac{\exp (x_{i})}{\sum_{j=1}^{n} \exp (x_{j})}) \xrightarrow[]{Y}- y_i \ln (\frac{\exp (x_{i})}{\sum_{j=1}^{n} \exp (x_{j})})</script><p>当考虑所有类别时，一般是将最右端这一项加权平均或加权求和。此处我们假设权重相等，对所有类别相加，则可以得到总损失。</p><script type="math/tex; mode=display">loss = - \sum_{i=1}^{n}y_i \ln (\frac{\exp (x_{i})}{\sum_{j=1}^{n} \exp (x_{j})}) = - y_c \ln (\frac{\exp (x_{i})}{\sum_{j=1}^{n} \exp (x_{j})})</script><p>其中$y_c = 1$，指当前样本的真实标记是 $c$。</p><script type="math/tex; mode=display">loss = - \ln (\frac{\exp (x_{c})}{\sum_{j=1}^{n} \exp (x_{j})})  = - \ln(Softmax(x_c)) = - \ln(S(x_c))</script><p>此处进行求导，</p><script type="math/tex; mode=display">\frac{\partial loss}{\partial x_i} = - \frac{1}{S(x_c)} \frac{\partial S(x_c)}{\partial x_i}</script><p>而对于 softmax 函数求导需要分类讨论：</p><ul><li>当$i=c$时, $\frac{\partial S(x_c)}{\partial x_i} = S(x_c)(1-S(x_c))$</li><li>当$i\neq c$时, $\frac{\partial S(x_c)}{\partial x_i} = - S(x_i)S(x_c)$</li></ul><p>所以导数为：</p><ul><li>当$i=c$时, $\frac{\partial loss}{\partial x_i} = S(x_i)-1$</li><li>当$i\neq c$时, $\frac{\partial loss}{\partial x_i} = S(x_i)-0$</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://en.wikipedia.org/wiki/Cross_entropy">Cross entropy - From Wikipedia, the free encyclopedia</a></li><li><a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence">Kullback–Leibler divergence - From Wikipedia, the free encyclopedia</a></li><li><a href="https://en.wikipedia.org/wiki/Information_theory">Information theory - From Wikipedia, the free encyclopedia</a></li><li><a href="https://zhuanlan.zhihu.com/p/131647655">Softmax以及Cross Entropy Loss求导</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;这里对机器学习中常用到的交叉熵做一个总结。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://superuier.github.io/categories/Machine-Learning/"/>
    
    
    <category term="neural-network" scheme="https://superuier.github.io/tags/neural-network/"/>
    
    <category term="machine-learning" scheme="https://superuier.github.io/tags/machine-learning/"/>
    
    <category term="loss" scheme="https://superuier.github.io/tags/loss/"/>
    
  </entry>
  
  <entry>
    <title>神经网络损失函数</title>
    <link href="https://superuier.github.io/programming/neural-network-loss/"/>
    <id>https://superuier.github.io/programming/neural-network-loss/</id>
    <published>2021-07-30T07:16:55.000Z</published>
    <updated>2021-08-02T12:11:55.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>由于总是忘记一些 loss 的常用场景和区别，此处记录一些常用的神经网络 loss。</p><a id="more"></a><p>Pytorch <a href="https://pytorch.org/docs/stable/nn.html#loss-functions">官方文档</a>中有十余种 loss 函数，其中常用的主要是<code>CrossEntropyLoss</code>、<code>NLLLoss</code>、<code>MSELoss</code>等。这里仅先对这些常用 loss 展开。</p><h2 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h2><p>负对数损失，常用于分类任务。值得注意的是，这里的输入 <code>X</code> 需要已经包含对于相应类别的 log-probability。<strong>使用这个 loss 的时候需要在模型最后加入 <code>LogSoftmax</code> 层。</strong></p><script type="math/tex; mode=display">l(X,y) = L = \{l_1,...,l_N\}^\top, \\l_n = -w_{y_n}X_{n,y_n},</script><p>其中 <code>X</code> 是输入，<code>y</code> 是目标，<code>w</code> 是类别的权重，<code>N</code> 是 batch size。</p><p>具体的计算例子可以看<a href="https://blog.csdn.net/qq_22210253/article/details/85229988">这篇文章</a>。</p><h2 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h2><p>交叉墒损失，同样常用于分类任务。这个标准结合了 LogSoftmax 和 NLLLoss 两个部分。</p><p>首相介绍一下 LogSoftmax，其包含 log 函数和 softmax 函数。</p><script type="math/tex; mode=display">\operatorname{LogSoftmax}\left(x_{i}\right)=\log \left(\frac{\exp \left(x_{i}\right)}{\sum_{j} \exp \left(x_{j}\right)}\right)</script><p>对于交叉墒损失，值得注意的是，这里的输入 <code>X</code> 需要已经包含对每一个类，未经处理的 unformalized score。<strong>换句话说，使用这个 loss 的时候不需要在模型最后加入 <code>Softmax</code> 层。</strong></p><script type="math/tex; mode=display">\operatorname{loss}(x, \text { class })=-\log \left(\frac{\exp (x[\text { class }])}{\sum_{j} \exp (x[j])}\right)=-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)</script><p>对于加权的情况，</p><script type="math/tex; mode=display">\operatorname{loss}(x, \text { class })=\text { weight }[\text { class }]\left(-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)\right)</script><p>最终的 loss 需要加权平均得到，</p><script type="math/tex; mode=display">\operatorname{loss}(x, \text { class })=\text { weight }[\text { class }]\left(-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)\right)</script><p>其中 <code>x</code> 是输入，<code>class</code> 是目标类，<code>weight</code> 是类别的权重，<code>N</code> 是 batch size。</p>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;由于总是忘记一些 loss 的常用场景和区别，此处记录一些常用的神经网络 loss。&lt;/p&gt;</summary>
    
    
    
    <category term="Programming" scheme="https://superuier.github.io/categories/programming/"/>
    
    
    <category term="neural-network" scheme="https://superuier.github.io/tags/neural-network/"/>
    
    <category term="machine-learning" scheme="https://superuier.github.io/tags/machine-learning/"/>
    
    <category term="Python" scheme="https://superuier.github.io/tags/Python/"/>
    
    <category term="Pytorch" scheme="https://superuier.github.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>好文阅读转载</title>
    <link href="https://superuier.github.io/reading-note/article-sharing/"/>
    <id>https://superuier.github.io/reading-note/article-sharing/</id>
    <published>2021-07-28T05:22:47.000Z</published>
    <updated>2021-08-09T07:04:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>此处记录一些之前阅读，感触颇深或者觉得有趣的文章，并附连接。</p><a id="more"></a><h2 id="人生成长"><a href="#人生成长" class="headerlink" title="人生成长"></a>人生成长</h2><ul><li><a href="https://yzhang-gh.github.io/notes/reading/reward-and-half-life.html">收益值与半衰期 - 采铜</a>：从个人提升的角度，评估一件事是否该做。</li><li><a href="https://zhuanlan.zhihu.com/p/82028811">25岁的年轻人，要想清两件事 - 北冥乘海生</a>：定义人生的目标函数，弄清人生的约束条件。</li></ul><h2 id="宗教信仰"><a href="#宗教信仰" class="headerlink" title="宗教信仰"></a>宗教信仰</h2><ul><li><a href="https://www.zhihu.com/question/426477472/answer/1590124777">怎么样回答外国朋友“这个世界人人有寄托，为什么中国人没有信仰”这个问题？ - 赵浪</a>：简单宗教探讨中，如何进行辩经。</li></ul><h2 id="哲学"><a href="#哲学" class="headerlink" title="哲学"></a>哲学</h2><ul><li><a href="https://www.zhihu.com/question/384441334/answer/1146525915">我们穷极一生，究竟追寻的是什么？ - 曹哲</a></li></ul><h2 id="写作与表达"><a href="#写作与表达" class="headerlink" title="写作与表达"></a>写作与表达</h2><ul><li><a href="https://mp.weixin.qq.com/s/nNl6qZtVx8OjyiT1rz6JJA">你以为我在剑桥读经典，其实我不过是学会说话</a>：准确使用语言。</li></ul>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;此处记录一些之前阅读，感触颇深或者觉得有趣的文章，并附连接。&lt;/p&gt;</summary>
    
    
    
    <category term="Knowledge from Growth" scheme="https://superuier.github.io/categories/knowledge-from-growth/"/>
    
    
    <category term="转载" scheme="https://superuier.github.io/tags/%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>合作的进化</title>
    <link href="https://superuier.github.io/reading-note/evolution-of-cooperation/"/>
    <id>https://superuier.github.io/reading-note/evolution-of-cooperation/</id>
    <published>2021-07-26T15:44:00.000Z</published>
    <updated>2021-07-26T15:44:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>《合作的进化》 罗伯特·阿克塞尔罗德</p><a id="more"></a><h2 id="极简简介"><a href="#极简简介" class="headerlink" title="极简简介"></a>极简简介</h2><p>罗伯特·阿克塞尔罗德是美国科学院院士，著名行为分析和博弈论专家。主要由他在博弈论和复杂性理论上基础性突破而广为人知。他是把计算机模型运用到社会科学问题领域的权威学者。</p><p>这本书是一本乐观的书。</p><p>本书从一个重复囚徒困境的竞赛结果出发，分析了表现优异策略的特点，并分析推广到社会生活中。阐述了合作如何诞生与发展，与合作存在的环境。通过分析“一报还一报”，将其特点作为重要元素，指导我们真实的合作进程。</p><h2 id="个人看法"><a href="#个人看法" class="headerlink" title="个人看法"></a>个人看法</h2><h3 id="极简评价"><a href="#极简评价" class="headerlink" title="极简评价"></a>极简评价</h3><p>这是一本蛮有趣的社会学书籍，其研究方法较为严谨符合逻辑。本书主张的观点，其实是可以为每一个正直人所用，的确有一些受益。最主要的观点在于合作中成功策略应该有四个特性，从个人角度看来，是可以在日常生活中运用的。这样在善良的前提下，可以保护自身利益。</p><p>此外作者对于合作产生的背景的研究也蛮有趣。我觉得这也是一个比较重要的部分。其重要性在于作为管理者，如何使被管理的部门产生且维持高效的合作。这样其实是会增大整体的效应。</p><p>本书主要观点其实不多。翟老师之前的介绍中其实已经基本涵盖。书中主要是多了很多实验细节和阐述，其实不需要刻意来读。</p><h3 id="缺点（喜爱挑毛病）"><a href="#缺点（喜爱挑毛病）" class="headerlink" title="缺点（喜爱挑毛病）"></a>缺点（喜爱挑毛病）</h3><p>这种基于强假设的逻辑推演局限性明显。因为真实环境，假设并不一定成立。</p><p>此外计算机的模拟其实并不能很好的作为“提炼道理”的工具。这就有些像日常生活中看到什么事，悟到了什么道理，这个事未必全面，这个道理未必准确。这也是很多 heuristic 方法的通病。</p><p>个人认为这本书其实是提出来一个假说。但是假说要通过演绎才能使其更加完备。</p><h1 id="按节讨论-摘录"><a href="#按节讨论-摘录" class="headerlink" title="按节讨论/摘录"></a>按节讨论/摘录</h1><h2 id="第一章：合作的问题"><a href="#第一章：合作的问题" class="headerlink" title="第一章：合作的问题"></a>第一章：合作的问题</h2><p>在什么条件下才能从没有集权的利己主义者中产生合作？国与国之间人与人之间都会面临这个问题。</p><p>本书重点研究追求各自利益的个体行为，并分析在社会系统中有哪些因素影响个体行为。目标是建立一个合作理论以帮助我们理解合作出现的必要条件。基本假设是个体追求自身利益。</p><p>假设下一步对局的收益是上一步对局收益的w倍（w是折扣系数，小于1）。<strong>那么如果折扣系数足够大，则不存在独立于对方所采用策略的最优策略。</strong>举例，参议院中，你最好喝将来会回报合作的人合作，而不是与将来行为不太受现在影响的人合作。</p><p>囚徒困境的框架：</p><ul><li>对策者的收益不可比较。</li><li>收益不必对称。</li><li>对策者收益是相对的而不是绝对的。</li><li>决定是否合作不必顾及他人的看法。</li><li>不必假设对策者是理性的，不必假设她们总是企图争取最大利润。</li><li>对策者的行为不必都是有意识地选择</li></ul><h2 id="第二章：“一报还一报”在计算机竞赛中的胜利"><a href="#第二章：“一报还一报”在计算机竞赛中的胜利" class="headerlink" title="第二章：“一报还一报”在计算机竞赛中的胜利"></a>第二章：“一报还一报”在计算机竞赛中的胜利</h2><p>“一报还一报”策略在重复囚徒困境中胜出。同时在演化计算的方式下，“一报还一报”在代际间保留，且比例增加。在演化后期，一些前期成功的不善良程序转折变差。</p><p>一个成功决策应有的四个特性：</p><ul><li>对方合作你就合作以避免不必要的冲突。</li><li>对方的背叛你是可激怒的。</li><li>在给挑衅以反击后你是宽容的。</li><li>行为要简单清晰。</li></ul><h2 id="第三章：合作的建立"><a href="#第三章：合作的建立" class="headerlink" title="第三章：合作的建立"></a>第三章：合作的建立</h2><p>探索结果的适用范围。简单来说要求<strong>个体有足够大的机会再次相遇</strong>，形成未来打交道的利害关系。在进化中数学表达就是当折扣系数足够大时，一报还一报是“集体稳定的”，不容易被侵入。</p><p>相似的，一个被认为在下次选举落选的国会议员很难在原有的信任和声誉上和同僚们做立法交易。</p><p>“总是背叛”的策略总是集体稳定的，可以阻止个体的侵入。但是新来者是一个小群体的话，就有机会建立合作。（相遇匹配不随机，新来者之间有机会建立合作。）善良的策略不能被单个个体侵入，那么也不能被这类个体的小群体侵入。</p><p>这样合作的进化可以分成三个阶段：</p><ol><li>起始阶段：合作可以在无条件背叛的世界里产生。有交往的可能，合作便会出现。</li><li>中间阶段：基于回报的策略在许多不同类型策略中成长起来。</li><li>最后阶段：基于回报的策略一旦建立，能防止其他不太合作的策略的侵入。因此社会进化的齿轮是不可逆转的。</li></ol><h2 id="第四章：一战中“自己活也让别人活”的系统"><a href="#第四章：一战中“自己活也让别人活”的系统" class="headerlink" title="第四章：一战中“自己活也让别人活”的系统"></a>第四章：一战中“自己活也让别人活”的系统</h2><p>四五章具体说明这个结果的适用范围。第四章论述“自己活也让别人活”系统。</p><p>讲述了一战英德前线对峙的例子。在都没有前进的企图时，双方愿意一起克制而不愿交替采取敌对行动。合作的开始，有同时进餐的因素，也有糟糕天气的因素。同时在合作中双方也会证明自己有必要是会报复的。</p><p>当然也有很多影响合作的问题：</p><ul><li>部队的换防</li><li>炮兵更不容易受到报复（在克制和报复中起到重要作用</li><li>最终由司令部的突然袭击破坏合作）</li></ul><h2 id="第五章：生物系统中的合作进化"><a href="#第五章：生物系统中的合作进化" class="headerlink" title="第五章：生物系统中的合作进化"></a>第五章：生物系统中的合作进化</h2><p>战壕里的士兵清楚理解和认识到回报在维持合作中的作用。但是参与者的这种理解并不是合作出现和稳定必须的。合作可以在没有预见的情况下产生。这一章主要是在生物学的角度论述。</p><p>在没有明显亲缘关系的情况下，自然界中形成了许多共生关系。这一章并没有太大兴趣，可以之后再仔细阅读。</p><h2 id="第六章：如何有效的选择"><a href="#第六章：如何有效的选择" class="headerlink" title="第六章：如何有效的选择"></a>第六章：如何有效的选择</h2><p>预见对于合作的进化不是必要的，但是的确很有帮助。</p><p>在持续的囚徒困境中如何表现，作者向个体选择提供四个方面的建议：</p><ul><li>不要妒忌对方的成功<ul><li>要求自己比对方做的更好不是一个很好的标准，否则可能导致危险的冲突</li><li>一报还一报总是表现不错，但是他从来没有比别人得更多的分。</li></ul></li><li>不要首先背叛</li><li>要对合作和背叛都做出回报</li><li>不耍小聪明<ul><li>不要用一些复杂的方法来推断对方，这些推断通常是错误的。</li><li>自己的策略复杂到对方不可理解是非常危险的。</li><li>在非零和博弈中，诀窍在于鼓励合作，表明你愿意回报。</li></ul></li></ul><h2 id="第七章：如何促进合作"><a href="#第七章：如何促进合作" class="headerlink" title="第七章：如何促进合作"></a>第七章：如何促进合作</h2><p>改革者通过改变相互作用的条件来促进合作：</p><ul><li>增大未来的影响，增加接触频率</li><li>改变收益值，使利害关系可以被看到</li><li>教育人们相互关心</li><li>教育人们要回报</li><li>改进辨别能力</li></ul><h2 id="第八章：合作的社会结构"><a href="#第八章：合作的社会结构" class="headerlink" title="第八章：合作的社会结构"></a>第八章：合作的社会结构</h2><p>合作理论应用的领域。不同类型社会结构如何影响合作发展方式。</p><p>四个能引起有趣的社会结构形式的因素：</p><ul><li>标记：划分种群。</li><li>信誉：体现在其他人对他采用策略的信心上。</li><li>管理：政府处理个人间公司间纠纷。</li><li>领地：策略在领地上传播。</li></ul><h2 id="第九章：回报的鲁棒性"><a href="#第九章：回报的鲁棒性" class="headerlink" title="第九章：回报的鲁棒性"></a>第九章：回报的鲁棒性</h2><p>进化的方法基于一个简单的原则：成功的东西可能在将来经常出现。这本书研究从在没有集权的情况下，合作如何从自私者中产生，扩展到当人们却是关心会发生什么和当有集权时又会发生什么。其中友谊不是合作的进化必要的，预见性也不是必要的。</p><p>从小群体开始合作，在善良、可激怒和某种程度的宽容规则中逐步成长，一旦成为一个群体，采用这种有识别力的策略的个体就能保护自己不受侵入，总体的合作水平是在上升而不是在下降。换句话说，合作的进化是不可逆转的。</p><p>最有价值的发现是：具有预见能力的参与者了解合作理论的真谛后，可以加快合作的进化。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;《合作的进化》 罗伯特·阿克塞尔罗德&lt;/p&gt;</summary>
    
    
    
    <category term="Reading Note" scheme="https://superuier.github.io/categories/reading-note/"/>
    
    
    <category term="读书" scheme="https://superuier.github.io/tags/%E8%AF%BB%E4%B9%A6/"/>
    
  </entry>
  
  <entry>
    <title>腾讯云相关记录</title>
    <link href="https://superuier.github.io/software-tools/hexo/tencent-cloud-hexo/"/>
    <id>https://superuier.github.io/software-tools/hexo/tencent-cloud-hexo/</id>
    <published>2021-07-22T06:51:00.000Z</published>
    <updated>2021-08-05T12:11:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录在腾讯云服务器上部署 Hexo 搭建的博客时遇到的问题。</p><a id="more"></a><h1 id="腾讯云相关"><a href="#腾讯云相关" class="headerlink" title="腾讯云相关"></a>腾讯云相关</h1><h2 id="远程连接"><a href="#远程连接" class="headerlink" title="远程连接"></a>远程连接</h2><p>需要在本地使用 ssh 连接云端 root 账户或者普通账户。首先在腾讯云里 ubuntu 下管理员账户名为 ubuntu 而不是 root。</p><p>第一次链接出现管理员账户无法使用密码和密钥连接其普通账户也无法使用密码登录的情况。这时需要在腾讯云实例的控制台中对 ssh 进行相关设置。具体来说修改以下文件 <code>/etc/ssh/sshd_config</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PasswordAuthentication yes <span class="comment"># 开启密码登录权限</span></span><br><span class="line">PubkeyAuthentication yes <span class="comment"># 使用密钥登录</span></span><br></pre></td></tr></table></figure><p>之后重启服务 <code>sudo service sshd restart</code> 即可。之后使用 ssh 和下载好的密钥进行连接，详见<a href="https://cloud.tencent.com/document/product/1207/44643#.E4.BD.BF.E7.94.A8.E5.AF.86.E9.92.A5.E7.99.BB.E5.BD.95">官方文档</a>。</p><h2 id="使用-nginx-部署-Server"><a href="#使用-nginx-部署-Server" class="headerlink" title="使用 nginx 部署 Server"></a>使用 nginx 部署 Server</h2><p>详见<a href="https://zhuanlan.zhihu.com/p/108720935">这篇文档</a></p><p>其中涉及到不少对 Nginx 的操作，下面记录一些基础命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先使用 Nginx 命令的时候需要使用管理员权限。</span></span><br><span class="line"><span class="comment">## 开启服务器</span></span><br><span class="line">sudo systemctl nginx</span><br><span class="line"><span class="comment">## 重新启动更新设置</span></span><br><span class="line">sudo systemctl restart nginx.service </span><br></pre></td></tr></table></figure><p>其中 Nginx 的配置文件位于 <code>/etc/nginx/nginx.conf</code>，更改过后重启即可。</p><h2 id="服务器-SSL-证书安装部署"><a href="#服务器-SSL-证书安装部署" class="headerlink" title="服务器 SSL 证书安装部署"></a>服务器 SSL 证书安装部署</h2><p>可以参考腾讯云的<a href="https://cloud.tencent.com/document/product/400/35244">这篇文档</a>。主要就是将证书上传再再 Nginx 中设置。需要注意的是颁发证书对应的域名一定要和真是域名相同，不要少前缀。</p><h2 id="本地-ssh-连接服务器长时间不操作断开问题"><a href="#本地-ssh-连接服务器长时间不操作断开问题" class="headerlink" title="本地 ssh 连接服务器长时间不操作断开问题"></a>本地 ssh 连接服务器长时间不操作断开问题</h2><p>具体来说修改以下文件 <code>~/.ssh/config</code>。增添以下内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Host *</span><br><span class="line">        <span class="comment"># 断开时重试连接的次数</span></span><br><span class="line">        ServerAliveCountMax 5</span><br><span class="line">        <span class="comment"># 每隔5秒自动发送一个空的请求以保持连接</span></span><br><span class="line">        ServerAliveInterval 5</span><br></pre></td></tr></table></figure><p>参考<a href="https://www.pkslow.com/archives/ssh-keep-alive">这篇文章</a>。</p><h2 id="使用-webhook-对-repo-的更新进行监控"><a href="#使用-webhook-对-repo-的更新进行监控" class="headerlink" title="使用 webhook 对 repo 的更新进行监控"></a>使用 webhook 对 repo 的更新进行监控</h2><p>详见这几篇文档：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/116136090">使用Github的Webhooks+Node完成网站的自动化部署</a></li><li><a href="https://jimmysong.io/blog/github-webhook-website-auto-deploy/">使用 GitHub Webhook 实现静态网站自动化部署</a></li><li><a href="https://jelly.jd.com/article/6006b1025b6c6a01506c878a">使用Github的webhooks进行网站自动化部署</a></li></ul><p>然而事实是我这次并没有从 webhook 的方向来部署，而是从 github action 中 ssh 到服务器进行操作。具体的部署步骤放在服务器的 deploy 文件中。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文记录在腾讯云服务器上部署 Hexo 搭建的博客时遇到的问题。&lt;/p&gt;</summary>
    
    
    
    <category term="Software Tools" scheme="https://superuier.github.io/categories/software-tools/"/>
    
    
    <category term="Hexo" scheme="https://superuier.github.io/tags/Hexo/"/>
    
    <category term="cloud" scheme="https://superuier.github.io/tags/cloud/"/>
    
  </entry>
  
  <entry>
    <title>Gaussian Process</title>
    <link href="https://superuier.github.io/machine-learning/gaussian-process/"/>
    <id>https://superuier.github.io/machine-learning/gaussian-process/</id>
    <published>2021-07-21T08:34:53.000Z</published>
    <updated>2021-07-21T08:34:53.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>一份理解高斯分布的笔记。</p><a id="more"></a><p>参考：</p><ul><li><a href="https://www.jgoertler.com/visual-exploration-gaussian-processes/">A Visual Exploration of Gaussian Processes</a></li><li><a href="https://zhuanlan.zhihu.com/p/56562456">看得见的高斯过程：这是一份直观的入门解读</a></li><li><a href="https://www.zhihu.com/question/46631426/answer/1735470753">如何通俗易懂地介绍 Gaussian Process？</a></li><li><a href="https://zhuanlan.zhihu.com/p/104601803">高斯过程回归：推导，实现和理解</a></li></ul><hr><h2 id="模型的理解"><a href="#模型的理解" class="headerlink" title="模型的理解"></a>模型的理解</h2><p>高斯分布一般用来做回归。其原理是把回归当成一个采样过程。在 n 个测试样本上预测的情况，可以想象成在一个 n 维的高斯分布下进行采样。在每个维度上的采样结果可以当作在这个样本上的预测值。所以说回归问题转化为如何构建这个 n 维的高斯分布，一旦构建完成则可以用来预测。</p><p>直接对预测数据$X$构建这个先验分布$P(X)$是困难的，所以说在有训练数据$Y$的情况下，我们期望找到的是一个后验概率分布$P(X|Y)$。</p><hr><h2 id="目标模型的构建"><a href="#目标模型的构建" class="headerlink" title="目标模型的构建"></a>目标模型的构建</h2><h3 id="1-包含训练数据的先验分布"><a href="#1-包含训练数据的先验分布" class="headerlink" title="1. 包含训练数据的先验分布"></a>1. 包含训练数据的先验分布</h3><p>首先我们需要先得到一个包含训练点 $X_a$ 和测试点 $X_b$ 先验分布 $P (X_a,X_b)$，维度为 $|X_a|+|X_b| = p + q$。这个先验分布同样也是高斯分布 $P(X_a,X_b) \sim \mathcal{N}(\mu,\,\Sigma)$。具体来说，</p><script type="math/tex; mode=display">X=\begin{bmatrix}X_a\\X_b\end{bmatrix}_{p+q}\quad\mu=\begin{bmatrix}\mu_a\\\mu_b\end{bmatrix}_{p+q}\quad \Sigma=\begin{bmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{bmatrix}_{p + q, p + q}</script><p>在这里我们需要定义 $\mu$ 和 $\Sigma$。对于均值 $\mu$，$\mu_a$ 是从训练集中得到的数值，$\mu_b$则需要人为设定或取 $\mu_a$ 的均值。一般在数据归一化的情况下，先验均值可以设为 0 函数。</p><p>对于协方差矩阵 $\Sigma$，则可以用核函数来生成。核函数，一般用来表示一种相似度的度量，这里用每个样本的特征 $x$ 建立样本间的相似度，再用其作为每个维度之间的协方差。此处的核函数有多种选择方式，同时也可以组合起来使用。通过不同核函数的选择可以起到添加人类的先验知识的作用。通过这一个步骤可以建立起联合分布的高斯分布表达式。</p><h3 id="2-通过先验分布得到后验分布"><a href="#2-通过先验分布得到后验分布" class="headerlink" title="2. 通过先验分布得到后验分布"></a>2. 通过先验分布得到后验分布</h3><p>此时我们可以通过条件作用从 $P(X_a,X_b)$ 得到 $P(X_b|X_a)$。这样是从一个维度为$|X_a|+|X_b|$的高斯分布得到一个维度为维度为 $|X_b|$ 的高斯分布。条件作用之后均值和标准差会发生变化，依据高斯分布的性质可以得到以下条件分布，</p><script type="math/tex; mode=display">X_b|X_a\sim N(\mu_{b|a},\Sigma_{b|a})</script><p>其中，$\mu_{b|a}=\Sigma_{ba}\Sigma^{-1}_{aa}(X_a-\mu_a)+\mu_b$，$\Sigma_{b|a}=\Sigma_{bb}-\Sigma_{ba}\Sigma^{-1}_{aa}\Sigma_{ab}$。</p><p>直观上讲，训练点是为候选的函数设了一个限定范围，所得到的函数需要通过训练点。所以在结果中，靠近训练数据点的区域预测不确定性会小，离得越远，不确定性越大。</p><hr><h2 id="在无限维的条件下"><a href="#在无限维的条件下" class="headerlink" title="在无限维的条件下"></a>在无限维的条件下</h2><p>对于观测点 $X$ 与其对应值 $Y$，所有的非观测点 $X^<em>$ 的值定义为 $f(X^</em>)$。这里我们再把均值向量替换为均值函数。那么有，</p><script type="math/tex; mode=display">\begin{bmatrix}Y\\f(X^*)\end{bmatrix}\sim N(\begin{bmatrix}\mu(X)\\\mu(X^*)\end{bmatrix}，\begin{bmatrix}k(X,X)& k(X,X^*)\\k(X^*,X)&k(X^*,X^*)\end{bmatrix})</script><p>同样的我们可以得到条件分布，</p><script type="math/tex; mode=display">f(X^*)|Y\sim N(\mu^*,k^*)</script><p>其中, $\mu^\ast=k(X^\ast,X)k(X,X)^{-1}(Y-\mu(X))+\mu(X^\ast)$，$k^\ast=k(X^\ast,X^\ast)-k(X^\ast,X)k(X,X)^{-1}k(X,X^\ast)$。</p><p>同样的，一般在数据归一化的情况下，先验均值可以设为 0 函数。在 <code>sklean</code> 中，可以通过调节参数 <code>normalize_y=True</code> 实现。</p>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;一份理解高斯分布的笔记。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://superuier.github.io/categories/Machine-Learning/"/>
    
    
    <category term="machine-learning" scheme="https://superuier.github.io/tags/machine-learning/"/>
    
    <category term="gaussian-process" scheme="https://superuier.github.io/tags/gaussian-process/"/>
    
  </entry>
  
  <entry>
    <title>杠杆率无法衡量经济风险</title>
    <link href="https://superuier.github.io/economy-finance/didongsheng/dibi/"/>
    <id>https://superuier.github.io/economy-finance/didongsheng/dibi/</id>
    <published>2021-07-15T15:46:57.000Z</published>
    <updated>2021-07-15T15:46:57.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>2019.12.</p><p>该视频是翟币的由来视频。</p><a id="more"></a><h2 id="去杠杆概念的源起"><a href="#去杠杆概念的源起" class="headerlink" title="去杠杆概念的源起"></a>去杠杆概念的源起</h2><p>Ray Dalio，桥水基金创始人。他的核心概念就是去杠杆。</p><p>他认为债务/GDP不能太高而且不能增长太快，否则经济会崩塌。但是这个可能是在胡扯。逻辑可能是错的。</p><p>债务是存量概念，GDP是流概念。能不能得出有意义的结论？</p><p>举例，银行放债。企业的销售额除以债务能不能说明问题。京东和字节跳动相比，字节跳动销售额虽然可能小，但是利润更大。</p><h2 id="杠杆率与经济危机无关"><a href="#杠杆率与经济危机无关" class="headerlink" title="杠杆率与经济危机无关"></a>杠杆率与经济危机无关</h2><p>日欧债务率都相当高，但是即使负利率，都很债务安全。杠杆理论的反例，债务率低的崩盘了很多。</p><p>那什么具有决定性意义呢？债务的定价货币是什么。</p><h2 id="翟币"><a href="#翟币" class="headerlink" title="翟币"></a>翟币</h2><p>用10万翟币债券债务关系买手机。一年之后还钱只需要再印十万就好。</p><p>格林斯潘：“女士们先生们，这些钱我们永远都不用还。”</p><h2 id="中国政府债务"><a href="#中国政府债务" class="headerlink" title="中国政府债务"></a>中国政府债务</h2><p>中国没有借美元债，都是人民币债。中国债务率大家都觉得高。中央政府债务率占GDP比例20%都不到。但是地方政府借了很多债。</p><p>即便中央地方债务都算进去，也仅有60%左右。和德国中央政府差不多。</p><p>本币债和外币债有重要区别。本币债其实本质上是一种隐形的税收。</p>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;2019.12.&lt;/p&gt;
&lt;p&gt;该视频是翟币的由来视频。&lt;/p&gt;</summary>
    
    
    
    <category term="Economy &amp; Finance" scheme="https://superuier.github.io/categories/economy-finance/"/>
    
    
    <category term="翟东升" scheme="https://superuier.github.io/tags/%E7%BF%9F%E4%B8%9C%E5%8D%87/"/>
    
  </entry>
  
</feed>
