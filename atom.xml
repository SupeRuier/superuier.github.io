<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Rui&#39;s Blog</title>
  
  
  <link href="https://superuier.github.io/atom.xml" rel="self"/>
  
  <link href="https://superuier.github.io/"/>
  <updated>2021-08-10T06:32:55.000Z</updated>
  <id>https://superuier.github.io/</id>
  
  <author>
    <name>Rui He</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>早停与验证集损失</title>
    <link href="https://superuier.github.io/programming/early-stopping/"/>
    <id>https://superuier.github.io/programming/early-stopping/</id>
    <published>2021-08-10T06:32:55.000Z</published>
    <updated>2021-08-10T06:32:55.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>一些有关神经网络训练早停及验证集损失的记录。实验中发现验证集损失与其准确度不严格单调负相关，所以搜索一下答案。</p><a id="more"></a><h2 id="早停"><a href="#早停" class="headerlink" title="早停"></a>早停</h2><p>训练神经网络时，随着训练迭代次数的增加，训练损失会下降。而验证集损失一般会经历一个先下降后上升的过程，上升时则为过拟合出现。在此情况下，一般选用早停来结束当前模型的训练。</p><p>对于某种评估指标，假设其越低越好（/越高越好），在训练过程中，如果在一定数量的 epoch 的容忍度之内，其没有比之前的最好评估要低（/高），则停止训练，并重载之前表现最好时的模型参数作为最终的模型输出。</p><p>一般的，我们使用在验证集上的损失来作为早停的依据，如果验证集损失不进一步下降，那么则停止训练。（这也是目前来看大多数人用的依据。）但是早停的依据到底是使用验证集上的 loss 还是 accuracy（或者其他相应的 metric）并没有看到一个统一的说法，互联网上两派的支持者有之。</p><p>对于这种情况，George 在其<a href="http://alexadam.ca/ml/2018/08/03/early-stopping.html">博文</a>中指出了早停的一些问题。其中最重要的一点是，验证集准确度并不随着验证集损失的减小而严格单调递增，如下图所示。</p><div style="width:70%;margin:auto"></div><p>所以之后 George 实现了一种双重标准的早停，只有当损失和准确率都变糟糕时才早停。</p><h2 id="验证集损失与准确率"><a href="#验证集损失与准确率" class="headerlink" title="验证集损失与准确率"></a>验证集损失与准确率</h2><p>一般来说，验证集损失与准确率呈负相关关系。但是有些时候，存在验证集损失上升，但是验证集准确性仍进一步提高的情况。</p><p>这出现于模型对于预测过于自信和极端的情况。换句话说，模型倾向于输出较为极端的预测值，这样使得在同样数量分错的情况下，少数预测错的样本主导了 loss，但是对整体准确率影响不大。</p><p>以下是一些可能出现的原因：</p><ul><li>训练集验证集数据分布不一致</li><li>训练集过小未包含验证集所有情况</li></ul><p>Github 上也有一个 <a href="https://github.com/thegregyang/LossUpAccUp">Repo</a> 来讨论此现象。对于解决方法来说似乎没有一个明确的结论。作者在尝试了较小模型和批正则化后仍然会出现此问题。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="http://alexadam.ca/ml/2018/08/03/early-stopping.html">Early Stopping and its Faults</a></li><li><a href="https://www.zhihu.com/question/318399418/answer/1202932315">验证集loss上升，准确率却上升该如何理解？ - 刘国洋的回答 - 知乎</a></li><li><a href="https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy">https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;一些有关神经网络训练早停及验证集损失的记录。
实验中发现验证集损失与其准确度不严格单调负相关，所以搜索一下答案。&lt;/p&gt;</summary>
    
    
    
    <category term="Programming" scheme="https://superuier.github.io/categories/programming/"/>
    
    
    <category term="machine-learning" scheme="https://superuier.github.io/tags/machine-learning/"/>
    
    <category term="neural-network" scheme="https://superuier.github.io/tags/neural-network/"/>
    
    <category term="loss" scheme="https://superuier.github.io/tags/loss/"/>
    
  </entry>
  
  <entry>
    <title>交叉熵小结</title>
    <link href="https://superuier.github.io/math/cross-entropy/"/>
    <id>https://superuier.github.io/math/cross-entropy/</id>
    <published>2021-08-09T13:54:55.000Z</published>
    <updated>2021-08-09T13:54:55.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>这里对机器学习中常用到的交叉熵做一个总结。</p><a id="more"></a><h2 id="信息论中的交叉熵"><a href="#信息论中的交叉熵" class="headerlink" title="信息论中的交叉熵"></a>信息论中的交叉熵</h2><p>首先我们要定义一下信息熵。</p><script type="math/tex; mode=display">H(X)=\mathbb{E}_{X}[I(x)]=\sum_{x \in \mathcal{X}} - p(x) \log _{2}\left({p(x)}\right)</script><p>其中$\mathcal{X}$为有限个事件$x$的集合，$X$是定义在$\mathcal{X}$上的随机变量。信息熵是随机事件不确定性的度量，也可以看作是信息量的期望。同时，信息熵是信源编码定理中，压缩率的下限。当我们用少于信息熵的资讯量做编码，那么我们一定有资讯的损失。</p><p>交叉熵也是信息论中的一个概念。典型情况下，$p$ 表示数据的真实分布，$q$ 表示数据的理论分布、估计的模型分布、或$p$的近似分布。在此基础上定义交叉熵，</p><script type="math/tex; mode=display">H(p, q)=\mathrm{E}_{p}[-\log q]=H(p)+D_{\mathrm{KL}}(p \| q)</script><p>其中 $H(p)$ 是 $p$ 的熵， $D_{\mathrm{KL}}(p | q)$ 是从 $p$ 与 $q$ 的KL散度(也被称为 $p$ 相对于 $q$ 的相对熵。</p><script type="math/tex; mode=display">D_{\mathrm{KL}}(p \| q)=-\sum_{x} p(x) \ln \frac{q(x)}{p(x)}</script><p>KL散度是两个概率分布 $p$ 和 $Q$ 差别的非对称性的度量。其用来度量使用基于 $Q$ 的分布来编码服从 $p$ 的分布的样本所需的额外的平均比特数。</p><p>所以对于离散分布 $p$ 和 $q$, 这意味着其交叉熵为：</p><script type="math/tex; mode=display">H(p, q)=-\sum_{x} p(x) \log q(x)</script><p>基于相同事件测度的两个概率分布 $p$ 和 $q$ 的交叉熵指，当基于一个“非自然”（相对于“真实”分布 $p$ 而言）的概率分布 $q$ 进行编码时，在事件集合中唯一标识一个事件所需要的平均比特数（bit）。(说人话就是使用基于$q$的分布来编码服从$p$的分布的样本所需的平均比特数。)</p><h2 id="机器学习中的交叉熵损失"><a href="#机器学习中的交叉熵损失" class="headerlink" title="机器学习中的交叉熵损失"></a>机器学习中的交叉熵损失</h2><p>机器学习中，常用交叉熵作为神经网络的损失函数。其形式为，</p><script type="math/tex; mode=display">loss = -\sum_{i=1}^{n} y \log(\hat{y}_{i})</script><p>其中$\hat{y}_{i}$是第 $i$ 个样本在不同类别上的概率，$y$ 是第 $i$ 个样本的真实标记（长度为 $c$ 的向量）。神经网络中这个概率常常使用 softmax 函数，或者 sigmoid 函数得到。</p><h2 id="Pytorch-中实现交叉熵损失"><a href="#Pytorch-中实现交叉熵损失" class="headerlink" title="Pytorch 中实现交叉熵损失"></a>Pytorch 中实现交叉熵损失</h2><p>在 pytorch 中有两种方式实现交叉熵损失。可以参考之前的<a href="/programming/neural-network-loss/" title="这篇笔记">这篇笔记</a>。可以调用<code>NLLLoss()</code>或<code>CrossEntropyLoss()</code>两个函数。两函数的不同点主要在于输入不同：</p><ul><li>最后一层全连接层的输出可以直接调用<code>CrossEntropyLoss()</code></li><li>对于<code>NLLLoss()</code>，要将最后一层全连接层的输出再通过一次<code>LogSoftmax()</code>计算才能调用。</li></ul><p>同时如果类别不平衡也可以在每一个类别上加上相应的权重。</p><h2 id="交叉熵的求导"><a href="#交叉熵的求导" class="headerlink" title="交叉熵的求导"></a>交叉熵的求导</h2><p>因为之前要的工作要手动计算损失函数反向传播时对最后一层参数的梯度，所以需要对交叉熵损失求导。此处我们只需要考虑对最后一层全连接层的输出求导即可，对参数可以之后再进一步求导。</p><p>先考虑一下正向的过程（在 n 个类别下使用 softmax 的情况），对于一个样本在最后一层全连接层的输出 $X = [x_1,…,x_n]$，我们考虑第 $i$ 个类，</p><script type="math/tex; mode=display">x_i\xrightarrow[i_{th} output]{LogSoftmax}- \ln (\frac{\exp (x_{i})}{\sum_{j=1}^{n} \exp (x_{j})}) \xrightarrow[]{Y}- y_i \ln (\frac{\exp (x_{i})}{\sum_{j=1}^{n} \exp (x_{j})})</script><p>当考虑所有类别时，一般是将最右端这一项加权平均或加权求和。此处我们假设权重相等，对所有类别相加，则可以得到总损失。</p><script type="math/tex; mode=display">loss = - \sum_{i=1}^{n}y_i \ln (\frac{\exp (x_{i})}{\sum_{j=1}^{n} \exp (x_{j})}) = - y_c \ln (\frac{\exp (x_{i})}{\sum_{j=1}^{n} \exp (x_{j})})</script><p>其中$y_c = 1$，指当前样本的真实标记是 $c$。</p><script type="math/tex; mode=display">loss = - \ln (\frac{\exp (x_{c})}{\sum_{j=1}^{n} \exp (x_{j})})  = - \ln(Softmax(x_c)) = - \ln(S(x_c))</script><p>此处进行求导，</p><script type="math/tex; mode=display">\frac{\partial loss}{\partial x_i} = - \frac{1}{S(x_c)} \frac{\partial S(x_c)}{\partial x_i}</script><p>而对于 softmax 函数求导需要分类讨论：</p><ul><li>当$i=c$时, $\frac{\partial S(x_c)}{\partial x_i} = S(x_c)(1-S(x_c))$</li><li>当$i\neq c$时, $\frac{\partial S(x_c)}{\partial x_i} = - S(x_i)S(x_c)$</li></ul><p>所以导数为：</p><ul><li>当$i=c$时, $\frac{\partial loss}{\partial x_i} = S(x_i)-1$</li><li>当$i\neq c$时, $\frac{\partial loss}{\partial x_i} = S(x_i)-0$</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ul><li><a href="https://en.wikipedia.org/wiki/Cross_entropy">Cross entropy - From Wikipedia, the free encyclopedia</a></li><li><a href="https://en.wikipedia.org/wiki/Kullback–Leibler_divergence">Kullback–Leibler divergence - From Wikipedia, the free encyclopedia</a></li><li><a href="https://en.wikipedia.org/wiki/Information_theory">Information theory - From Wikipedia, the free encyclopedia</a></li><li><a href="https://zhuanlan.zhihu.com/p/131647655">Softmax以及Cross Entropy Loss求导</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;这里对机器学习中常用到的交叉熵做一个总结。&lt;/p&gt;</summary>
    
    
    
    <category term="Math" scheme="https://superuier.github.io/categories/Math/"/>
    
    
    <category term="machine-learning" scheme="https://superuier.github.io/tags/machine-learning/"/>
    
    <category term="neural-network" scheme="https://superuier.github.io/tags/neural-network/"/>
    
    <category term="loss" scheme="https://superuier.github.io/tags/loss/"/>
    
  </entry>
  
  <entry>
    <title>神经网络损失函数</title>
    <link href="https://superuier.github.io/programming/neural-network-loss/"/>
    <id>https://superuier.github.io/programming/neural-network-loss/</id>
    <published>2021-07-30T07:16:55.000Z</published>
    <updated>2021-08-02T12:11:55.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>由于总是忘记一些 loss 的常用场景和区别，此处记录一些常用的神经网络 loss。</p><a id="more"></a><p>Pytorch <a href="https://pytorch.org/docs/stable/nn.html#loss-functions">官方文档</a>中有十余种 loss 函数，其中常用的主要是<code>CrossEntropyLoss</code>、<code>NLLLoss</code>、<code>MSELoss</code>等。这里仅先对这些常用 loss 展开。</p><h2 id="NLLLoss"><a href="#NLLLoss" class="headerlink" title="NLLLoss"></a>NLLLoss</h2><p>负对数损失，常用于分类任务。值得注意的是，这里的输入 <code>X</code> 需要已经包含对于相应类别的 log-probability。<strong>使用这个 loss 的时候需要在模型最后加入 <code>LogSoftmax</code> 层。</strong></p><script type="math/tex; mode=display">l(X,y) = L = \{l_1,...,l_N\}^\top, \\l_n = -w_{y_n}X_{n,y_n},</script><p>其中 <code>X</code> 是输入，<code>y</code> 是目标，<code>w</code> 是类别的权重，<code>N</code> 是 batch size。</p><p>具体的计算例子可以看<a href="https://blog.csdn.net/qq_22210253/article/details/85229988">这篇文章</a>。</p><h2 id="CrossEntropyLoss"><a href="#CrossEntropyLoss" class="headerlink" title="CrossEntropyLoss"></a>CrossEntropyLoss</h2><p>交叉墒损失，同样常用于分类任务。这个标准结合了 LogSoftmax 和 NLLLoss 两个部分。</p><p>首相介绍一下 LogSoftmax，其包含 log 函数和 softmax 函数。</p><script type="math/tex; mode=display">\operatorname{LogSoftmax}\left(x_{i}\right)=\log \left(\frac{\exp \left(x_{i}\right)}{\sum_{j} \exp \left(x_{j}\right)}\right)</script><p>对于交叉墒损失，值得注意的是，这里的输入 <code>X</code> 需要已经包含对每一个类，未经处理的 unformalized score。<strong>换句话说，使用这个 loss 的时候不需要在模型最后加入 <code>Softmax</code> 层。</strong></p><script type="math/tex; mode=display">\operatorname{loss}(x, \text { class })=-\log \left(\frac{\exp (x[\text { class }])}{\sum_{j} \exp (x[j])}\right)=-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)</script><p>对于加权的情况，</p><script type="math/tex; mode=display">\operatorname{loss}(x, \text { class })=\text { weight }[\text { class }]\left(-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)\right)</script><p>最终的 loss 需要加权平均得到，</p><script type="math/tex; mode=display">\operatorname{loss}(x, \text { class })=\text { weight }[\text { class }]\left(-x[\text { class }]+\log \left(\sum_{j} \exp (x[j])\right)\right)</script><p>其中 <code>x</code> 是输入，<code>class</code> 是目标类，<code>weight</code> 是类别的权重，<code>N</code> 是 batch size。</p>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;由于总是忘记一些 loss 的常用场景和区别，此处记录一些常用的神经网络 loss。&lt;/p&gt;</summary>
    
    
    
    <category term="Programming" scheme="https://superuier.github.io/categories/programming/"/>
    
    
    <category term="machine-learning" scheme="https://superuier.github.io/tags/machine-learning/"/>
    
    <category term="neural-network" scheme="https://superuier.github.io/tags/neural-network/"/>
    
    <category term="Python" scheme="https://superuier.github.io/tags/Python/"/>
    
    <category term="Pytorch" scheme="https://superuier.github.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>好文阅读转载</title>
    <link href="https://superuier.github.io/reading-note/article-sharing/"/>
    <id>https://superuier.github.io/reading-note/article-sharing/</id>
    <published>2021-07-28T05:22:47.000Z</published>
    <updated>2021-08-09T07:04:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>此处记录一些之前阅读，感触颇深或者觉得有趣的文章，并附连接。</p><a id="more"></a><h2 id="人生成长"><a href="#人生成长" class="headerlink" title="人生成长"></a>人生成长</h2><ul><li><a href="https://yzhang-gh.github.io/notes/reading/reward-and-half-life.html">收益值与半衰期 - 采铜</a>：从个人提升的角度，评估一件事是否该做。</li><li><a href="https://zhuanlan.zhihu.com/p/82028811">25岁的年轻人，要想清两件事 - 北冥乘海生</a>：定义人生的目标函数，弄清人生的约束条件。</li></ul><h2 id="宗教信仰"><a href="#宗教信仰" class="headerlink" title="宗教信仰"></a>宗教信仰</h2><ul><li><a href="https://www.zhihu.com/question/426477472/answer/1590124777">怎么样回答外国朋友“这个世界人人有寄托，为什么中国人没有信仰”这个问题？ - 赵浪</a>：简单宗教探讨中，如何进行辩经。</li></ul><h2 id="写作与表达"><a href="#写作与表达" class="headerlink" title="写作与表达"></a>写作与表达</h2><ul><li><a href="https://mp.weixin.qq.com/s/nNl6qZtVx8OjyiT1rz6JJA">你以为我在剑桥读经典，其实我不过是学会说话</a>：准确使用语言。</li></ul>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;此处记录一些之前阅读，感触颇深或者觉得有趣的文章，并附连接。&lt;/p&gt;</summary>
    
    
    
    <category term="Knowledge from Growth" scheme="https://superuier.github.io/categories/knowledge-from-growth/"/>
    
    
    <category term="转载" scheme="https://superuier.github.io/tags/%E8%BD%AC%E8%BD%BD/"/>
    
  </entry>
  
  <entry>
    <title>合作的进化</title>
    <link href="https://superuier.github.io/reading-note/evolution-of-cooperation/"/>
    <id>https://superuier.github.io/reading-note/evolution-of-cooperation/</id>
    <published>2021-07-26T15:44:00.000Z</published>
    <updated>2021-07-26T15:44:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>《合作的进化》 罗伯特·阿克塞尔罗德</p><a id="more"></a><h2 id="极简简介"><a href="#极简简介" class="headerlink" title="极简简介"></a>极简简介</h2><p>罗伯特·阿克塞尔罗德是美国科学院院士，著名行为分析和博弈论专家。主要由他在博弈论和复杂性理论上基础性突破而广为人知。他是把计算机模型运用到社会科学问题领域的权威学者。</p><p>这本书是一本乐观的书。</p><p>本书从一个重复囚徒困境的竞赛结果出发，分析了表现优异策略的特点，并分析推广到社会生活中。阐述了合作如何诞生与发展，与合作存在的环境。通过分析“一报还一报”，将其特点作为重要元素，指导我们真实的合作进程。</p><h2 id="个人看法"><a href="#个人看法" class="headerlink" title="个人看法"></a>个人看法</h2><h3 id="极简评价"><a href="#极简评价" class="headerlink" title="极简评价"></a>极简评价</h3><p>这是一本蛮有趣的社会学书籍，其研究方法较为严谨符合逻辑。本书主张的观点，其实是可以为每一个正直人所用，的确有一些受益。最主要的观点在于合作中成功策略应该有四个特性，从个人角度看来，是可以在日常生活中运用的。这样在善良的前提下，可以保护自身利益。</p><p>此外作者对于合作产生的背景的研究也蛮有趣。我觉得这也是一个比较重要的部分。其重要性在于作为管理者，如何使被管理的部门产生且维持高效的合作。这样其实是会增大整体的效应。</p><p>本书主要观点其实不多。翟老师之前的介绍中其实已经基本涵盖。书中主要是多了很多实验细节和阐述，其实不需要刻意来读。</p><h3 id="缺点（喜爱挑毛病）"><a href="#缺点（喜爱挑毛病）" class="headerlink" title="缺点（喜爱挑毛病）"></a>缺点（喜爱挑毛病）</h3><p>这种基于强假设的逻辑推演局限性明显。因为真实环境，假设并不一定成立。</p><p>此外计算机的模拟其实并不能很好的作为“提炼道理”的工具。这就有些像日常生活中看到什么事，悟到了什么道理，这个事未必全面，这个道理未必准确。这也是很多 heuristic 方法的通病。</p><p>个人认为这本书其实是提出来一个假说。但是假说要通过演绎才能使其更加完备。</p><h1 id="按节讨论-摘录"><a href="#按节讨论-摘录" class="headerlink" title="按节讨论/摘录"></a>按节讨论/摘录</h1><h2 id="第一章：合作的问题"><a href="#第一章：合作的问题" class="headerlink" title="第一章：合作的问题"></a>第一章：合作的问题</h2><p>在什么条件下才能从没有集权的利己主义者中产生合作？国与国之间人与人之间都会面临这个问题。</p><p>本书重点研究追求各自利益的个体行为，并分析在社会系统中有哪些因素影响个体行为。目标是建立一个合作理论以帮助我们理解合作出现的必要条件。基本假设是个体追求自身利益。</p><p>假设下一步对局的收益是上一步对局收益的w倍（w是折扣系数，小于1）。<strong>那么如果折扣系数足够大，则不存在独立于对方所采用策略的最优策略。</strong>举例，参议院中，你最好喝将来会回报合作的人合作，而不是与将来行为不太受现在影响的人合作。</p><p>囚徒困境的框架：</p><ul><li>对策者的收益不可比较。</li><li>收益不必对称。</li><li>对策者收益是相对的而不是绝对的。</li><li>决定是否合作不必顾及他人的看法。</li><li>不必假设对策者是理性的，不必假设她们总是企图争取最大利润。</li><li>对策者的行为不必都是有意识地选择</li></ul><h2 id="第二章：“一报还一报”在计算机竞赛中的胜利"><a href="#第二章：“一报还一报”在计算机竞赛中的胜利" class="headerlink" title="第二章：“一报还一报”在计算机竞赛中的胜利"></a>第二章：“一报还一报”在计算机竞赛中的胜利</h2><p>“一报还一报”策略在重复囚徒困境中胜出。同时在演化计算的方式下，“一报还一报”在代际间保留，且比例增加。在演化后期，一些前期成功的不善良程序转折变差。</p><p>一个成功决策应有的四个特性：</p><ul><li>对方合作你就合作以避免不必要的冲突。</li><li>对方的背叛你是可激怒的。</li><li>在给挑衅以反击后你是宽容的。</li><li>行为要简单清晰。</li></ul><h2 id="第三章：合作的建立"><a href="#第三章：合作的建立" class="headerlink" title="第三章：合作的建立"></a>第三章：合作的建立</h2><p>探索结果的适用范围。简单来说要求<strong>个体有足够大的机会再次相遇</strong>，形成未来打交道的利害关系。在进化中数学表达就是当折扣系数足够大时，一报还一报是“集体稳定的”，不容易被侵入。</p><p>相似的，一个被认为在下次选举落选的国会议员很难在原有的信任和声誉上和同僚们做立法交易。</p><p>“总是背叛”的策略总是集体稳定的，可以阻止个体的侵入。但是新来者是一个小群体的话，就有机会建立合作。（相遇匹配不随机，新来者之间有机会建立合作。）善良的策略不能被单个个体侵入，那么也不能被这类个体的小群体侵入。</p><p>这样合作的进化可以分成三个阶段：</p><ol><li>起始阶段：合作可以在无条件背叛的世界里产生。有交往的可能，合作便会出现。</li><li>中间阶段：基于回报的策略在许多不同类型策略中成长起来。</li><li>最后阶段：基于回报的策略一旦建立，能防止其他不太合作的策略的侵入。因此社会进化的齿轮是不可逆转的。</li></ol><h2 id="第四章：一战中“自己活也让别人活”的系统"><a href="#第四章：一战中“自己活也让别人活”的系统" class="headerlink" title="第四章：一战中“自己活也让别人活”的系统"></a>第四章：一战中“自己活也让别人活”的系统</h2><p>四五章具体说明这个结果的适用范围。第四章论述“自己活也让别人活”系统。</p><p>讲述了一战英德前线对峙的例子。在都没有前进的企图时，双方愿意一起克制而不愿交替采取敌对行动。合作的开始，有同时进餐的因素，也有糟糕天气的因素。同时在合作中双方也会证明自己有必要是会报复的。</p><p>当然也有很多影响合作的问题：</p><ul><li>部队的换防</li><li>炮兵更不容易受到报复（在克制和报复中起到重要作用</li><li>最终由司令部的突然袭击破坏合作）</li></ul><h2 id="第五章：生物系统中的合作进化"><a href="#第五章：生物系统中的合作进化" class="headerlink" title="第五章：生物系统中的合作进化"></a>第五章：生物系统中的合作进化</h2><p>战壕里的士兵清楚理解和认识到回报在维持合作中的作用。但是参与者的这种理解并不是合作出现和稳定必须的。合作可以在没有预见的情况下产生。这一章主要是在生物学的角度论述。</p><p>在没有明显亲缘关系的情况下，自然界中形成了许多共生关系。这一章并没有太大兴趣，可以之后再仔细阅读。</p><h2 id="第六章：如何有效的选择"><a href="#第六章：如何有效的选择" class="headerlink" title="第六章：如何有效的选择"></a>第六章：如何有效的选择</h2><p>预见对于合作的进化不是必要的，但是的确很有帮助。</p><p>在持续的囚徒困境中如何表现，作者向个体选择提供四个方面的建议：</p><ul><li>不要妒忌对方的成功<ul><li>要求自己比对方做的更好不是一个很好的标准，否则可能导致危险的冲突</li><li>一报还一报总是表现不错，但是他从来没有比别人得更多的分。</li></ul></li><li>不要首先背叛</li><li>要对合作和背叛都做出回报</li><li>不耍小聪明<ul><li>不要用一些复杂的方法来推断对方，这些推断通常是错误的。</li><li>自己的策略复杂到对方不可理解是非常危险的。</li><li>在非零和博弈中，诀窍在于鼓励合作，表明你愿意回报。</li></ul></li></ul><h2 id="第七章：如何促进合作"><a href="#第七章：如何促进合作" class="headerlink" title="第七章：如何促进合作"></a>第七章：如何促进合作</h2><p>改革者通过改变相互作用的条件来促进合作：</p><ul><li>增大未来的影响，增加接触频率</li><li>改变收益值，使利害关系可以被看到</li><li>教育人们相互关心</li><li>教育人们要回报</li><li>改进辨别能力</li></ul><h2 id="第八章：合作的社会结构"><a href="#第八章：合作的社会结构" class="headerlink" title="第八章：合作的社会结构"></a>第八章：合作的社会结构</h2><p>合作理论应用的领域。不同类型社会结构如何影响合作发展方式。</p><p>四个能引起有趣的社会结构形式的因素：</p><ul><li>标记：划分种群。</li><li>信誉：体现在其他人对他采用策略的信心上。</li><li>管理：政府处理个人间公司间纠纷。</li><li>领地：策略在领地上传播。</li></ul><h2 id="第九章：回报的鲁棒性"><a href="#第九章：回报的鲁棒性" class="headerlink" title="第九章：回报的鲁棒性"></a>第九章：回报的鲁棒性</h2><p>进化的方法基于一个简单的原则：成功的东西可能在将来经常出现。这本书研究从在没有集权的情况下，合作如何从自私者中产生，扩展到当人们却是关心会发生什么和当有集权时又会发生什么。其中友谊不是合作的进化必要的，预见性也不是必要的。</p><p>从小群体开始合作，在善良、可激怒和某种程度的宽容规则中逐步成长，一旦成为一个群体，采用这种有识别力的策略的个体就能保护自己不受侵入，总体的合作水平是在上升而不是在下降。换句话说，合作的进化是不可逆转的。</p><p>最有价值的发现是：具有预见能力的参与者了解合作理论的真谛后，可以加快合作的进化。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;《合作的进化》 罗伯特·阿克塞尔罗德&lt;/p&gt;</summary>
    
    
    
    <category term="Reading Note" scheme="https://superuier.github.io/categories/reading-note/"/>
    
    
  </entry>
  
  <entry>
    <title>腾讯云相关记录</title>
    <link href="https://superuier.github.io/software-tools/hexo/tencent-cloud-hexo/"/>
    <id>https://superuier.github.io/software-tools/hexo/tencent-cloud-hexo/</id>
    <published>2021-07-22T06:51:00.000Z</published>
    <updated>2021-08-05T12:11:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文记录在腾讯云服务器上部署 Hexo 搭建的博客时遇到的问题。</p><a id="more"></a><h1 id="腾讯云相关"><a href="#腾讯云相关" class="headerlink" title="腾讯云相关"></a>腾讯云相关</h1><h2 id="远程连接"><a href="#远程连接" class="headerlink" title="远程连接"></a>远程连接</h2><p>需要在本地使用 ssh 连接云端 root 账户或者普通账户。首先在腾讯云里 ubuntu 下管理员账户名为 ubuntu 而不是 root。</p><p>第一次链接出现管理员账户无法使用密码和密钥连接其普通账户也无法使用密码登录的情况。这时需要在腾讯云实例的控制台中对 ssh 进行相关设置。具体来说修改以下文件 <code>/etc/ssh/sshd_config</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PasswordAuthentication yes <span class="comment"># 开启密码登录权限</span></span><br><span class="line">PubkeyAuthentication yes <span class="comment"># 使用密钥登录</span></span><br></pre></td></tr></table></figure><p>之后重启服务 <code>sudo service sshd restart</code> 即可。之后使用 ssh 和下载好的密钥进行连接，详见<a href="https://cloud.tencent.com/document/product/1207/44643#.E4.BD.BF.E7.94.A8.E5.AF.86.E9.92.A5.E7.99.BB.E5.BD.95">官方文档</a>。</p><h2 id="使用-nginx-部署-Server"><a href="#使用-nginx-部署-Server" class="headerlink" title="使用 nginx 部署 Server"></a>使用 nginx 部署 Server</h2><p>详见<a href="https://zhuanlan.zhihu.com/p/108720935">这篇文档</a></p><p>其中涉及到不少对 Nginx 的操作，下面记录一些基础命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先使用 Nginx 命令的时候需要使用管理员权限。</span></span><br><span class="line"><span class="comment">## 开启服务器</span></span><br><span class="line">sudo systemctl nginx</span><br><span class="line"><span class="comment">## 重新启动更新设置</span></span><br><span class="line">sudo systemctl restart nginx.service </span><br></pre></td></tr></table></figure><p>其中 Nginx 的配置文件位于 <code>/etc/nginx/nginx.conf</code>，更改过后重启即可。</p><h2 id="服务器-SSL-证书安装部署"><a href="#服务器-SSL-证书安装部署" class="headerlink" title="服务器 SSL 证书安装部署"></a>服务器 SSL 证书安装部署</h2><p>可以参考腾讯云的<a href="https://cloud.tencent.com/document/product/400/35244">这篇文档</a>。主要就是将证书上传再再 Nginx 中设置。需要注意的是颁发证书对应的域名一定要和真是域名相同，不要少前缀。</p><h2 id="本地-ssh-连接服务器长时间不操作断开问题"><a href="#本地-ssh-连接服务器长时间不操作断开问题" class="headerlink" title="本地 ssh 连接服务器长时间不操作断开问题"></a>本地 ssh 连接服务器长时间不操作断开问题</h2><p>具体来说修改以下文件 <code>~/.ssh/config</code>。增添以下内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Host *</span><br><span class="line">        <span class="comment"># 断开时重试连接的次数</span></span><br><span class="line">        ServerAliveCountMax 5</span><br><span class="line">        <span class="comment"># 每隔5秒自动发送一个空的请求以保持连接</span></span><br><span class="line">        ServerAliveInterval 5</span><br></pre></td></tr></table></figure><p>参考<a href="https://www.pkslow.com/archives/ssh-keep-alive">这篇文章</a>。</p><h2 id="使用-webhook-对-repo-的更新进行监控"><a href="#使用-webhook-对-repo-的更新进行监控" class="headerlink" title="使用 webhook 对 repo 的更新进行监控"></a>使用 webhook 对 repo 的更新进行监控</h2><p>详见这几篇文档：</p><ul><li><a href="https://zhuanlan.zhihu.com/p/116136090">使用Github的Webhooks+Node完成网站的自动化部署</a></li><li><a href="https://jimmysong.io/blog/github-webhook-website-auto-deploy/">使用 GitHub Webhook 实现静态网站自动化部署</a></li><li><a href="https://jelly.jd.com/article/6006b1025b6c6a01506c878a">使用Github的webhooks进行网站自动化部署</a></li></ul><p>然而事实是我这次并没有从 webhook 的方向来部署，而是从 github action 中 ssh 到服务器进行操作。具体的部署步骤放在服务器的 deploy 文件中。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文记录在腾讯云服务器上部署 Hexo 搭建的博客时遇到的问题。&lt;/p&gt;</summary>
    
    
    
    <category term="Software Tools" scheme="https://superuier.github.io/categories/software-tools/"/>
    
    
    <category term="Hexo" scheme="https://superuier.github.io/tags/Hexo/"/>
    
    <category term="cloud" scheme="https://superuier.github.io/tags/cloud/"/>
    
  </entry>
  
  <entry>
    <title>Gaussian Process</title>
    <link href="https://superuier.github.io/math/gaussian-process/"/>
    <id>https://superuier.github.io/math/gaussian-process/</id>
    <published>2021-07-21T08:34:53.000Z</published>
    <updated>2021-07-21T08:34:53.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>一份理解高斯分布的笔记。</p><a id="more"></a><p>参考：</p><ul><li><a href="https://www.jgoertler.com/visual-exploration-gaussian-processes/">A Visual Exploration of Gaussian Processes</a></li><li><a href="https://zhuanlan.zhihu.com/p/56562456">看得见的高斯过程：这是一份直观的入门解读</a></li><li><a href="https://www.zhihu.com/question/46631426/answer/1735470753">如何通俗易懂地介绍 Gaussian Process？</a></li><li><a href="https://zhuanlan.zhihu.com/p/104601803">高斯过程回归：推导，实现和理解</a></li></ul><hr><h2 id="模型的理解"><a href="#模型的理解" class="headerlink" title="模型的理解"></a>模型的理解</h2><p>高斯分布一般用来做回归。其原理是把回归当成一个采样过程。在 n 个测试样本上预测的情况，可以想象成在一个 n 维的高斯分布下进行采样。在每个维度上的采样结果可以当作在这个样本上的预测值。所以说回归问题转化为如何构建这个 n 维的高斯分布，一旦构建完成则可以用来预测。</p><p>直接对预测数据$X$构建这个先验分布$P(X)$是困难的，所以说在有训练数据$Y$的情况下，我们期望找到的是一个后验概率分布$P(X|Y)$。</p><hr><h2 id="目标模型的构建"><a href="#目标模型的构建" class="headerlink" title="目标模型的构建"></a>目标模型的构建</h2><h3 id="1-包含训练数据的先验分布"><a href="#1-包含训练数据的先验分布" class="headerlink" title="1. 包含训练数据的先验分布"></a>1. 包含训练数据的先验分布</h3><p>首先我们需要先得到一个包含训练点 $X_a$ 和测试点 $X_b$ 先验分布 $P (X_a,X_b)$，维度为 $|X_a|+|X_b| = p + q$。这个先验分布同样也是高斯分布 $P(X_a,X_b) \sim \mathcal{N}(\mu,\,\Sigma)$。具体来说，</p><script type="math/tex; mode=display">X=\begin{bmatrix}X_a\\X_b\end{bmatrix}_{p+q}\quad\mu=\begin{bmatrix}\mu_a\\\mu_b\end{bmatrix}_{p+q}\quad \Sigma=\begin{bmatrix}\Sigma_{aa}&\Sigma_{ab}\\\Sigma_{ba}&\Sigma_{bb}\end{bmatrix}_{p + q, p + q}</script><p>在这里我们需要定义 $\mu$ 和 $\Sigma$。对于均值 $\mu$，$\mu_a$ 是从训练集中得到的数值，$\mu_b$则需要人为设定或取 $\mu_a$ 的均值。一般在数据归一化的情况下，先验均值可以设为 0 函数。</p><p>对于协方差矩阵 $\Sigma$，则可以用核函数来生成。核函数，一般用来表示一种相似度的度量，这里用每个样本的特征 $x$ 建立样本间的相似度，再用其作为每个维度之间的协方差。此处的核函数有多种选择方式，同时也可以组合起来使用。通过不同核函数的选择可以起到添加人类的先验知识的作用。通过这一个步骤可以建立起联合分布的高斯分布表达式。</p><h3 id="2-通过先验分布得到后验分布"><a href="#2-通过先验分布得到后验分布" class="headerlink" title="2. 通过先验分布得到后验分布"></a>2. 通过先验分布得到后验分布</h3><p>此时我们可以通过条件作用从 $P(X_a,X_b)$ 得到 $P(X_b|X_a)$。这样是从一个维度为$|X_a|+|X_b|$的高斯分布得到一个维度为维度为 $|X_b|$ 的高斯分布。条件作用之后均值和标准差会发生变化，依据高斯分布的性质可以得到以下条件分布，</p><script type="math/tex; mode=display">X_b|X_a\sim N(\mu_{b|a},\Sigma_{b|a})</script><p>其中，$\mu_{b|a}=\Sigma_{ba}\Sigma^{-1}_{aa}(X_a-\mu_a)+\mu_b$，$\Sigma_{b|a}=\Sigma_{bb}-\Sigma_{ba}\Sigma^{-1}_{aa}\Sigma_{ab}$。</p><p>直观上讲，训练点是为候选的函数设了一个限定范围，所得到的函数需要通过训练点。所以在结果中，靠近训练数据点的区域预测不确定性会小，离得越远，不确定性越大。</p><hr><h2 id="在无限维的条件下"><a href="#在无限维的条件下" class="headerlink" title="在无限维的条件下"></a>在无限维的条件下</h2><p>对于观测点 $X$ 与其对应值 $Y$，所有的非观测点 $X^<em>$ 的值定义为 $f(X^</em>)$。这里我们再把均值向量替换为均值函数。那么有，</p><script type="math/tex; mode=display">\begin{bmatrix}Y\\f(X^*)\end{bmatrix}\sim N(\begin{bmatrix}\mu(X)\\\mu(X^*)\end{bmatrix}，\begin{bmatrix}k(X,X)& k(X,X^*)\\k(X^*,X)&k(X^*,X^*)\end{bmatrix})</script><p>同样的我们可以得到条件分布，</p><script type="math/tex; mode=display">f(X^*)|Y\sim N(\mu^*,k^*)</script><p>其中, $\mu^\ast=k(X^\ast,X)k(X,X)^{-1}(Y-\mu(X))+\mu(X^\ast)$，$k^\ast=k(X^\ast,X^\ast)-k(X^\ast,X)k(X,X)^{-1}k(X,X^\ast)$。</p><p>同样的，一般在数据归一化的情况下，先验均值可以设为 0 函数。在 <code>sklean</code> 中，可以通过调节参数 <code>normalize_y=True</code> 实现。</p>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;一份理解高斯分布的笔记。&lt;/p&gt;</summary>
    
    
    
    <category term="Math" scheme="https://superuier.github.io/categories/Math/"/>
    
    
    <category term="math" scheme="https://superuier.github.io/tags/math/"/>
    
    <category term="gaussian-process" scheme="https://superuier.github.io/tags/gaussian-process/"/>
    
  </entry>
  
  <entry>
    <title>杠杆率无法衡量经济风险</title>
    <link href="https://superuier.github.io/economy-finance/didongsheng/dibi/"/>
    <id>https://superuier.github.io/economy-finance/didongsheng/dibi/</id>
    <published>2021-07-15T15:46:57.000Z</published>
    <updated>2021-07-15T15:46:57.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>2019.12.</p><p>该视频是翟币的由来视频。</p><a id="more"></a><h2 id="去杠杆概念的源起"><a href="#去杠杆概念的源起" class="headerlink" title="去杠杆概念的源起"></a>去杠杆概念的源起</h2><p>Ray Dalio，桥水基金创始人。他的核心概念就是去杠杆。</p><p>他认为债务/GDP不能太高而且不能增长太快，否则经济会崩塌。但是这个可能是在胡扯。逻辑可能是错的。</p><p>债务是存量概念，GDP是流概念。能不能得出有意义的结论？</p><p>举例，银行放债。企业的销售额除以债务能不能说明问题。京东和字节跳动相比，字节跳动销售额虽然可能小，但是利润更大。</p><h2 id="杠杆率与经济危机无关"><a href="#杠杆率与经济危机无关" class="headerlink" title="杠杆率与经济危机无关"></a>杠杆率与经济危机无关</h2><p>日欧债务率都相当高，但是即使负利率，都很债务安全。杠杆理论的反例，债务率低的崩盘了很多。</p><p>那什么具有决定性意义呢？债务的定价货币是什么。</p><h2 id="翟币"><a href="#翟币" class="headerlink" title="翟币"></a>翟币</h2><p>用10万翟币债券债务关系买手机。一年之后还钱只需要再印十万就好。</p><p>格林斯潘：“女士们先生们，这些钱我们永远都不用还。”</p><h2 id="中国政府债务"><a href="#中国政府债务" class="headerlink" title="中国政府债务"></a>中国政府债务</h2><p>中国没有借美元债，都是人民币债。中国债务率大家都觉得高。中央政府债务率占GDP比例20%都不到。但是地方政府借了很多债。</p><p>即便中央地方债务都算进去，也仅有60%左右。和德国中央政府差不多。</p><p>本币债和外币债有重要区别。本币债其实本质上是一种隐形的税收。</p>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;2019.12.&lt;/p&gt;
&lt;p&gt;该视频是翟币的由来视频。&lt;/p&gt;</summary>
    
    
    
    <category term="Economy &amp; Finance" scheme="https://superuier.github.io/categories/economy-finance/"/>
    
    
    <category term="翟东升" scheme="https://superuier.github.io/tags/%E7%BF%9F%E4%B8%9C%E5%8D%87/"/>
    
  </entry>
  
  <entry>
    <title>全球发展不平等，中国在其中扮演了什么角色？</title>
    <link href="https://superuier.github.io/economy-finance/seminar-talks/global-inequality-china/"/>
    <id>https://superuier.github.io/economy-finance/seminar-talks/global-inequality-china/</id>
    <published>2021-07-14T14:39:46.000Z</published>
    <updated>2021-07-14T14:39:46.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>海南绿色金融研究院为世界银行原首席经济学家Branco Milanovic教授与翟东生老师组织了一个线上对话。Branco 是个很有思想的学者，他提出的“大象曲线”用以描述和解释全球化时代的财富再分配效应及其政治后果，是国际政治经济学中的经典概念。</p><a id="more"></a> <h1 id="讲座部分"><a href="#讲座部分" class="headerlink" title="讲座部分"></a>讲座部分</h1><h2 id="Global-Income-Inequality-and-the-Role-of-China-全球收入不平等和中国的作用"><a href="#Global-Income-Inequality-and-the-Role-of-China-全球收入不平等和中国的作用" class="headerlink" title="Global Income Inequality and the Role of China 全球收入不平等和中国的作用"></a>Global Income Inequality and the Role of China 全球收入不平等和中国的作用</h2><div style="width:60%; margin:auto"><img src="/economy-finance/seminar-talks/global-inequality-china/gini.png" class=""></div><p>大概分成三个部分：</p><ul><li>1950年前，英国日本等经济增长，而中国下降。不平等增加。</li><li>1950-2000，二战后美国在不平等中占据重要地位，不平等并未加剧。</li><li>1980年后中国印度崛起，这些国家之前贫穷，不平等减少。</li></ul><p>虽然中国等国家的崛起减少的国家间的不平等，但是国内的不平等仍在加剧。中国对减少全球不平等起着积极作用。但是随着中国越来越富有，作为中上收入国家，其实中国是会加剧全球不平等。印度非洲对减少全球不平等越来越重要。</p><h2 id="Inequality-in-US-美国的不平等"><a href="#Inequality-in-US-美国的不平等" class="headerlink" title="Inequality in US 美国的不平等"></a>Inequality in US 美国的不平等</h2><p>美国自由主义资本中的系统性不平等：</p><ul><li>资本收入对总收入的重要性提升，资本集中度提高。</li><li>高收入人群劳动收入也很高。不像传统的资本家只有资本收入。高等级人群在劳动力市场和资本市场都很富有。（homoploutia）</li><li>人们门当户对的结合。选型婚配。（homogamy）</li><li>富有的人对政治控制程度的增高。</li><li>大学入学率与父母收入正相关。</li></ul><h2 id="Inequality-in-China-中国的不平等"><a href="#Inequality-in-China-中国的不平等" class="headerlink" title="Inequality in China 中国的不平等"></a>Inequality in China 中国的不平等</h2><p>中国的不平等在2010年之后稳定并逐步下降。城镇不平等和总不平等高度相关。</p><div style="width:60%; margin:auto"><img src="/economy-finance/seminar-talks/global-inequality-china/gini-china.png" class=""></div><p>三个可解决的不平等，相比之下，美国较小：</p><ul><li>省份收入的不平等。可以提高GDP用财政政策解决。</li><li>城乡地理间收入的不平等。可以通过劳动力流动和解决户口解决。</li><li>腐败带来的不平等。反腐政策解决。</li></ul><p>三个可困难的不平等：</p><ul><li>资本收入增加。越高收入的人，资本收入越多。</li><li>高等级人群在劳动力市场和资本市场都很富有。近十年此指标增长较高。homoploutia。</li><li>社会流动性降低，教育与父母收入相关。</li></ul><h1 id="提问部分（翟东升）"><a href="#提问部分（翟东升）" class="headerlink" title="提问部分（翟东升）"></a>提问部分（翟东升）</h1><ol><li>质疑一下中国的发展未来会限制世界不平等，同时更依赖印度非洲等地区对不平等作出贡献。仅从GDP角度分析可能是这样，但是还是应该考虑一些其他因素。中国正在转型为资本提供者。从2013年来，中国把越来越多的钱投入到全球外围地区。所谓的一带一路。如果中国对人类命运共同体贡献成功，还是可以减轻全球不平等。在未来我们会通过长期促进全球外围地区发展来推动平等。</li><li>homoploutia，其实是一个可以解决的办法。新时代中国特色社会主义市场经济，整个世界的体制正在趋同。什么样的资本主义应该实行？美国的撒钱（helicopter money）更像共产主义来。欧洲的高福利其实使人懒惰。如何平衡成本和收益？对欧洲福利进行调整，构建一种社会福利体系。欧洲很多的福利被用于或者浪费于公共系统，或者让老年人多活一个月，这是性价比很低的。公共部门为了挽救人们生命和财富分配的干预应该致力于减少年轻人之间的不平等。年轻人无法选择，但是年长的人应该对自己的行为负责。所以财力应该节省出来给年轻人。中国的资本增长主要在于地产，年轻人要承担这个后果。在中国我们应该补偿这种代际剥夺。</li><li>抛去资本所有权的集中，更应该谈论数据所有权的集中。1971年之前资本指黄金，之后指大国的主权信用。1980后我们见证着资本稀缺性的下降。目前最重要的是数据的所有权。在美国是公司高管们拥有这些数据。</li></ol><h1 id="回复部分（BM）"><a href="#回复部分（BM）" class="headerlink" title="回复部分（BM）"></a>回复部分（BM）</h1><ol><li>一带一路可以为非洲带来增长，但是技术上讲，还是会不可避免的加剧不平衡。</li><li>代际不平衡类似人口老龄化，这种不平衡在美欧都有。年轻人进步的机会比其父母更少。</li><li>资本收入包括数据等资产。资本的种类的确在变化。</li></ol>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;海南绿色金融研究院为世界银行原首席经济学家Branco Milanovic教授与翟东生老师组织了一个线上对话。
Branco 是个很有思想的学者，他提出的“大象曲线”用以描述和解释全球化时代的财富再分配效应及其政治后果，是国际政治经济学中的经典概念。&lt;/p&gt;</summary>
    
    
    
    <category term="Economy &amp; Finance" scheme="https://superuier.github.io/categories/economy-finance/"/>
    
    
    <category term="翟东升" scheme="https://superuier.github.io/tags/%E7%BF%9F%E4%B8%9C%E5%8D%87/"/>
    
  </entry>
  
  <entry>
    <title>主动学习与交互决策过程</title>
    <link href="https://superuier.github.io/machine-learning/active-reinforcement/"/>
    <id>https://superuier.github.io/machine-learning/active-reinforcement/</id>
    <published>2021-06-24T09:00:00.000Z</published>
    <updated>2021-06-24T09:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>之前导师让调研一下“基于主动学习技术的可交互策略学习方法”，此处整理成文便于留存。</p><a id="more"></a><h1 id="调研背景介绍"><a href="#调研背景介绍" class="headerlink" title="调研背景介绍"></a>调研背景介绍</h1><p><font color=red>调研关键词：</font>主动学习，可交互，策略学习，降低标注成本</p><h2 id="相关领域"><a href="#相关领域" class="headerlink" title="相关领域"></a>相关领域</h2><h3 id="主动学习"><a href="#主动学习" class="headerlink" title="主动学习"></a>主动学习</h3><p>首先我们指出广义场景下<strong>主动学习</strong>有两种含义，技术面和任务面（不冲突，侧重点不同）。但是不论是指技术还是任务，当我们提到主动学习的时候，总是伴随着对某项<strong>成本降低的期望</strong>。</p><ul><li>技术面：一种以某种原则选取数据的技术（此种含义不必须与人交互）</li><li>任务面：一种以某种原则选取数据从而节省监督学习标记成本的任务（标记任务需要与人交互）</li></ul><h3 id="策略学习"><a href="#策略学习" class="headerlink" title="策略学习"></a>策略学习</h3><p>一般来说，策略是在指定任务中为完成任务目标而实行的行动准则。目前来看，有两种被广泛使用的<strong>策略学习</strong>方法：</p><ul><li>强化学习（需要与环境交互）</li><li>模仿学习（需要与专家系统交互）</li></ul><hr><h2 id="具体调研的方向"><a href="#具体调研的方向" class="headerlink" title="具体调研的方向"></a>具体调研的方向</h2><p>故此调研的调研重点置于主动学习AL与强化学习RL与模仿学习IL的交叉方向上面(<strong>以降低成本为目的</strong>)。<u><em>此处的分析及本文的调研以问题为导向，不特别深入到具体技术的归类。</em></u></p><p>问题分类（调研结构以此为标准）：</p><ul><li>传统AL数据标记任务下的问题（监督学习中设计相关策略减少标记成本）（标记任务中与人交互）<ul><li>使用RL/IL的policy作为AL标记任务的选取策略（AL的任务面）</li></ul></li><li>RL场景下的问题（有时与环境交互复杂昂贵）<ul><li>定义一种数据选取的原则来指导policy的学习（AL的技术面）</li></ul></li><li>IL场景下的问题（与专家系统交互复杂昂贵）<ul><li>定义一种数据选取的原则来指导policy的学习（AL的技术面）</li></ul></li></ul><p>另一种分类（在此调研中可找到相应模块）：</p><ul><li>在策略学习中无人类参与<ul><li>Active Reinforcement Learning</li></ul></li><li>在策略学习中有人类参与<ul><li>Imitation/Reinforcement Learning in Conventional AL Process</li><li>Interactive Reinforcement Learning</li><li>Active Imitation Learning</li></ul></li></ul><hr><h1 id="三类相关方向"><a href="#三类相关方向" class="headerlink" title="三类相关方向"></a>三类相关方向</h1><h2 id="1-传统AL数据标记任务"><a href="#1-传统AL数据标记任务" class="headerlink" title="1. 传统AL数据标记任务"></a>1. 传统AL数据标记任务</h2><h3 id="背景简介"><a href="#背景简介" class="headerlink" title="背景简介"></a>背景简介</h3><p>在AL的任务面下， AL与RL有着相似的任务框架，都可以看作MDP。</p><div style="width:60%;margin:auto"><img src="/machine-learning/active-reinforcement/agent_environment_MDP.png" class=""></div><div style="width:60%;margin:auto"><img src="/machine-learning/active-reinforcement/AL_framework.png" class=""></div><p>两类问题描述如下：</p><div class="table-container"><table><thead><tr><th></th><th>RL</th><th>AL（任务面）</th></tr></thead><tbody><tr><td>Task</td><td>Decision tasks</td><td>Instance selection tasks with supervised learning</td></tr><tr><td>Purpose</td><td>Learn best policy to maximize the total return</td><td>Construct best labeled condition (coming with best prediction results)</td></tr><tr><td>Goal</td><td>Policy</td><td>Labeled condition</td></tr></tbody></table></div><p>我们将两类问题的相关概念进行对比如下：</p><div class="table-container"><table><thead><tr><th></th><th>RL</th><th></th><th>AL（任务面）</th></tr></thead><tbody><tr><td>agent</td><td>Apply [current state =&gt; action]</td><td>selector</td><td>Apply [current labeled state =&gt; selection]</td></tr><tr><td>state</td><td>Describe current condition</td><td>labeled state</td><td>Current labeled and unlabeled data</td></tr><tr><td>action</td><td>The response from the agent under the certain state</td><td>selection</td><td>The response from the selector under the curtain labeled state. (A set of unlabeled data)</td></tr><tr><td>environment</td><td>Apply [state + action =&gt; next state + reward]</td><td>oracle/interaction</td><td>Apply [labeled state + selection =&gt; next labeled state]. Oracle annotate the selection set.</td></tr><tr><td>reward</td><td>Given by the environment after an action is taken</td><td></td><td></td></tr><tr><td>policy</td><td>The guideline for the agent to take optimal actions</td><td>query strategy</td><td>The guideline for the selector to make optimal selections</td></tr><tr><td>value</td><td>Evaluation of the current state/action/state-action-pair</td><td>score</td><td>Evaluation of the current potential selections</td></tr><tr><td>model (not necessary)</td><td>The learned formula of the environment</td><td>model</td><td>The learned formula of the oracle</td></tr></tbody></table></div><p>我们发现RL中的Policy其实对应着AL中的选取策略。在此方向上，我们可以发现两者的区别</p><ul><li>相似处：RL policy和AL strategy都应该同时考虑exploration&amp;exploitation。</li><li>不同处：RL学习policy，但是AL通常使用预先启发式设定的strategy。</li></ul><hr><h3 id="问题与思路"><a href="#问题与思路" class="headerlink" title="问题与思路"></a>问题与思路</h3><p><font color=red>存在的问题：</font>AL通常使用预先启发式设定的strategy其实很多时候并不是最优，而且对不同场景不一定适用。这就带来了一个问题，这个预先设定好的heuristic有时候并不是最优，对不同场景不一定适用。</p><p><font color=red>直观的方法：</font>从RL与AL的相似性出发，我们或许可以运用强化学习学习policy的方法来学习一个strategy。</p><p>目前主要有三类工作（数据标记任务，所以需要与人简单交互）：</p><ul><li>在一个相关的领域学习策略<ul><li>之后不伴随策略在数据集间的迁移</li><li>之后伴随策略在数据集间的迁移</li></ul></li><li>直接在目标领域学习策略</li></ul><p><font color=red>KEY WORDS：</font> Learning to actively learn, Data-driven AL, Meta-active learning (ability to work with all kinds of data)</p><hr><h3 id="现有工作列表"><a href="#现有工作列表" class="headerlink" title="现有工作列表"></a>现有工作列表</h3><p>在一个相关的领域学习策略，之后不伴随策略在数据集间的迁移：</p><ul><li><a href="http://papers.nips.cc/paper/7010-learning-active-learning-from-data">Learning active learning from data [2017, NIPS]</a>: <strong>LAL</strong>.<strong>In pool-based AL setting</strong>.Random forest as basic classifiers.The state consist the current classifier parameters and a randomly selected data (Monte-Carlo).Train a random forest regressor that predicts the expected error reduction for a candidate sample in a particular learning state.The regressor works as the value function of the state action pair.Train the policy on the representative dataset and use it on the target dataset (without updating).</li><li><a href="https://www.aclweb.org/anthology/P18-1174.pdf">Learning How to Actively Learn: A Deep Imitation Learning Approach [2018, Annual Meeting of the Association for Computational Linguistics]</a>: <strong>ALIL</strong>.<strong>In pool-based AL setting</strong>.The task is named entity recognition in a cross lingual setting.The state consists of the labeled and unlabelled datasets paired with the parameters of the currently trained model.An action corresponds to the selection of a query data point, and the reward function is the loss of the trained model on the hold-out evaluation set.Imitation Learning directly learns the map (policy) from state to action in a supervised manner.The policy is then used on the target dataset (without updating).</li><li><a href="https://arxiv.org/pdf/2010.15382.pdf">Learning to Actively Learn: A Robust Approach [2020]</a>: Address that previous works in this area learn a policy by optimizing with respect to data observed through prior experience (e.g., metalearning or transfer learning).This approach makes no assumptions about what parameters are likely to be encountered at test time, and therefore produces algorithms that do not suffer from a potential mismatch of priors.</li></ul><p>在一个相关的领域学习策略，之后伴随策略在数据集间的迁移：</p><ul><li><a href="https://arxiv.org/abs/1708.02383">Learning how to Active Learn: A Deep Reinforcement Learning Approach [2017, Arxiv]</a>: <strong>PAL</strong>(Policy based Active Learning).<strong>In stream-based AL setting</strong> where the policy/strategy is to decide whether to query the current instance.The task is named entity recognition in a cross lingual setting, where transfer the learned strategy/policy to the target domain (where no enough data to learn a strategy).The policy is learned by deep Q-network.The state is the current instance the previous constructed dataset.The reward is given by a hold-out set.Then the learned policy is transferred to the target dataset with policy updating.</li></ul><p>直接在目标领域学习策略：</p><ul><li><a href="https://ieeexplore.ieee.org/abstract/document/6248108">RALF: A reinforced active learning formulation for object class recognition [2012, CVPR]</a>: First to consider AL as a MDL process.They use Q-learning to learn the adaptive combination between exploration and exploitation.Use the overall entropy as the reward in each iteration.The state is the strategy combination condition and the action is the trade-off parameter.They also used a guided initialization for Q-table.</li><li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.707.8414&amp;rep=rep1&amp;type=pdf">Active Learning by Learning [2015, AAAI]</a>: <strong>ALBL</strong>.By Hsuan-Tien Lin (NTU).Connect AL with the well-known multi-armed bandit problem.Choose strategies in the AL process by estimating the performance of different strategies on the fly.Use EXP4.P as the core solver.The action is to choose a specific strategy.</li><li><a href="https://openaccess.thecvf.com/content_CVPR_2019/html/Yoo_Learning_Loss_for_Active_Learning_CVPR_2019_paper.html">Learning Loss for Active Learning [2019, CVPR]</a>: Attach a small parametric module, named “loss prediction module”, to a target network, and learn it to predict target losses of unlabeled inputs.The “loss prediction module” could be considered as the value function of the state-action pair.The state is the current model parameters and the action is the selected instance.The idea to directly predict the loss is similar to <em>[Learning active learning from data [2017, NIPS]]</em>, but this work doesn’t need to train the policy in advance.The policy is trained during the AL process.</li><li><a href="https://www.aclweb.org/anthology/P19-1401.pdf">Learning How to Active Learn by Dreaming [2020, ACL]</a>: The follow-up work for <em>[Learning How to Actively Learn: A Deep Imitation Learning Approach]</em>.Recent data-driven AL policy learning methods are also restricted to learn from closely related domains.This method fine-tune the initial AL policy directly on the target domain of interest by using wake and dream cycles.Cross-domain and cross-lingual text classification and named entity recognition tasks.Wake learning is an AL process, and the dream learn is a policy updating process.The current weak model was used as a weak annotator to train the policy in the dream phase.</li></ul><p>此类方法在其他类型标注任务中的相关应用:</p><ul><li><a href="https://dl.acm.org/doi/abs/10.1145/3340555.3353742">Multi-modal Active Learning From Human Data: A Deep Reinforcement Learning Approach [2019, ICMI]</a></li><li><a href="https://arxiv.org/abs/2002.06583">Reinforced active learning for image segmentation [2020, ICLR]</a></li><li><a href="https://www.aclweb.org/anthology/K18-1033.pdf">Learning to Actively Learn Neural Machine Translation [2020, CoNLL]</a></li><li><a href="https://link.springer.com/chapter/10.1007%2F978-3-030-59710-8_4">Deep Reinforcement Active Learning for Medical Image Classification [2020, MICCAI]</a></li></ul><hr><h3 id="分析与看法"><a href="#分析与看法" class="headerlink" title="分析与看法"></a>分析与看法</h3><p>首先这一节我们面对的还是传统的AL场景，面对的主要是数据标注的任务。相比于传统的AL启发式方法，此类学习Policy的方法对算力的要求很大，虽然标称的表现也会好，但是如果真的实际使用则需要权衡。</p><p>浅见：</p><ul><li>对不同任务设计特异性的AL策略（不论是启发还是学习一个策略）是当下的趋势。</li><li>对于一些复杂任务，基于RL策略的迁移或许是一种良好的解决方案。</li></ul><hr><h2 id="2-Reinforcement-Learning-场景"><a href="#2-Reinforcement-Learning-场景" class="headerlink" title="2. Reinforcement Learning 场景"></a>2. Reinforcement Learning 场景</h2><h3 id="背景简介-1"><a href="#背景简介-1" class="headerlink" title="背景简介"></a>背景简介</h3><p>强化学习相关知识与概念在此不赘述。具体可以参见<a href="/machine-learning/reinforcement-learning/" title="这篇文章">这篇文章</a>。</p><p>强化学习面对的问题场景就是一个广义的学习一个智能 agent 的场景。一般通过与环境的交互来学习 agent 的 policy。环境交互的过程中一般会黑盒地出状态的转移以及相应的奖励。Policy 的训练依赖于观测的结果。一般是在观测到的 trajectory 中进行训练: <script type="math/tex">\{S_0,A_0,R_0,S_1,A_1,R_1,S_2,A_2,R_2,...,S_{n-1},A_{n-1},R_{n-1},S_n,R_n\}</script>（主流的强化学习方法分为 model-based 与 model-free 两种，区别在 policy 的训练是否依赖对环境的状态转移建模。）</p><p>训练的过程中存在一个问题：若持续以当前的学到的策略与环境进行交互（exploitation），那么学习的结果则容易陷入一个局部最优当中。所以在强化学习中 exploitation 需要伴随 exploration，这样可以避免陷入局部最优并且找到更好的策略。最简单的分配方式是 $\epsilon-greedy$，按照一定比例分配 exploitation 和 exploration。如果 exploitation 过多，则容易陷入局部最优，如果 exploration 过多，则较难有充足的数据训练一个较好的policy。所以说 exploitation 之外的 exploration 也十分重要。</p><p>通常情况下RL假设与环境的交互是即时完成且没有花费的。然而现实情况下，与环境的交互，通常是需要付出代价的（时间或算力或实际成本）。在 large, high-dimensional environment 中，这个成本更为高昂（复杂的模拟计算等）。此时我们希望我们每一次的交互都能被更好的利用起来：</p><ul><li>在 exploitation 中用更好的训练方式（大部分强化学习在研究的的）</li><li>在 exploration 中选择更好的交互样本，更高效探索（active）</li></ul><p>于是RL则与AL选取更优样本的逻辑产生了交叉，主要是聚焦于探索的场景下。《人工智能：一种现代的方法》21.3中的主动强化学习提到了这一现象。我们想更优更高效地 exploration（Agent 不必关心那些它知道不需要的而且可以避开的状态的具体效用）。此处我们称这种选取为 active reinforcement learning。</p><p>此处的 Active 不一定一定要有人的存在。这里的 Active 含义: the agent seeks out novelty based on its own “internal” estimate of what action sequences will lead to interesting transitions.</p><hr><h3 id="问题与思路-1"><a href="#问题与思路-1" class="headerlink" title="问题与思路"></a>问题与思路</h3><p><font color=red>存在的问题：</font>RL与环境交互会产生交互成本。较多的探索会影响模型的训练，同时也会造成一定的cost损失。</p><p><font color=red>直观的方法：</font>借鉴AL的思路，制定或学习探索的方式，更高效的探索，使相同数量的探索下训练的policy更好。</p><p>目前主要有以下几类工作：</p><ul><li>ARL: 整体框架仍是RL的框架。加入探索的heuristic。（不与人交互，只与环境交互）<ul><li>ARL-1: 与环境交互获取状态转移和奖励时产生cost</li><li>ARL_2: 只在与环境交互获取奖励时产生cost，状态转移不产生cost。</li></ul></li><li>IRL: 以RL的框架为基础，添加interactive的部分。（需要与人交互）</li></ul><p><font color=red>KEY WORDS：</font> active exploration, active reinforcement learning, interactive reinforcement learning</p><hr><h3 id="现有工作"><a href="#现有工作" class="headerlink" title="现有工作"></a>现有工作</h3><p>ARL-1:与环境交互获取状态转移和奖励时产生cost。(No human interactions)</p><p>由于需要主动的选择来与环境进行交互，一般来说是要选取我们认为环境能反馈更多信息的样例。通常的做法是在对环境的建模上来衡量informativeness。具体衡量的方法与AL中的方法相似（QBC/Entropy/Information Gain, etc.）所以说绝大部分的工作都是基于Model-based RL。</p><ul><li><a href="https://dl.acm.org/doi/abs/10.1145/1390156.1390194">Active Reinforcement Learning [ICML, 2008]</a>: Before this work, many ideas are trying to explore the actions which agents has experienced the fewest number of times in the past.In this paper, the agent determines the sensitivity of the optimal policy to changes in transitions and rewards.It then focuses its exploration on the regions of space to which the optimal policy is most sensitive.The process is similar to ADP.Use Taylor’s approximation to model the local sensitivity of the utility on current policy.</li><li><a href="https://proceedings.neurips.cc/paper/2016/file/abd815286ba1007abfbb8415b83ae2cf-Paper.pdf">VIME: Variational Information Maximizing Exploration [2016, NIPS]</a>:Agent should take actions that maximize the reduction in uncertainty about the dynamics (transition).Use BNN to learn the model.This can be formalized as maximizing the sum of reductions in entropy of transition probability.The exploration to the uncertain state of the models should reward more.</li><li><a href="http://proceedings.mlr.press/v97/shyam19a.html">Model-based active exploration [2019, ICML]</a>: Better state-action pair makes the model ensemble changes more.The utility of the state action pair is the disagreement among the next-state distributions given s and a, in terms of JSD (Jensen-Shannon Divergence), of all possible transition functions weighted by their probability.(用可能的transaction分布变化作为criteria。)</li><li><a href="https://arxiv.org/abs/1906.04161">Self-supervised exploration via disagreement [2019, ICML]</a>: The idea is similar to <em>Model-based active exploration</em>.Generate an intrinsic reward, defined as some difference measure (i.e., KL-divergence, total variation) across the output of different models in the ensemble drives exploration.(Such exploration might be ineffective, as the policy may visit regions of the state space which have no relation to solving the task.)</li><li><a href="https://proceedings.neurips.cc/paper/2019/hash/03b264c595403666634ac75d828439bc-Abstract.html">Explicit explore-exploit algorithms in continuous state spaces [2019, NIPS]</a>: Applicable in large or infinite state spaces.Exploration and exploitation are controlled by a threshold.Exploration is guided by the disagreement in the model set.And the models updated after get the exploration trajectories.Exploitation is guided by one of the model in the model set.</li><li><a href="https://arxiv.org/pdf/2002.02693.pdf">Ready Policy One: World Building Through Active Learning [2020, Arxiv]</a>: View MBRL exploration as an active learning problem (select trajectory from trajectory space), where we aim to improve the world model in the fewest samples possible.Acquiring data that most likely leads to subsequent improvement in the model.The exploration metric is based on <strong>reward variance</strong> computed from a (finite) collection of models.</li><li><a href="https://arxiv.org/pdf/2006.09436.pdf">SAMBA: Safe Model-Based &amp; Active Reinforcement Learning [2020, Arxiv]</a>: From Huawei UK.<strong>Safe-RL</strong> setting, every trajectory comes with a cost.Aim at minimizing cost, maximizing active exploration (bi-objective), and meeting safety constraints.Increases in regions with dense training-data (due to the usage of CVaR constraint) to aid agents in exploring novel yet safe tuples.The exploration use an expected leave-one-out semi-metric between two Gaussian processes defined, for a one query state-action pair. (Model change)</li></ul><hr><p>ARL-2:只在与环境交互获取奖励时产生cost，状态转移不产生cost。</p><p>Specify the cost C:</p><ul><li><a href="https://arxiv.org/abs/1803.04926">Active reinforcement learning with monte-carlo tree search [2018]</a>: The reward needs to be paid to observe.The received reward has the discount for the querying cost.</li><li><a href="https://arxiv.org/abs/2005.12697">Active Measure Reinforcement Learning for Observation Cost Minimization [2020]</a>: The policy applied with Q-learning.Add whether to query as part of the action in Q-table, so the number of action would be twice as large as the ordinary Q-table.After each query, it jump to the queried state and update a transition model.Otherwise, it go to the state according to the transition model.(没太懂这篇文章的intuition)</li></ul><hr><p>IRL:与环境交互获取状态转移和奖励时产生cost，人类交互不产生cost</p><p>此类问题的背景仍旧是搜索空间大，搜索耗时且复杂。所以仍然是在与环境交互时产生cost，希望使用人类专家在训练时的介入来缓解这个问题。人类专家的交互仍然是为了使exploration更高效。但是需要强调，此处的问题并没有对与人类专家交互的数量与质量进行假设。</p><ul><li><a href="https://www.cc.gatech.edu/~ksubrama/files/efd-paper-aamas16.pdf">Exploration from Demonstration for Interactive Reinforcement Learning [2016, aamas]</a>: A model-free policy-based approach, uses human demonstrations to guide search space exploration.The demonstrations are used to learn an exploration policy that actively guides the agent towards important aspects of the problem.In the proceeding of a episode, when the agent reaches a informative state (computed by leverage and discrepancy), it queries the action from the state with the closest leverage to the current state, then update the policy parameters.A self-play with the environment followed in this episode, and the parameters are keep updating.</li><li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/InteractiveTeaching.pdf">Interactive Teaching Strategies for Agent Training [2016]</a>:A work by Microsoft.Include two agent: student(learn from RL) and teacher(fixed human policy).Contains two module: student initiated and teacher initiated.Query when the student has a low Q-value difference on the current state.(the student is uncertain about which action to take)</li><li><a href="https://arxiv.org/pdf/1701.04079.pdf">Agent-Agnostic Human-in-the-Loop Reinforcement Learning [2016, NIPS]</a>:Develop a framework for human-agent interaction that is agent-agnostic and can capture a wide range of ways a human can help an RL agent (e.g. Q-values, action optimality, or the true reward.)</li></ul><hr><h3 id="分析与看法-1"><a href="#分析与看法-1" class="headerlink" title="分析与看法"></a>分析与看法</h3><p>ARL和普通RL的工作界限其实并不是很明显，因为很多 RL 的工作也是需要处理 exploitation 和 exploration。此处只是挑了一些很明显使用到 AL 相关思路的文章，如果要详尽调研则较为困难。</p><p>IRL 其实比较有趣，它主要是想通过人的参与让与环境的交互更高效，人在这里只是起到画龙点睛的作用，或许这种交互可以用到很多 RL 系统。</p><hr><h2 id="3-Imitation-Learning-场景"><a href="#3-Imitation-Learning-场景" class="headerlink" title="3. Imitation Learning 场景"></a>3. Imitation Learning 场景</h2><h3 id="背景简介-2"><a href="#背景简介-2" class="headerlink" title="背景简介"></a>背景简介</h3><p>模仿学习相关知识与概念在此不赘述。具体可以参见<a href="/machine-learning/reinforcement-learning/" title="这篇文章">这篇文章</a>。</p><p>简单来说，模仿学习会在学习的过程中与专家交互，同时获得专家对指定 state 所作出的 action <script type="math/tex">\pi_*(s)</script>。模仿学习的目标为学习一个 policy，使其与专家的 policy 尽可能接近（模型生成的状态-动作轨迹分布和输入的轨迹分布相匹配/获得同样的 reward 等）。</p><p>通常情况下，根据应用场景的不同，IL可以分为以下几类（By ICML 2018 IL Tutorial）:</p><ul><li>Behavior cloning</li><li>Direct policy learning (multiple step BC)</li><li>Inverse reinforcement learning (assume learning R is statistically easier)</li></ul><p>与RL类似，IL同样是在观测到的trajectory中进行训练: <script type="math/tex">\{S_0,A_0,S_1,A_1,S_2,A_2,...,S_{n-1},A_{n-1},S_n\}</script>。不同的是，在 trajectory 中并没有每一步的 reward。</p><hr><h3 id="问题与思路-2"><a href="#问题与思路-2" class="headerlink" title="问题与思路"></a>问题与思路</h3><p><font color=red>存在的问题：</font>当我们想要通过模仿来学习策略的时候，唯一的学习来源来自于 Expert 对所询问的 stat e的回答（相应 action）。这种与 Expert 的交互是昂贵的。</p><p><font color=red>直观的方法：</font>询问专家时，挑选更有价值的 state 来询问相应的 action。</p><p>目前的工作也是基于 IL 的分类，存在于以下两类：</p><ul><li>Direct Policy Learning</li><li>Inverse reinforcement learning</li></ul><p>在 IL 中，一部分工作使用一定数量与<strong>专家</strong>交互下 policy 的表现作为评估，类似于 AL 中的学习曲线。所以说一部分 IL 天然的可以看作 AL 的问题（以 AL 选取策略降低与专家交互成本）。（另一部分使用一定数量与<strong>环境</strong>交互下 policy 的表现作为评估，是以人的参与降低与环境交互成本，类似之前提到的 IRL。）</p><p><font color=red>KEY WORDS：</font> imitation learning, active imitation learning, active inverse reinforcement learning</p><hr><h3 id="现有工作-1"><a href="#现有工作-1" class="headerlink" title="现有工作"></a>现有工作</h3><p>Under Direct Policy Learning:</p><p>此类工作的目标是学一个 state 到 action 的 map，是一个监督学习的过程，选取的方式基于学习到的 supervised model。</p><ul><li><a href="https://jmlr.org/papers/volume15/judah14a/judah14a.pdf">Active Imitation Learning: Formal and Practical Reductions to I.I.D. Learning [2014, JMLR]</a>: Under the subclass <em>Direct Policy Learning</em>.Construct a committee by bootstrap sampling (posterior over policies).Select states by density weighted QBC strategy and get the corresponding action.The density is from trajectory from the simulated environment.Maintain a supervised labeled state-action pairs.</li><li><a href="https://dl.acm.org/doi/abs/10.5555/3298483.3298654">Query-Efﬁcient Imitation Learning for End-to-End Simulated Driving [2017, AAAI]</a>:Maintain a safety classifier to decide which state needs to be queried.Use supervised loss between the action given by trained policy and reference policy on the visited states.</li></ul><hr><p>Under Inverse reinforcement learning:</p><p>由于目标之一是学习 value function，所以通常是以当前的 value function 来构建选取 state 的逻辑。</p><ul><li><a href="https://link.springer.com/chapter/10.1007/978-3-642-04174-7_3">Active Learning for Reward Estimation in Inverse Reinforcement Learning [2009, ECML PKDD]</a>: Take the entropy of the value prediction on the state as the sample strategy (where the reward is unsure).i.e. the disagreement on the learned reward functions.It query the specific action from the oracle on the selected state. (162)</li><li><a href="https://www-cs.stanford.edu/people/ebrun/pdfs/mandel2017add.pdf">Where to Add Actions in Human-in-the-Loop Reinforcement Learning [2017, AAAI]</a>:Selecting the state s to query the action where adding the next action most improves the estimated value of state.</li><li><a href="http://proceedings.mlr.press/v80/kang18a/kang18a.pdf">Policy Optimization with Demonstrations [2018, ICML]</a>:Use discriminator between human demonstrations and trajectory from environment to make the exploration close to the human’s action.The purpose is to use the current demonstration to train a policy as good as possible.(Demonstration is collected in loops, could be considered as an AL process.)</li><li><a href="https://arxiv.org/pdf/1905.11867.pdf">Interactive Teaching Algorithms for Inverse Reinforcement Learning [2019, IJCAI]</a>:Focus on how could a teacher provide an informative sequence of demonstrations to an IRL learner to speed up the learning process.The main idea is to pick a demonstration with maximal discrepancy between the learner’s current policy and the teacher’s policy in terms of expected reward.</li></ul><hr><h3 id="分析与看法-2"><a href="#分析与看法-2" class="headerlink" title="分析与看法"></a>分析与看法</h3><p>Imitation Learning 天然和 AL 有很多相似的地方。AL 是想模仿 oracle 的黑盒模型，IL 是想模仿 expert 的黑盒策略。人在 IL 中的重要程度就像人在 AL 中的重要程度一样，因为人类是系统唯一可以获得监督信息的地方。所以说本节所在意的并不是环境的交互成本，而是人类的交互成本，这就使 AIL 与 IRL 产生了区别（问题的定义不同）。</p><hr><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文以两种维度介绍了主动学习与可交互策略相关的问题，并以第一种维度进行了展开（第二种维度同样可以找到相应模块）。</p><p>其中对于人类专家存在的交互式的策略学习，当前已经有了一些综述或者前瞻的文章。这些工作主要介绍了不同交互的方式与目的，并不一定包含减少标记成本的模块。</p><ul><li>Power to the people: The role of humans in interactive machine learning [2014, AI Magazine]</li><li><a href="https://arxiv.org/pdf/1811.07871.pdf">Scalable agent alignment via reward modeling: a research direction [2018]</a>: From Deepmind.</li><li><a href="https://arxiv.org/pdf/1909.09906.pdf">Leveraging Human Guidance for Deep Reinforcement Learning Tasks [2019, IJCAI]</a></li><li><a href="https://dl.acm.org/doi/abs/10.1145/3357236.3395525">A Survey on Interactive Reinforcement Learning: Design Principles and Open Challenges [2020, ACM DIS]</a></li></ul><h2 id="最新进展-amp-发展趋势"><a href="#最新进展-amp-发展趋势" class="headerlink" title="最新进展&amp;发展趋势"></a>最新进展&amp;发展趋势</h2><p>本文并未对相关技术的最新进展展开探讨，但是从问题的角度来讲的确出现了一些新问题。包括但不限于：</p><ul><li>由 active RL 到 active safe RL</li><li>IRL 中考虑交互的方式（不同形式的指导，或者相同形式指导的不同展现方式）</li></ul><p>但是现在来看问题的大类并没有发生什么改变。交互的成本只有两类：与环境交互成本，与人类专家交互成本。在这次简要的调查中并没有发现其他类的问题。</p>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;之前导师让调研一下“基于主动学习技术的可交互策略学习方法”，此处整理成文便于留存。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://superuier.github.io/categories/Machine-Learning/"/>
    
    
    <category term="machine-learning" scheme="https://superuier.github.io/tags/machine-learning/"/>
    
    <category term="reinforcement-learning" scheme="https://superuier.github.io/tags/reinforcement-learning/"/>
    
    <category term="imitation-learning" scheme="https://superuier.github.io/tags/imitation-learning/"/>
    
    <category term="active-learning" scheme="https://superuier.github.io/tags/active-learning/"/>
    
  </entry>
  
  <entry>
    <title>强化学习 &amp; 模仿学习基础知识</title>
    <link href="https://superuier.github.io/machine-learning/reinforcement-learning/"/>
    <id>https://superuier.github.io/machine-learning/reinforcement-learning/</id>
    <published>2021-06-24T05:00:00.000Z</published>
    <updated>2021-06-24T05:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>总是会看到强化学习及模仿学习的内容，每次看完都会忘记，此处把之前的小笔记总结一下。</p><a id="more"></a><h1 id="基本术语-Terminologies"><a href="#基本术语-Terminologies" class="headerlink" title="基本术语 Terminologies"></a>基本术语 Terminologies</h1><p>以下术语在强化学习和模仿学习中都经常见到。</p><ul><li>agent: the intelligent individual</li><li>environment: The agent is acting in an environment.  </li><li>state: Current condition. The agent can stay in one of many states of the environment</li><li>action: The agent chooses to take one of many actions under the certain states.</li><li>reward: Once an action is taken, the environment delivers a reward as feedback.</li><li>policy: Agents’ behavior.(s =&gt; a) The agent’s policy π provides the guideline on what is the optimal action to take in a certain state with the goal to maximize the total rewards.</li><li>value: (s =&gt; value) Each state is associated with a value function V(s) predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy. <ul><li>state-value of a state s is the expected return if we are in this state at time t.</li><li>action-value (“Q-value”; Q as “Quality” I believe?) of a state-action pair is expected return if we are in this state at time t and take action a.</li><li>A-value: The difference between action-value and state-value is the action advantage function (“A-value”):</li></ul></li><li>model: Transition and reward. (s,a =&gt; s’ &amp; r) How the environment reacts to certain actions (we may or may not know). </li></ul><h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><h2 id="1-马尔科夫决策过程"><a href="#1-马尔科夫决策过程" class="headerlink" title="1. 马尔科夫决策过程"></a>1. 马尔科夫决策过程</h2><p>In more formal terms, almost all the <strong>RL problems</strong> can be framed as <strong>Markov Decision Processes (MDPs)</strong>. All states in MDP has “Markov” property, referring to the fact that the future only depends on the current state, not the history.The goal is to react on each state to <strong>maximize the total reward</strong>.</p><div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/MDP.png" class="" title="MDP"></div><p>如果所有 MDP 成分都已知，我们便可以较容易的训练出来一个 agent。但是现实情况是很多时候，我们的 agent 对 transition function $P$ 和 reward function $R$ 一无所知，所有的信息都来自于同环境的交互。</p><h3 id="1-1-强化学习方法分类"><a href="#1-1-强化学习方法分类" class="headerlink" title="1.1. 强化学习方法分类"></a>1.1. 强化学习方法分类</h3><ol><li>以是否对环境建模分类:<ul><li>Doesn’t model the environment:<ul><li>Model-free RL: Doesn’t need to know the transition function (“model”), neither the real function nor a learned function.</li></ul></li><li>Model the environment:<ul><li>Model-based RL: Need to know the transition function (“model”), either the real function or a learned function.</li><li>Inverse reinforcement learning: Need to learn a value function for a state. (Imitation learning)</li></ul></li></ul></li><li>以行动策略和评估策略是否相同分类：<ul><li>On-policy: Use the deterministic outcomes or samples from the target policy to train the algorithm. 行动策略和评估策略相同</li><li>Off-policy: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy. 行动策略和评估策略不同</li></ul></li></ol><h3 id="1-2-对价值函数进行评估和分解"><a href="#1-2-对价值函数进行评估和分解" class="headerlink" title="1.2. 对价值函数进行评估和分解"></a>1.2. 对价值函数进行评估和分解</h3><p>强化学习的目标是可以使最终价值最大化，所以需要对其进行评估。</p><p>Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values.</p><script type="math/tex; mode=display">\begin{aligned}V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \big) \\Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi} (s', a')\end{aligned}</script><h2 id="2-Common-Approaches"><a href="#2-Common-Approaches" class="headerlink" title="2. Common Approaches"></a>2. Common Approaches</h2><div style="width:80%;margin:auto"><img src="/machine-learning/reinforcement-learning/taxonomy.png" class="" title="Taxonomy"></div><h3 id="2-1-Dynamic-Programming"><a href="#2-1-Dynamic-Programming" class="headerlink" title="2.1. Dynamic Programming"></a>2.1. Dynamic Programming</h3><p>When the model is fully known, following Bellman equations, we can use Dynamic Programming (DP) to iteratively evaluate value functions and improve policy. The policy would greedy based on the Q-value. Iteratively update the state value and the Q-value.</p><p>Generalized Policy Iteration (GPI) adaptive dynamic process</p><script type="math/tex; mode=display">\pi_0 \xrightarrow[]{\text{evaluation}} V_{\pi_0} \xrightarrow[]{\text{improve}}\pi_1 \xrightarrow[]{\text{evaluation}} V_{\pi_1} \xrightarrow[]{\text{improve}}\pi_2 \xrightarrow[]{\text{evaluation}} \dots \xrightarrow[]{\text{improve}}\pi_* \xrightarrow[]{\text{evaluation}} V_*</script><h3 id="2-2-Monte-Carlo-Methods"><a href="#2-2-Monte-Carlo-Methods" class="headerlink" title="2.2. Monte-Carlo Methods"></a>2.2. Monte-Carlo Methods</h3><p>Model-free method.It learns from episodes of raw experience without modeling the environmental dynamics and computes the observed mean return as an approximation of the expected return</p><h3 id="2-3-Temporal-Difference-Learning"><a href="#2-3-Temporal-Difference-Learning" class="headerlink" title="2.3. Temporal-Difference Learning"></a>2.3. Temporal-Difference Learning</h3><p>TD Learning is model-free and learns from episodes of experience. However, TD learning can learn from <strong>incomplete</strong> episodes and hence we don’t need to track the episode up to termination.</p><p>主要思想是将效用估计朝着理想均衡方向调整:</p><ul><li>TD调整一个状态与已观察到的后继状态相一致</li><li>ADP调整一个状态与可能出现的的后继状态相一致</li><li>TD可视为对ADP的一个粗略有效的一阶近似</li></ul><script type="math/tex; mode=display">\begin{aligned}V(S_t) &\leftarrow (1- \alpha) V(S_t) + \alpha G_t \\V(S_t) &\leftarrow V(S_t) + \alpha (G_t - V(S_t)) \\V(S_t) &\leftarrow V(S_t) + \alpha (R_{t+1} + \gamma V(S_{t+1}) - V(S_t)) \\Q(S_t, A_t) &\leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))\end{aligned}</script><p>To learn optimal policy:</p><ul><li>SARSA: On-Policy TD control<ul><li>“SARSA” refers to the procedure for updating the Q-value.</li><li>Same routine of GPI.</li><li>In each step of SARSA, we need to choose the next action according to the current policy.</li></ul></li><li>Q-Learning: Off-policy TD control<ul><li>The key difference from SARSA is that Q-learning does not follow the current policy to pick the second action (off-policy).</li><li>Q-learning may suffer from instability and divergence when combined with an nonlinear Q-value function approximation</li></ul></li><li>Deep Q-Network<ul><li>It quickly becomes computationally infeasible to memorize Q-table when the state and action space are large. </li><li>Use functions (i.e. a machine learning model) to approximate Q values and this is called function approximation.</li><li>greatly improve and stabilize the training procedure of Q-learning by two innovative mechanisms:<ul><li>Experience Replay: improves data efficiency, removes correlations in the observation sequences, and smooths over changes in the data distribution.</li><li>Periodically Updated Target: only periodically updated, overcomes the short-term oscillations</li></ul></li></ul></li></ul><h3 id="2-4-Combining-TD-and-MC-Learning"><a href="#2-4-Combining-TD-and-MC-Learning" class="headerlink" title="2.4. Combining TD and MC Learning"></a>2.4. Combining TD and MC Learning</h3><p>In TD learning, we only trace one step further down the action chain when calculating the TD target. One can easily extend it to take multiple steps to estimate the return.</p><div style="width:90%;margin:auto"><img src="/machine-learning/reinforcement-learning/TD_MC_DP_backups.png" class=""></div><h3 id="2-5-Policy-Gradient"><a href="#2-5-Policy-Gradient" class="headerlink" title="2.5. Policy Gradient"></a>2.5. Policy Gradient</h3><p>All the methods we have introduced above aim to learn the state/action value function and then to select actions accordingly. Policy Gradient methods instead learn the policy directly with a parameterized function.</p><ul><li>Measure the quality of a policy with the policy score function.</li><li>Use policy gradient ascent to find the best parameter that improves the policy.</li></ul><h3 id="2-6-Asynchronous-Advantage-Actor-Critic-A3C"><a href="#2-6-Asynchronous-Advantage-Actor-Critic-A3C" class="headerlink" title="2.6. Asynchronous Advantage Actor-Critic (A3C)"></a>2.6. Asynchronous Advantage Actor-Critic (A3C)</h3><ul><li>Asynchronous: Several agents are trained in it’s own copy of the environment and the model form these agent’s are gathered in a master agent. The reason behind this idea, is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.</li><li>Advantage: Similarly to PG where the update rule used the dicounted returns from a set of experiences in order to tell the agent which actions were “good” or “bad”.</li><li>Actor-critic: combines the benefits of both approaches from policy-iteration method as PG and value-iteration method as Q-learning (See below). The network will estimate both a value function V(s) (how good a certain state is to be in) and a policy π(s). Agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.</li></ul><div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/a3c.png" class=""></div><h2 id="3-Known-Problems"><a href="#3-Known-Problems" class="headerlink" title="3. Known Problems"></a>3. Known Problems</h2><ul><li>Exploration-Exploitation Dilemma</li><li>Deadly Triad Issue: off-policy, nonlinear function approximation, and bootstrapping are combined in one RL algorithm, the training could be unstable and hard to converge.</li></ul><h1 id="模仿学习"><a href="#模仿学习" class="headerlink" title="模仿学习"></a>模仿学习</h1><h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p>Background:</p><ul><li>Given: demonstrations or demonstrator </li><li>Goal: train a policy to mimic demonstrations</li></ul><p>Intuition: </p><ul><li>人们并不总是知道执行某项任务所获得的报酬</li><li>但是人们可能会知道“做什么是正确的事情（最佳策略）</li></ul><p>Rollout: sequentially execute $\pi(s_0)$ on an initial state</p><ul><li>Produce trajectory $\mathcal{T}=(s_0,a_0,s_1,a_1,…)$</li></ul><h2 id="2-模仿学习分类"><a href="#2-模仿学习分类" class="headerlink" title="2. 模仿学习分类"></a>2. 模仿学习分类</h2><ul><li>Behavior cloning </li><li>Direct policy learning (multiple step BC)</li><li>Inverse reinforcement learning (assume learning R is statistically easier)</li></ul><div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/IL_types_1.png" class="" title="Imitation learning types 1"></div><div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/IL_types_2.png" class="" title="Imitation learning types 2"></div><h3 id="2-1-Behavioral-Cloning-simplest-Imitation-Learning-setting"><a href="#2-1-Behavioral-Cloning-simplest-Imitation-Learning-setting" class="headerlink" title="2.1. Behavioral Cloning (simplest Imitation Learning setting)"></a>2.1. Behavioral Cloning (simplest Imitation Learning setting)</h3><p>Treat experts’ states-actions pairs i.i.d and as training example use supervised learning (from state to action).Distribution provided exogenously.</p><p>When to use BC?</p><div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/BC.png" class="" title="BC"></div><h3 id="2-2-Direct-Policy-Learning"><a href="#2-2-Direct-Policy-Learning" class="headerlink" title="2.2. Direct Policy Learning"></a>2.2. Direct Policy Learning</h3><p>Learning reduction: Reduce “harder” learning problem to “easier” one</p><p>Idea:</p><ul><li>Construct a sequence of distributions or sequence of supervised learning problems.</li><li>Query interactive oracle about the state and construct a loss function according to our action and expert action on this state.</li></ul><div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/DPL_example.png" class="" title="DPL_example"></div><div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/IL_sequential.png" class="" title="Sequential process"></div><h3 id="2-3-Inverse-reinforcement-learning"><a href="#2-3-Inverse-reinforcement-learning" class="headerlink" title="2.3. Inverse reinforcement learning"></a>2.3. Inverse reinforcement learning</h3><p>Inverse RL指我们需要对环境的reward进行建模。RL与IRL的对比如下图所示：</p><div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/RL_IRL.png" class="" title="RL-IRL"></div><p>In a traditional RL setting, the goal is to learn a decision process to produce behavior that maximizes some predefined reward function. Inverse reinforcement learning (IRL), flips the problem and instead attempts to extract the reward function from the observed behavior of an agent.IRL seeks the reward functions that ‘explains’ the demonstrations.</p><p>此时同样存在是否依赖 transition function 的情况</p><div style="width:70%;margin:auto"><img src="/machine-learning/reinforcement-learning/InverseRL.png" class="" title="InverseRL"></div><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning">https://en.wikipedia.org/wiki/Reinforcement_learning</a></li><li><a href="https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc">https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc</a></li><li><a href="https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html">https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html</a></li><li>人工智能：一种现代的方法</li><li><a href="https://sites.google.com/view/icml2018-imitation-learning/">https://sites.google.com/view/icml2018-imitation-learning/</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;总是会看到强化学习及模仿学习的内容，每次看完都会忘记，此处把之前的小笔记总结一下。&lt;/p&gt;</summary>
    
    
    
    <category term="Machine Learning" scheme="https://superuier.github.io/categories/Machine-Learning/"/>
    
    
    <category term="machine-learning" scheme="https://superuier.github.io/tags/machine-learning/"/>
    
    <category term="reinforcement-learning" scheme="https://superuier.github.io/tags/reinforcement-learning/"/>
    
    <category term="imitation-learning" scheme="https://superuier.github.io/tags/imitation-learning/"/>
    
  </entry>
  
  <entry>
    <title>翟东升政经启翟系列视频</title>
    <link href="https://superuier.github.io/economy-finance/didongsheng/zjqd/"/>
    <id>https://superuier.github.io/economy-finance/didongsheng/zjqd/</id>
    <published>2021-06-15T03:00:00.000Z</published>
    <updated>2021-07-12T16:24:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>翟老师与观视频合作的系列视频，内容可能较为广，此处总结要点便于回顾。整个系列视频的时间为 2020-06 至 2021-02。整个系列视频约有40节，内容涵盖政治经济教育文化等方面。是翟老师其自身研究观点的整理与科普输出，值得思考。</p><a id="more"></a><h1 id="2020-06-03-雄安，你就是中国的学园都市了！"><a href="#2020-06-03-雄安，你就是中国的学园都市了！" class="headerlink" title="2020-06-03 雄安，你就是中国的学园都市了！"></a>2020-06-03 雄安，你就是中国的学园都市了！</h1><p>如何在新型举国体制下处理政府和市场关系</p><ul><li>市场是政府的创造物，是一种基础性公共产品，目的是覆盖消费者。</li><li>政府要帮助新的商业模式探清道路。</li><li>科学由政府来做，技术由市场和企业来做</li><li>政府需要向国民提供安全。</li><li>政府可以为企业分担小部分的技术研发风险。</li></ul><p>科技创新：</p><ul><li>技术路线不能单主体决策</li><li>适当的远离市场，“养一批闲人”在雄安，创造一个良好的科研氛围。</li></ul><h1 id="2020-06-10-美国以后不让我们摸了，中国该怎么自己过河？"><a href="#2020-06-10-美国以后不让我们摸了，中国该怎么自己过河？" class="headerlink" title="2020-06-10 美国以后不让我们摸了，中国该怎么自己过河？"></a>2020-06-10 美国以后不让我们摸了，中国该怎么自己过河？</h1><p>创新人才基础存在（智商较高）。</p><p>中国文化抑制创新的因素：</p><ul><li>重物而轻人，不愿意为人才买单只愿意为物品买单。</li><li>对说谎造假的惩罚和仇视不够，机会不能给到有真才实学的人。</li><li>好面子怕出丑怕红脸，尊老。</li><li>不喜辩论不喜批判。</li><li>教育考试体系求全，要求平衡发展。</li><li>人才之间跨国交流，目前被抑制。</li></ul><h1 id="2020-06-17-曾经的华尔街之狼，为什么在特朗普时代成了哈士奇？"><a href="#2020-06-17-曾经的华尔街之狼，为什么在特朗普时代成了哈士奇？" class="headerlink" title="2020-06-17 曾经的华尔街之狼，为什么在特朗普时代成了哈士奇？"></a>2020-06-17 曾经的华尔街之狼，为什么在特朗普时代成了哈士奇？</h1><p>美国金融业对美国内政外交的影响力产生持续大规模的衰落。</p><p>1960年代，华尔街背后家族有多个族群，基本都是犹太人。1970年代前，美国国家利益就是有利于通用公司的利益。1980年代，什么有利于华尔街什么就有利于美国。</p><p>上位过程：</p><ul><li>华尔街通过捐赠逐步上位。同时搞人员渗透（1960开始持续上升，最高达到60%）。</li><li>华尔街搞意识形态渗透，逐步上位。例如格林斯潘。</li><li>华尔街大而不能倒，绑架内政外交。</li></ul><p>下位过程：</p><ul><li>08年金融危机之后政治声望一落千丈</li><li>和其他医药协会、步枪协会比不占上风</li></ul><p>世界资本主义利益分配结构：</p><ul><li>美国产业通过股权债券外包给欧日韩</li><li>欧日韩FDI（外商直接投资）产业迁移到中国实现中国工业化（要付给外商15%）</li><li>中国买美国国债，和华尔街有紧密联系（3%收益率）<ul><li>3%-15%是快速工业化的代价</li><li>中方和华尔街关系紧密（互利），2016年之后华尔街影响力急剧衰弱。</li></ul></li></ul><p>负利率时代，受伤最严重的就是金融系统，所以华尔街影响力可能还会持续下降。</p><h1 id="2020-06-24-中国年轻人压力这么大，怎么才能让他们有钱消费？"><a href="#2020-06-24-中国年轻人压力这么大，怎么才能让他们有钱消费？" class="headerlink" title="2020-06-24 中国年轻人压力这么大，怎么才能让他们有钱消费？"></a>2020-06-24 中国年轻人压力这么大，怎么才能让他们有钱消费？</h1><p>《美国真相》：政府市场关系的分析。</p><p>市场私人部门繁荣是因为政府能提供有效公共产品：</p><ul><li>和平安全</li><li>基础设施</li><li>国际贸易条约</li><li>基础科研基础教育</li><li>知识产权保护</li></ul><p>在保证公共产品的情况下，综合税率越低越好。</p><p>为何瑞典和北欧区域如此成功？不是因为税收高，企业就不干活，而是政府有钱可以转化为有效公共产品，导致私人部门的繁荣。</p><p>国内主要矛盾：全球总需求本国总需求增速偏低，但是供给能力偏强，供给过剩。</p><p>所以：</p><ul><li>内循环，让中国年轻人有钱去消费。</li><li>中国综合税率和美国差不太多，并不适合给企业减税，降税是增加了后代债务，给全球消费者输出了通货紧缩，加强了中国与外部的竞争力度。</li><li>鼓励年轻人生孩子。</li></ul><h1 id="2020-07-01-数据时代中国真正的对手还是只有一个：美国"><a href="#2020-07-01-数据时代中国真正的对手还是只有一个：美国" class="headerlink" title="2020-07-01 数据时代中国真正的对手还是只有一个：美国"></a>2020-07-01 数据时代中国真正的对手还是只有一个：美国</h1><p>新技术带来的挑战：科技进步对中国对世界的冲击和影响。科技进步利于再分配而不是创造新财富。 以更低成本以满足原有需求。</p><p>现象：</p><ul><li>垄断性平台剥削生产商</li><li>商家相对消费者能力更大了</li></ul><p>科技进步后果：</p><ul><li>贫富分化进一步增大</li><li>大企业和政府谈判地位更强</li></ul><p>负利率时代，人工智能发展时代，金融业能容纳的就业人数未来将会萎缩。教师等细分领域的人员也会需求降低。英语汉语西语对其他语言的文明的冲击可能十分明显。</p><p>如何建设新时代中国特色社会主义，不能刻舟求剑。重新思考教育制度，终身教育。退休制度，福利制度。养老育儿制度。</p><h1 id="2020-07-08-黑人运动在美国翻不了天，真正会使美国蛮族化的力量是…"><a href="#2020-07-08-黑人运动在美国翻不了天，真正会使美国蛮族化的力量是…" class="headerlink" title="2020-07-08 黑人运动在美国翻不了天，真正会使美国蛮族化的力量是…"></a>2020-07-08 黑人运动在美国翻不了天，真正会使美国蛮族化的力量是…</h1><p>美国文化革命的根源：</p><ul><li>种族的结构性变化</li><li>贫富分化</li></ul><p>美版文革特点：种族党派贫富分化叠加一起。</p><ul><li>共和党几乎全部白人。民主党五花八门。</li><li>美国文化革命发展缓慢，呈现周期性，需要社会动能去刺激。持续时间长。</li></ul><p>运动的前景：</p><ul><li>虽然以黑人为导火索，拉丁人真正动员起来才是真正的高潮。</li><li>平权的悖论：肤色，性别，年龄段。是要求机会的平等还是结果的平等？</li><li>如何评判革命是否成功？（是否通过斗争达到团结？）美国正在出现古罗马的蛮族化。</li></ul><p>对中国的启示：</p><ul><li>要对中美竞争有信心。</li><li>信心源于，我专而敌分。</li><li>美国种族主义的原罪。</li><li>加强对拉丁裔文化的研究。什么样的新美国化？对美政策有的放矢。</li></ul><h1 id="2020-07-15-翟老师：下代霸主应该是个十亿人口规模的国家…印度：正是在下"><a href="#2020-07-15-翟老师：下代霸主应该是个十亿人口规模的国家…印度：正是在下" class="headerlink" title="2020-07-15 翟老师：下代霸主应该是个十亿人口规模的国家…印度：正是在下"></a>2020-07-15 翟老师：下代霸主应该是个十亿人口规模的国家…印度：正是在下</h1><p>全球化的周期（贸易全球化指标：出口/GDP）：</p><ul><li>1870-1914 上升</li><li>1914-1945 下降：英帝国的衰落</li><li>1945-1979 平行竞争，半球化时代</li><li>1979-2008 全球化时代，中国融入世界市场，英美提出新自由主义全球化。</li><li>2008-至今 全球化指标下行，逆全球化。</li></ul><p>为什么全球化会有一个波动？全球化是历史的必然趋势吗？</p><ul><li>广义全球化：大航海突破地理隔阂。</li><li>狭义全球化：</li><li>霸权周期就是全球化周期。</li></ul><p>几波全球化：</p><ul><li>荷兰大航海，使商品跨境流通。人口大规模增长。百万人口。</li><li>英国工业文明诞生，FDI跨界流通。人均GDP跨境流通。千万人口。</li><li>美国，信息革命，货币跨界流动。亿级人口。</li><li>十亿人口规模？中国？印度？欧盟？</li></ul><h1 id="2020-07-22-统治世界400年的新教，就要被我们“筷子教“打败了？"><a href="#2020-07-22-统治世界400年的新教，就要被我们“筷子教“打败了？" class="headerlink" title="2020-07-22 统治世界400年的新教，就要被我们“筷子教“打败了？"></a>2020-07-22 统治世界400年的新教，就要被我们“筷子教“打败了？</h1><p>看起来是霸权更替，但是荷兰英国美国都是新教文明。</p><p>12年前，指标上出现重大变化。2019年，东亚文化制造业总产出超过新教文明圈。东亚文明圈的共同点，都在用筷子。</p><p>过去两百年形成的大众政治体系在逐渐过时。小众政治时代，对全人类的政治稳定提出巨大挑战。</p><h1 id="2020-07-29-欧盟又开始追求大一统了？中国先笑出了声……"><a href="#2020-07-29-欧盟又开始追求大一统了？中国先笑出了声……" class="headerlink" title="2020-07-29 欧盟又开始追求大一统了？中国先笑出了声……"></a>2020-07-29 欧盟又开始追求大一统了？中国先笑出了声……</h1><p>全球市场对欧洲一体化前景乐观。</p><p>需要中央政府对利益进行再分配。因为内部不同省份竞争力变化。在欧盟下看，汇率对德国低谷，对南欧高估，所以强者愈强，弱者恒弱。但是以前欧盟的转移支付能力有限。新冠疫情下的转移支付法案，让他家看到了一体化的希望。</p><p>欧盟对中国的定位（疫情之前）：</p><ul><li>系统性对手：政治和社会制度方面</li><li>谈判伙伴：国际问题</li><li>经济竞争</li></ul><p>未来三角游戏：</p><ul><li>军事：中美俄</li><li>经济：中美欧</li></ul><p>欧盟体系：成员国太多，成事不足败事有余。</p><h1 id="2020-08-05-眼看和平演变马上要成功-公知和特朗普：快！“救”中国"><a href="#2020-08-05-眼看和平演变马上要成功-公知和特朗普：快！“救”中国" class="headerlink" title="2020-08-05 眼看和平演变马上要成功..公知和特朗普：快！“救”中国"></a>2020-08-05 眼看和平演变马上要成功..公知和特朗普：快！“救”中国</h1><p>黑格尔：重要的历史往往上演两次，第一次是悲剧，第二次是闹剧。</p><p>美国对华派系</p><ol><li>老一代中国问题专家，知华派愿意接触中国，白人为主。</li><li>新一代中国问题专家，很多由中国去美国，对中国文化抱有仇恨态度，认为接触派是错的。坚持有原则的现实主义，施压中国。</li></ol><p>进入美国体系，经济上的获益，是以独立性丧失为代价。虽然特朗普政府短期带来的损失比较多，但是戳破了过去四十年美国对中国实施的和平演变政策，而且客观的太高了中国的国际地位。</p><p>两个时代的中美关系：</p><ul><li>奥巴马时代：不平等，不对抗</li><li>特朗普时代：平等，对抗</li></ul><h1 id="2020-08-12-美国：我得癌症了，急需医生！美国医保：可肿瘤就是我"><a href="#2020-08-12-美国：我得癌症了，急需医生！美国医保：可肿瘤就是我" class="headerlink" title="2020-08-12 美国：我得癌症了，急需医生！美国医保：可肿瘤就是我"></a>2020-08-12 美国：我得癌症了，急需医生！美国医保：可肿瘤就是我</h1><p>美帝国为什么相对衰弱了？</p><ul><li>过度扩张穷兵黩武？这个观点站不住脚。国防占比占财务支出规模减少。</li><li>美帝国体系最大的问题是内耗。医疗医保支出持续扩大，医保体系存在巨大的系统性问题。</li></ul><p>美国医疗产业越来越垄断，形成强大利益集团。药品福利管理局也在合并垄断，推荐高价格药品。保险业不受联邦反垄断法管理。且这一系列链条向政府提供了很多的政治献金。猫鼠沆瀣一气。</p><p>如何游说的？华盛顿K街，很多游说公司。和国会议员助理沟通，再简短的和议员见面写支票，进入他下一次的竞选资金池。</p><h1 id="2020-08-19-从小父母告诉我读书可以改变命运！美国学生：我酸了…"><a href="#2020-08-19-从小父母告诉我读书可以改变命运！美国学生：我酸了…" class="headerlink" title="2020-08-19 从小父母告诉我读书可以改变命运！美国学生：我酸了…"></a>2020-08-19 从小父母告诉我读书可以改变命运！美国学生：我酸了…</h1><p>两种教育思潮的撕扯：</p><ul><li>自由主义教育思想：爱与自由，自然生长。重视文科艺术。个人价值本位。</li><li>非自由主义教育思想：调用本民族教育资源。从严要求约束。重视理工科。集体价值本位。</li></ul><h2 id="美国"><a href="#美国" class="headerlink" title="美国"></a>美国</h2><p>美国自由主义教育好不好？</p><ul><li>公立教育质量非常差。<ul><li>很早放学。</li><li>体育娱乐活动多。</li><li>理工科难度比较低。</li><li>中小学老师待遇低，素质差。</li></ul></li><li>读书改变命运的观念不存在。读书无用论。</li><li>美国高等教育存在巨大泡沫。</li><li>人文领域人才来自私立教育，理工人才来自欧亚大陆。</li></ul><h2 id="日本"><a href="#日本" class="headerlink" title="日本"></a>日本</h2><p>看到日本年轻人综合素质比较低下。举例：</p><ul><li>中文专业无法用中文英文交流。</li><li>旅游专业无法安排行程。日语也无法讲解。</li></ul><p>1980教育改革，让孩子自由生长。造成平成废宅。</p><h2 id="百年前英德"><a href="#百年前英德" class="headerlink" title="百年前英德"></a>百年前英德</h2><p>自由主义和非自由主义教育理念之争，百年前英德已经完美演绎了</p><p>1870-1914：</p><ul><li>英国<ul><li>小学入学率偏低（50%），中学更低（25%）</li><li>着重于发现天赋</li><li>嘲讽对技能的培训</li></ul></li><li>德国作为一个落后国家<ul><li>重视教育，认为教育是义务</li><li>强调纪律和礼貌</li><li>重视理工科</li></ul></li></ul><h2 id="中国"><a href="#中国" class="headerlink" title="中国"></a>中国</h2><p>不能学习英美的自由主义教育。坚持为中华民族崛起而读书。应该继续重视理工科。在高考下，寒门仍能出贵子。针对目前教育的问题，可以通过更实际的方式解决。</p><h1 id="2020-08-26-压低老百姓的福利来发展本国经济，这不是必须滴！"><a href="#2020-08-26-压低老百姓的福利来发展本国经济，这不是必须滴！" class="headerlink" title="2020-08-26 压低老百姓的福利来发展本国经济，这不是必须滴！"></a>2020-08-26 压低老百姓的福利来发展本国经济，这不是必须滴！</h1><p>以德意志第二帝国的成功经验为例。</p><ol><li>发达的公共产品：<ul><li>市场经济</li><li>福利制度</li><li>全序列高质量教育</li><li>基础设施建设</li><li>立法保护创新</li></ul></li><li>德意志帝国产业部门兼并联合。真实世界中垄断不一定带来低效，可能会以全球消费者的福利损失来通过工资和福利分享给本国国民。</li><li>德国金融为实体产业服务，鼓励金融和实体产业股权人事上高度交叉。</li></ol><p>对中国启示：</p><ul><li>加大教育研发投入，保护知识产权</li><li>搞好社会再分配，实现社会团结</li><li>不必压低本国福利来谋求经济发展</li></ul><h1 id="2020-09-02-这个盛世帝国的灭亡，给中国复兴之路留下的5个经验教训"><a href="#2020-09-02-这个盛世帝国的灭亡，给中国复兴之路留下的5个经验教训" class="headerlink" title="2020-09-02 这个盛世帝国的灭亡，给中国复兴之路留下的5个经验教训"></a>2020-09-02 这个盛世帝国的灭亡，给中国复兴之路留下的5个经验教训</h1><p>以德意志第二帝国的灭亡为例。二十世纪早期的德意志帝国和当今中国有相似之处：</p><ul><li>后发的工业国，通过新一轮工业革命的基于实现对原有大国的赶超</li><li>与原有大国是盟友关系，但趋于反目</li><li>陆海复合型国家，且安于陆权</li><li>想过与原有大国重温旧梦（英德，中美G2）</li><li>新型大国韬光养晦</li><li>反对老大国的自由主义经济学，强调国情独特，强调个体服从整体，爱国主义价值观</li><li>国内搭建铁路，海外延伸，要实现大陆体系互联互通</li><li>模仿发达国家再技术反超</li><li>面临外国邻国的仇怨</li><li>工业化之后通过财富再分配缓解国内分化</li><li>认为老霸主是全球帝国，不会集中所有力量，对方会上门来打，自己有主场优势。把问题想象的相对简单。</li></ul><p>会重蹈覆辙吗？有若干点关键不同。</p><ul><li>政治制度不同。世袭封建和资本主义的嫁接vs民主集中制度。</li><li>名族个性不同。中国人实用唯物主义。德意志浪漫主义，内在的自杀倾向，悲剧之美。</li><li>人口规模不同。中国相当于美欧日只和。人口质量长期来看不同。</li><li>外交制度不同。中国不搞扩张主义。</li><li>中国不穷兵黩武。</li></ul><p>对我们的教训：</p><ul><li>不要过度刺激民族主义。给本国民众提供平等发展空间。不能热衷操纵民意。</li><li>联盟战略。</li><li>少搞存量博弈，多搞增量博弈。</li><li>军事力量建设，不能面面俱到要有专长。在一两个维度有绝对优势。</li><li>充分战略规划和估计，不能太过乐观。</li></ul><h1 id="2020-09-09-讨论“中国GDP何时超美”没意义，这不是时间能决定的问题"><a href="#2020-09-09-讨论“中国GDP何时超美”没意义，这不是时间能决定的问题" class="headerlink" title="2020-09-09 讨论“中国GDP何时超美”没意义，这不是时间能决定的问题"></a>2020-09-09 讨论“中国GDP何时超美”没意义，这不是时间能决定的问题</h1><p>用线性外推推测中国GDP合适超过美国是存在很严重的错误的。大国GDP相对力量变化背后结构性因素逻辑是什么。</p><p>中国经济超过美国不是客观趋势而是<strong>一种选择</strong>。背后的逻辑结构：</p><ul><li>过去100年绝大多数国家GDP难以长期超过美国。</li><li>中心国家比外围国家富有且稳定。</li><li>大国人均GDP占美国比例会不断上移趋进却无法达到美国经济。</li><li>美国的GDP和其他国家的GDP不一样。美国提供流动性，其经济是虚的。</li><li>美国经济的“虚”决定其他经济的“实”。</li></ul><p>美国GDP占全球1/4：</p><ul><li>非美经济规模应该等于美经济规模（生产=消费）</li><li>美元占全球货币市场份额50%多。</li></ul><p>所以：</p><ul><li>中国如果以美元储备为基础，永远无法超过美国GDP。</li><li>出口换美元存在局限</li></ul><h1 id="2020-09-16-美帝国自以为一切尽在掌握，不料失控而让这里再度崛起"><a href="#2020-09-16-美帝国自以为一切尽在掌握，不料失控而让这里再度崛起" class="headerlink" title="2020-09-16 美帝国自以为一切尽在掌握，不料失控而让这里再度崛起"></a>2020-09-16 美帝国自以为一切尽在掌握，不料失控而让这里再度崛起</h1><p>东亚供应链的政治经济学含义。目前三大供应链：东亚，美加墨，欧洲。</p><p>日本是龙头，其成功之后低端产业转移到四小龙，之后再转移到中国，现在正在转移到越南。为什么东亚能成功？龙腾文化。</p><ul><li>东亚具有强政府，对于国民比较强势。工业化往往是由强政府推动。强政府一般是由战争而来。</li><li>东亚民众智商高。全球人均智商最高的区域。</li><li>东亚文化总体不信神。（升官发财生儿子。交易心态。）高储蓄率。</li></ul><p>东亚模式缺陷：依附性的出口导向的发展，以本国民众的血汗换取别的国家的主权信用。</p><p>美国用投资收益顺差来支付贸易逆差。经济表现不错，通胀低位。但是以中国加入东亚供应链而转变。于是美国提出TPP想把中国踢出，但是特朗普推出。所以RCEP获得发展。</p><h1 id="2020-09-23-中国工业化搞全产业链通吃？这是违背经济规律的幼稚做法！"><a href="#2020-09-23-中国工业化搞全产业链通吃？这是违背经济规律的幼稚做法！" class="headerlink" title="2020-09-23 中国工业化搞全产业链通吃？这是违背经济规律的幼稚做法！"></a>2020-09-23 中国工业化搞全产业链通吃？这是违背经济规律的幼稚做法！</h1><p>中国实现最大规模最快的工业化，主要原因1992年之后：</p><ul><li>区别于日韩，中国欢迎别的国家来投资。（超国民待遇）</li></ul><p>这一系列工业化浪潮，打乱了日本的雁行计划。1994年人命币贬值，与美元非正式挂钩。开放欢迎<strong>制造业</strong>外资。</p><p>中国角色的变化，从低端到终端。本土品牌，本土供应链崛起。目前全产业链都有。</p><h2 id="中国要保持全产业链吗？"><a href="#中国要保持全产业链吗？" class="headerlink" title="中国要保持全产业链吗？"></a>中国要保持全产业链吗？</h2><p>不能。汇率定价，要素价格配置不能同时适应低附加值和高附加值的商品。</p><p>通过产业升级改造，可以产生贸易顺差，人民币汇率会进一步升高。此时劳动密集型产品没有生存空间。中国劳动力总人数萎缩，劳动力成本越来越高（这是我们发展的目的。）。</p><p>我们不是要保持全产业链，而是要保留高附加值的产业。不能有卡脖子的事情。低端的污染的产业要让别人承担一部分。低端产业也不能转移到一个国家，非东亚国家。</p><h2 id="新冠会导致去中国化加速吗？"><a href="#新冠会导致去中国化加速吗？" class="headerlink" title="新冠会导致去中国化加速吗？"></a>新冠会导致去中国化加速吗？</h2><p>会强化中国制造业的地位。虽然东亚制造业先断裂再快速恢复，介入到了很多原先排斥我们的地方。中国在东亚取得了中心地位。</p><h2 id="政治影响"><a href="#政治影响" class="headerlink" title="政治影响"></a>政治影响</h2><p>之前日本经地区首要地位时，政治上十分软弱。而中国的地区领导力令人刮目相看。</p><h1 id="2020-09-30-日本究竟毁在哪里？日本政策界：我们也反思了35年…"><a href="#2020-09-30-日本究竟毁在哪里？日本政策界：我们也反思了35年…" class="headerlink" title="2020-09-30 日本究竟毁在哪里？日本政策界：我们也反思了35年…"></a>2020-09-30 日本究竟毁在哪里？日本政策界：我们也反思了35年…</h1><p>中国汇率提升会不会重蹈日本的覆辙？</p><h2 id="日本有什么问题"><a href="#日本有什么问题" class="headerlink" title="日本有什么问题"></a>日本有什么问题</h2><p>和德国相比，日元升值很晚。大企业财阀推动少升值晚升值。导致产业升级得慢。德国不靠汇率低估来获取竞争力。所以德国央行不需要大规模放水导致泡沫。同时日本生育率持续低迷。</p><p>广场协议日元升值。广场协议之前日本大幅低息放债。这种快速升值，伤害了其制造业。日本通过资本项目放开和离岸中心来拉动经济。导致资产泡沫化。</p><p>问题所在：开始拒绝升值，后来短期大幅升值。</p><h2 id="对于中国"><a href="#对于中国" class="headerlink" title="对于中国"></a>对于中国</h2><p>2005年开始升值，每年6%左右。政府给予纺织等行业补贴。但是去除补贴他们的利益也创新高。</p><h1 id="2020-10-07-中国低端制造业能不能转移到内陆？取决于这两个关键因素"><a href="#2020-10-07-中国低端制造业能不能转移到内陆？取决于这两个关键因素" class="headerlink" title="2020-10-07 中国低端制造业能不能转移到内陆？取决于这两个关键因素"></a>2020-10-07 中国低端制造业能不能转移到内陆？取决于这两个关键因素</h1><p>观点：不可行</p><h2 id="云南行"><a href="#云南行" class="headerlink" title="云南行"></a>云南行</h2><p>想要将江浙沪玩具产业转移到云南的园区。但是价差很微小。长期来看这些制造业难以生存。人民币汇率长期看涨。且老龄化形势严峻。</p><p>云南的要素价格难以与东南亚等国PK。劳动力产业东南沿海的原因，运输成本低。云南运输成本很高。</p><h2 id="重庆黄奇帆"><a href="#重庆黄奇帆" class="headerlink" title="重庆黄奇帆"></a>重庆黄奇帆</h2><p>引入其他地方的产业，补贴成本。仍然没有提高重庆占国内的制造业占比。</p><h2 id="我们发展的目的"><a href="#我们发展的目的" class="headerlink" title="我们发展的目的"></a>我们发展的目的</h2><p>让人民过上好日子，而不是老板过上好日子。不是为了拥有这些制造业。一些无关国家安全的产业可以放弃。</p><h1 id="2020-10-14-听说欧洲人有好为人师的习惯，那就给他个机会，顺便打击美国反华势力"><a href="#2020-10-14-听说欧洲人有好为人师的习惯，那就给他个机会，顺便打击美国反华势力" class="headerlink" title="2020-10-14 听说欧洲人有好为人师的习惯，那就给他个机会，顺便打击美国反华势力"></a>2020-10-14 听说欧洲人有好为人师的习惯，那就给他个机会，顺便打击美国反华势力</h1><p>新时代中国应该向谁学习？欧洲。</p><p>过去的中国，先学习苏联，再进行中国特色社会主义探索，再学习美国及其盟友美日韩。</p><p>为什么一定要向他人学习呢？</p><ul><li>向自己学习不吉利（lol），中国之前向谁学习谁就变差了。</li><li>师心自用和我们传统文化相悖，我们虚心好学。</li><li>和世界态势有关。国外势力新冷战，中国也没有意愿推广中国模式。以美国为首对中国模式敌视。如果抬高欧洲有利于制造统一战线。</li></ul><p>向欧洲学习：</p><ul><li>我们依旧谦虚</li><li>不再向美国学习</li><li>寻求政治共识，孤立分化共和党反华</li></ul><p>学欧洲什么？欧洲不是一个统一的模式：</p><ul><li>莱茵模式，以德国为代表</li><li>斯堪的纳维亚半岛北欧模式</li><li>地中海模式</li><li>盎格鲁撒克逊模式（比较市场化与美国类似），出口导向</li></ul><p>我们想要学的是前两种模式以及作为一个整体的模式</p><ul><li>德国的资本主义：重视国有企业在命脉产业，大陆法系，发挥国家市场两种力量</li><li>利益相关者资本主义模式（不是stockholder）</li></ul><p>欧洲各国在专利发展，人口阶级纵向流动，绿色发展等方面都十分领先。欧洲最大的公共产品是和平，其实现了一个人类命运共同体。</p><p>学习欧洲需要避免的教训：</p><ul><li>移民政策</li><li>福利政策，给老年人福利偏高</li><li>个人价值本位的人权政策，抽象人权。人权不能神圣化，我们要搞集体价值本位的人权。</li></ul><h1 id="2020-10-21-打破人口外流与经济衰退之间的恶性循环，振兴东北奥里给！"><a href="#2020-10-21-打破人口外流与经济衰退之间的恶性循环，振兴东北奥里给！" class="headerlink" title="2020-10-21 打破人口外流与经济衰退之间的恶性循环，振兴东北奥里给！"></a>2020-10-21 打破人口外流与经济衰退之间的恶性循环，振兴东北奥里给！</h1><h2 id="东北有什么问题？"><a href="#东北有什么问题？" class="headerlink" title="东北有什么问题？"></a>东北有什么问题？</h2><p>东北为什么不行？</p><ul><li>并不是因为讲人情搞腐败，90年代的广东也有这些问题。不能用普遍因素解释区域性的特殊现象。</li><li>和地理区域有关，冬天工作时间少。但是这个观点也不合理，北欧和美国东海岸都冷。</li><li>全球化和老龄化带来的人口流动产业变迁才是根本问题。</li></ul><p>全球化：</p><ul><li>解放前是工业化最早的地区，解放后学苏联也是很领先全中国的。</li><li>1992年之后，中国成为美欧体系外围，东南沿海区位优势，人口优势处于这个体系中。东南沿海的枝叶嫁接在别人的根上，而放弃了自己的根东北。</li><li>东北军工科技和其他地区脱钩。</li></ul><p>老龄化：</p><ul><li>老龄化社会产出不会产少，可以用机器</li><li>但是老龄化社会消费萎缩，人的生命是有欲望周期，48岁是欲望高峰。</li><li>下岗潮和老龄化浪潮导致东北和其他地区拉开差距。</li></ul><p>这两个因素导致人口自由流动，年轻人出去回不来，加剧了东北老龄化趋势。本地的就业进一步下降。人不是物以稀为贵。人越密集不是竞争更激烈更难，而是每个年轻人发展机会越大。认识群居动物，人聚在一起分工规模扩大，交易机会增加，合作成本降低。</p><p>为什么东北成为中国老龄化最快的区域？因为东北是现代化城市化工业化最早的区域。现代化核心指标，女性的受教育提高。女性个体意识觉醒，导致离婚率上升，生育率下降。这个对女性是好事，但是长时间轴看，老龄化趋势加重。</p><h2 id="如何振兴东北？"><a href="#如何振兴东北？" class="headerlink" title="如何振兴东北？"></a>如何振兴东北？</h2><p>寻找一个契机，是东北区域形成新的良性循环。e.g. 图们江出海口打通，每年多几个月北冰洋到西欧的海上通道。再做一个新城大城，提供特殊政策，吸引东北年轻人，甚至中国世界的年轻人。</p><h1 id="2020-10-28-地方政府亟需从抢资本转向抢人：得年轻人者，得天下！"><a href="#2020-10-28-地方政府亟需从抢资本转向抢人：得年轻人者，得天下！" class="headerlink" title="2020-10-28 地方政府亟需从抢资本转向抢人：得年轻人者，得天下！"></a>2020-10-28 地方政府亟需从抢资本转向抢人：得年轻人者，得天下！</h1><p>中国地方政府操盘过程中从强资本转向抢年轻人的逻辑。</p><h2 id="资本不再稀缺，人才稀缺"><a href="#资本不再稀缺，人才稀缺" class="headerlink" title="资本不再稀缺，人才稀缺"></a>资本不再稀缺，人才稀缺</h2><p>人才是21世纪最重要的发展要素。这个转变的原因：</p><ul><li>全球需求的萎缩，产能过剩<ul><li>别国更富的人变老了，在去杠杆</li><li>中国人也在变老，产能过剩</li><li>欧日长期处于负利率时代</li></ul></li></ul><p>所以目前是“资产荒”，有钱人不愿消费，年轻人没有钱。蛋糕难以做大。存量博弈时代，需求侧更加重要，才要抢人。</p><p>从别的地方抢年轻人，以邻为壑。抢人是划算的。极少部分人才是重要的，其他人只是为了保持基因多样性。10000人中最优秀的人被吸引走了，那这10000人价值其实很少了。人才的现金流十分可观。</p><p>年轻人的养老教育等都是将来时。但是年轻人带来的效益是立竿见影的。（深圳直呼内行。）</p><h2 id="鼓励地方政府恶性竞争？"><a href="#鼓励地方政府恶性竞争？" class="headerlink" title="鼓励地方政府恶性竞争？"></a>鼓励地方政府恶性竞争？</h2><p>这个从来不是新的东西。以前一直都是抢资本的恶性竞争。但是抢人会改变竞争的方式和重点。把补贴外国资本的资金拿来补贴本国年轻人。以人为中心，比倒贴资本来说，格局更有合理性。</p><h2 id="先有人还是先有产业？"><a href="#先有人还是先有产业？" class="headerlink" title="先有人还是先有产业？"></a>先有人还是先有产业？</h2><p>蛋生鸡还是鸡生蛋？有人就有市场。不一定是要人才，有需求的年轻人都是需要争取的对象。</p><p>BBC这种无良媒体，是媒体经济学家。他们不懂底层逻辑，只拿数据对比。人口流出地区，借很少的债都是在作孽。人口流入地区，多借债是没问题的。比如武汉区域位置很好，大学教育先进，长期来看，武汉的大学生一半左右留在武汉。此处来看基础设施建设很必要。</p><h2 id="如何抢人？"><a href="#如何抢人？" class="headerlink" title="如何抢人？"></a>如何抢人？</h2><ol><li>要形成良性竞争。年轻人在你这里生活很方便。基础设施，医疗教育跟上。</li><li>抢人再着急也不能抢外国人。</li><li>中西部不适合抢人，抢人适合大城市。</li></ol><p>政治经济学原理：</p><ul><li>财富的源头是人而不是物。</li><li>发展的本质是人的能力的提升而不是物的堆积。</li><li>技术进步导致人的消费比劳动更重要。</li></ul><p>所以年轻人比老人更重要。自由主义经济学主张，善待企业家。但是这个很容易变成，善待有钱人，厚待有钱人。民本主义政治经济学要善待本地的年轻人。</p><h1 id="2020-11-04-是什么让资源红利成了诅咒？政府需四大调控政策避免这些小城坐吃山空"><a href="#2020-11-04-是什么让资源红利成了诅咒？政府需四大调控政策避免这些小城坐吃山空" class="headerlink" title="2020-11-04 是什么让资源红利成了诅咒？政府需四大调控政策避免这些小城坐吃山空"></a>2020-11-04 是什么让资源红利成了诅咒？政府需四大调控政策避免这些小城坐吃山空</h1><h2 id="资源诅咒的问题"><a href="#资源诅咒的问题" class="headerlink" title="资源诅咒的问题"></a>资源诅咒的问题</h2><p>当一个国家发现某个市场需求巨大的自然资源，最初会有经济热潮，之后回落，升值可能有长期的经济萧条。资源反而是上天的诅咒。这个行业挣钱不挣钱和人是否努力关系不大</p><h2 id="国家的分类"><a href="#国家的分类" class="headerlink" title="国家的分类"></a>国家的分类</h2><p>四种分类</p><ul><li>中心美英：提供货币信用</li><li>次中心欧日韩：提供研发，高附加值产品</li><li>再外围中涂墨越等：卖血汗</li><li>最外围国家中东非洲拉美：卖资源</li></ul><p>在国家内部也存在这样的分工。发展不均衡。发展不是物的堆积，发展是让人的能力不断提升。</p><h2 id="资源诅咒的病理机制"><a href="#资源诅咒的病理机制" class="headerlink" title="资源诅咒的病理机制"></a>资源诅咒的病理机制</h2><p>多重作用路径复杂交织。</p><ol><li>市场角度，大宗商品波动大。给能源产地经济带来巨大冲击。<ul><li>价格上升期，其他产业萎靡，储蓄资金都涌入能源</li><li>价格下行期，围绕资源的产业全部倒掉。所有人几乎同时失业。</li><li>（老百姓放高利贷，利滚利发大财）</li></ul></li><li>政治和社会机制。<ul><li>收益容易被当地有钱有势的人窃取。</li><li>采掘技术要和大公司合作，民选总统往往是大公司的傀儡。</li><li>最后留给人民的往往是一地鸡毛。</li><li>存量博弈，导致当地矛盾越来越大。</li></ul></li></ol><h2 id="如何治理资源诅咒现象"><a href="#如何治理资源诅咒现象" class="headerlink" title="如何治理资源诅咒现象"></a>如何治理资源诅咒现象</h2><p>宏观调控，对资源采掘业抑制发展。</p><ul><li>深度研发，生产下游产品。</li><li>控制总收入。价格低多卖，价格高少卖。</li><li>把资源采掘业获得的现金流培养本地年轻人。</li><li>搞储备，建立财富基金，维持平稳收入和经济波动。</li></ul><h1 id="2020-11-11-特朗普用贸易战拖住中国，之后拜登能“贡献”什么？日本：我有一计…"><a href="#2020-11-11-特朗普用贸易战拖住中国，之后拜登能“贡献”什么？日本：我有一计…" class="headerlink" title="2020-11-11 特朗普用贸易战拖住中国，之后拜登能“贡献”什么？日本：我有一计…"></a>2020-11-11 特朗普用贸易战拖住中国，之后拜登能“贡献”什么？日本：我有一计…</h1><p>大选的看点：</p><ol><li>共和党已经抛弃特朗普，权贵阶层也抛弃。但是特朗普并没有输得很惨。拜登大赢没有出现。<ul><li>一部分古巴裔年轻人背叛了民主党。公交车原理，上车之后希望后面人不要来。</li></ul></li><li>投票和拜登关系不大，其实是对特朗普的表决。<ul><li>提前投票，邮寄投票很利于民主党。</li></ul></li><li>这次大选有可能严重伤害美国政治治理体系，戳破美国政治神话。<ul><li>民主神话，撕裂美国，无论选谁，另一半都不服。</li><li>法制神话，如果诉诸司法，最高法院，大法官怎么判。</li></ul></li><li>国会的归属，可能出现跛鸭。</li><li>民主党共和党分歧会扩大。</li></ol><h2 id="未来中美欧关系"><a href="#未来中美欧关系" class="headerlink" title="未来中美欧关系"></a>未来中美欧关系</h2><p>唯二能交流的话题：</p><ul><li>医疗环保</li><li>公共卫生</li></ul><h2 id="拜登为人"><a href="#拜登为人" class="headerlink" title="拜登为人"></a>拜登为人</h2><p>心胸宽广，情商高情绪稳定，可以谈成合作。但是年纪比较大。而且他其实也只是过客。关税战一定会被拜登停掉。即使加入TPP也对中国影响较小。</p><h1 id="2020-11-18-它是决策者的参考也是投资者的僚机，请看今天的人口解读！"><a href="#2020-11-18-它是决策者的参考也是投资者的僚机，请看今天的人口解读！" class="headerlink" title="2020-11-18 它是决策者的参考也是投资者的僚机，请看今天的人口解读！"></a>2020-11-18 它是决策者的参考也是投资者的僚机，请看今天的人口解读！</h1><p>如何从中国人口曲线研究中国经济？人口是慢变量，塑造的是长期趋势。</p><p>2014年那个时间节点，中国各年龄段的人口分布：</p><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/zjqd/popularization-curve-2014.png" class=""></div><h2 id="中国人口曲线的特点"><a href="#中国人口曲线的特点" class="headerlink" title="中国人口曲线的特点"></a>中国人口曲线的特点</h2><p>从曲线来看</p><ul><li>50岁之后：自然规律，人的老化，曲线下降</li><li>50岁之前：社会规律，向左下方倾斜。总和生育率TFR下降（世界银行数据）</li><li>出生波动性很大。</li></ul><p>回顾中国人口（联合国儿童基金会数据）</p><ul><li>婴儿潮1963-1970中国，每年接近3000万。</li><li>回声潮1985-1989中国，每年近2500万。</li><li>2000年后新生儿掉到2000万之下。</li><li>单独二胎，2016全面二胎，2017反弹到1780万，然后迅速下跌。</li></ul><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/zjqd/birth-curve.png" class=""></div><p>人口的质量来看：</p><ul><li>接受过高等教育的人数随代际变化很快。<ul><li>婴儿潮（约30/3000）</li><li>今天（800/1500）</li></ul></li></ul><h2 id="人口曲线对政策制定的影响"><a href="#人口曲线对政策制定的影响" class="headerlink" title="人口曲线对政策制定的影响"></a>人口曲线对政策制定的影响</h2><p>2009年的人口数据，可以推断出2011年起中国人工工资将会出现大幅上涨。后金融危机时代，中国60后人口退出低端劳动力市场（比高端劳动力退的更早）。3000万人退休，但是只有不到1000万未接受高等教育的人接替，所以农民工数量大幅减少。所以工资水平上升。所以不担心农民工失业现象，即使没有经济增长，因为我们生不出孩子。</p><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/zjqd/popularization-curve-2009.png" class=""></div><p>中国经济增速保GDP稳定增加意义何在？1%GDP能多创造约近100万非农就业岗位。这是一种确保中国社会稳定的政治需要。</p><p>2012年之后，中国政府并不故意维持较高增速，我们低端劳动力就业压力降低。即便经济增速2020年，2%中国东南沿海企业仍然招工难。</p><p>未来很长一段时间，真正面临的是<strong>大学生就业压力</strong>。引入低端产业的同时，其实是把高端产业让给了西方国家。新冠疫情加速了这样一种以内循环为主的方向的切换。只有这样创造的岗位才是自主品牌设计，高端产业，研发等。</p><h2 id="人口曲线对个人投资决策的影响"><a href="#人口曲线对个人投资决策的影响" class="headerlink" title="人口曲线对个人投资决策的影响"></a>人口曲线对个人投资决策的影响</h2><p>需要配合另一条曲线。人一生的消费曲线。年轻人没钱有欲望，老年人有钱没欲望，所以消费的都少。消费高峰期时是中年人。从统计意义来看，人哪一年生孩子哪一年买车买房，哪一年孩子出国是一个定数，符合大数定理。</p><p>一般来看，大学毕业要结婚会买房。42岁事业有成收入较高换大房子。47岁左右消费高峰。52岁买豪车。60岁医药支出的高峰。65岁养老度假高峰。</p><p>这样结合婴儿潮和回声潮：</p><ul><li>2005-2012全中国会有一个房地产牛市（从90年代推断）。</li><li>2016-2021会有房地产小牛市，小户型为主，大城市远郊区。<ul><li>85-89年婚房消费高峰。</li></ul></li><li>2027-2030最后一次温和的牛市，好于2016-2021，弱于2005-2012。<ul><li>集中在大户型，回声潮买房人生巅峰。婴儿潮养老房。</li></ul></li></ul><p>年轻人口持续聚居的区域依旧会上涨。流失的区域则会下跌。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>过去式，未来中国人口曲线波动性很少。但是可以用到，一带一路沿线国家投资。</p><h1 id="2020-11-25-美国：10年了，我都想把中国从全球经济里叉出去！RCEP：现在梦醒了吗？"><a href="#2020-11-25-美国：10年了，我都想把中国从全球经济里叉出去！RCEP：现在梦醒了吗？" class="headerlink" title="2020-11-25 美国：10年了，我都想把中国从全球经济里叉出去！RCEP：现在梦醒了吗？"></a>2020-11-25 美国：10年了，我都想把中国从全球经济里叉出去！RCEP：现在梦醒了吗？</h1><h2 id="中国加入RCEP的影响"><a href="#中国加入RCEP的影响" class="headerlink" title="中国加入RCEP的影响"></a>中国加入RCEP的影响</h2><p>短期内对中国的经济影响有限。10-20年生效。给各国国内一个缓冲阶段。</p><p>较发达国家使用负面清单，而中国需要6年从正面清单转向负面清单。这个要求政府对经济运行体系管制效率题能力要求更高。</p><p>（来自朋友的数据）经济学量化模型，模拟对各国福利增长的影响，增长有限。但是RCEP的价值更多的在于重塑东亚地区地缘经济体系。</p><h2 id="RCEP对东亚供应链影响"><a href="#RCEP对东亚供应链影响" class="headerlink" title="RCEP对东亚供应链影响"></a>RCEP对东亚供应链影响</h2><p>中日韩等经济链依附于美欧消费市场，是全球外围地区。现在通过全世界最大的自由贸易协定，整个体系就能够构成一个较为完整的经济体。但是还缺一个大市场，之后中国的再分配和经济升级，中国将成为全球最大市场。</p><h2 id="RCEP对中日关系的影响"><a href="#RCEP对中日关系的影响" class="headerlink" title="RCEP对中日关系的影响"></a>RCEP对中日关系的影响</h2><p>日本在过去50年间难以扮演实质的东亚经济领头羊角色。RCEP实质实现了中日之间的自由贸易协定。可以理解为日本即将回归亚洲，以中国为中心的东亚贸易体系（经济上）。只有在经济政治安全建立深度依赖的时候才能看到“亚洲是亚洲人的亚洲”。</p><h2 id="RCEP对CPTPP的影响"><a href="#RCEP对CPTPP的影响" class="headerlink" title="RCEP对CPTPP的影响"></a>RCEP对CPTPP的影响</h2><p>面对TPP中国需要做很多改革：工会、知识产权、环保、争端裁量权。否则需要自绝于东亚。所以中国加入RCEP谈判，这是一种对TPP的反制。2017美国退出TPP，日本看摊CPTPP。</p><p>中国政策延续性很好，不会否定前任。如果2021年初把中欧投资协定搞定，中日中欧大局已定。美国排挤中国的图谋难以成功了。</p><h2 id="RCEP看中印对比"><a href="#RCEP看中印对比" class="headerlink" title="RCEP看中印对比"></a>RCEP看中印对比</h2><p>印度自绝于东亚经贸新体系。印度国家能力较弱。国家能力指可以强势的进行国内利益再分配。（来自社会革命改造，人民的信任。）印度的自治自立更多的是一种政治审美，而不是社会科学。逃遁避世，拒绝先进，是一条自我毁灭之路。穆迪想学邓小平，但是缺乏毛泽东帮他进行必要的社会改造。</p><h1 id="2021-01-06-复盘2020年我的十个预判，2021年再做七个展望"><a href="#2021-01-06-复盘2020年我的十个预判，2021年再做七个展望" class="headerlink" title="2021-01-06 复盘2020年我的十个预判，2021年再做七个展望"></a>2021-01-06 复盘2020年我的十个预判，2021年再做七个展望</h1><h2 id="2020年初的判断"><a href="#2020年初的判断" class="headerlink" title="2020年初的判断"></a>2020年初的判断</h2><ol><li>1月份，乐观面对疫情，认为5月能战胜病毒。认为病毒对欧美伤害更严重。是一次大考，考国家动员能力。但是没有想到美欧如此惨烈。</li><li>西方社会不仅会面临金融波动，还会大规模失业，引起政治危机。有些误判。之前支持桑德斯年轻人会打败建制派，预测错了。</li><li>预判对华舆情恶化。人的本能，在困难的时候妒忌和指责。2020，中国形象面临挑战。</li><li>反对美股会崩，预判标普跌破2300之后反弹，到冬季疫情重来之后再次下跌。但是没有料到美元防水情况下市场特立独行。</li><li>认为人民币成为全球金融避险地之一。成为逆周期货币。预测对了，人民币升值。（虽然比多年前的预测弱了一些）</li><li>大宗商品的波动。现在来看只能说大体正确。</li><li>预测美国传统产业出现破产潮。但是在美联储和政府的救助之下，没有出现。这个预测是错的。</li><li>认为2020年制造业不会离开中国反而会有全球占比扩张和升级。这方面说对了。东亚复苏最早，相对占比升高。</li><li>预测全世界封锁中国到中国封锁全世界。</li><li>中国资本市场，会有一个小牛市。我们需要一个牛市，预判破4000，甚至到4300.局部牛市体现在科创板创业板。</li></ol><p>做预测判断不是为了牟利。是为了检验底层知识框架和对世界的理解和认识是否准确。总体来看，方向大致正确，精确度有待提高。</p><h2 id="2021年的预测"><a href="#2021年的预测" class="headerlink" title="2021年的预测"></a>2021年的预测</h2><ol><li>2021年，中美关系局部回暖，但是科技战，技术封锁持续。</li><li>中欧之间双边投资协定的签订和生效可能会有反复。目前仅完成了草签。部分国家议会可能难以支持。</li><li>中国内循环方面，解决卡脖子问题方面将会有多个好消息。</li><li>2021新冠疫苗生产效能竞争会成为热点。这里中国拥有优势。</li><li>2021全球多边主义有所回暖。尤其是中美欧三大经济体讨论环保节能气候变化。整治避税天堂。</li><li>2021美欧发达国家可能会使坏，怂恿重债发展中国家向中国减免债务。面临外交压力。</li><li>人民币CFET指数仍热保持一定强势。</li></ol><h1 id="2021-01-13-美联储已经躺平乎？你我正在买单矣"><a href="#2021-01-13-美联储已经躺平乎？你我正在买单矣" class="headerlink" title="2021-01-13 美联储已经躺平乎？你我正在买单矣"></a>2021-01-13 美联储已经躺平乎？你我正在买单矣</h1><p>之前2020年三月专家会认为道指会掉到10000点之下。但是却涨破30000点。这就是无铆货币时代的印钱的魅力。</p><h2 id="美联储量化宽松政策回顾和预测"><a href="#美联储量化宽松政策回顾和预测" class="headerlink" title="美联储量化宽松政策回顾和预测"></a>美联储量化宽松政策回顾和预测</h2><p>次贷危机之后</p><ul><li>2008年9000亿</li><li>2014年4.2万亿</li><li>2019年8月中3.7万亿</li><li>2020年疫情之前4.2万亿</li><li>2020年12月7.2万亿</li></ul><p>据说2022年下一位美联储主席可能是义务现代货币理论簇拥着的女性专家。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">现代货币理论认为，现代货币体系实际上是一种政府信用货币体系。</span><br><span class="line">主权货币不与任何商品与其他货币挂钩，只与未来税收债券相对应。</span><br></pre></td></tr></table></figure>所以2025年拜登第一任期结束，美联储资产负债表可能是15-16万亿美元左右。那么从2008到2025年年复合增速大约为18%（印钱速度，美元资产贬值速度）。美元资产增值速度低于18%则是在为美国国民福利长治久安做贡献。</p><h2 id="美联储资产负债表扩张，为什么美国没有严重通胀？"><a href="#美联储资产负债表扩张，为什么美国没有严重通胀？" class="headerlink" title="美联储资产负债表扩张，为什么美国没有严重通胀？"></a>美联储资产负债表扩张，为什么美国没有严重通胀？</h2><p>第一大持有人是美联储，规模大于所有外国政府持有的总和。两种通胀缓冲作用：</p><ul><li>金融系统的冻结效应，弱化了信用创造过程，m1扩张但是m2、m3还没有扩张，推迟了通胀的表达。</li><li>美元作为全球储备货币，可以全球获得商品服务。通胀是全球性的。越外围通胀越高。在产能过剩，老龄化严重的经济体甚至可能出现通缩。</li></ul><h2 id="美元大放水流动性去哪里了？"><a href="#美元大放水流动性去哪里了？" class="headerlink" title="美元大放水流动性去哪里了？"></a>美元大放水流动性去哪里了？</h2><p>三个池子吸收流动性：</p><ol><li>商品：可贸易品。全球产能过剩，美国也人口老化消费降低，所以价格不会涨得特别快。这个池子不会吸收太多流动性。</li><li>服务：增加理发餐饮价格，但是这些东西不可囤积。不能作为对抗通胀保值增值的手段。</li><li>资产：股市房地产债券。</li></ol><h2 id="美联储可以无限印钞而没有约束吗？"><a href="#美联储可以无限印钞而没有约束吗？" class="headerlink" title="美联储可以无限印钞而没有约束吗？"></a>美联储可以无限印钞而没有约束吗？</h2><p>理论上没有限制，只要全世界愿意用劳动换美元。</p><p>共和党强调小政府，控制赤字。但是共和党总统债务扩张大幅加快。现代货币理论学者，说债务大不是问题。</p><p>扩张没有边界，但是扩张速度还是有限制。利息支出不能失控。过去20多年美国国债存量巨幅增长，但是债务利息支出稳定。美国国债利率一路降低。</p><p>如果未来通胀加剧，美联储不得不加息，导致美联邦财政借新还旧滚动存量债务利息翻倍。全球和美国本土储蓄者敢不敢相信美元？哪些因素导致美国本土通胀率走高呢？（理论和政策研究的富矿，值得研究）</p><h2 id="全球性防水带来的影响"><a href="#全球性防水带来的影响" class="headerlink" title="全球性防水带来的影响"></a>全球性防水带来的影响</h2><ol><li>操纵汇率。美财政部将越南（操盘水平低）瑞士（货币受欢迎必须管控汇率）列为汇率操纵国。中国连续三年是汇率操纵的警告对象。</li><li>资产价格飙升。美国股市，房地产。数字货币。全球资产配置者需要把小比例资金放入某种保险柜对冲。老年人的保险柜（黄金），年轻人的保险柜（数字货币）。小泡沫（黄金比特币）并不是大事。</li><li>汇率的波动。美元指数年初100+到现在89。美国贸易逆差大幅扩张。</li></ol><h2 id="中美金融体制重大差异"><a href="#中美金融体制重大差异" class="headerlink" title="中美金融体制重大差异"></a>中美金融体制重大差异</h2><p>中国m2和美国比较，中国GDP低但是m2大，不专业也不合理。</p><ul><li>美国m2:m1+储蓄存款、小额定期、零售的货币市场基金</li><li>中国m2:m1+个人存款企业存款、人民币存款和外汇存款</li><li>美国m2不包含10w远以上的大额和外汇存款。美国m2统计口径小于中国。</li><li>真正可比的是m3</li><li>或者比较中国净主权信用货币（资产负债表-外汇占款），和美国的资产负债表。中国信用增长率10.8%，美国18%。</li></ul><p>中国不应该建立巨额的外汇储备。</p><h1 id="2021-01-20-经济战该怎么打？拿破仑大陆封锁政策是最生动的反面教材！"><a href="#2021-01-20-经济战该怎么打？拿破仑大陆封锁政策是最生动的反面教材！" class="headerlink" title="2021-01-20 经济战该怎么打？拿破仑大陆封锁政策是最生动的反面教材！"></a>2021-01-20 经济战该怎么打？拿破仑大陆封锁政策是最生动的反面教材！</h1><h2 id="什么是经济战"><a href="#什么是经济战" class="headerlink" title="什么是经济战"></a>什么是经济战</h2><p>使用经济金融手段，打击削弱对方的实力和社会稳定性，改变对方政治政策和行为。</p><p>历史上：</p><ul><li>法国拿破仑对英国的大陆封锁</li><li>1950苏伊士运河，美国对英法制裁，迫使从埃及撤兵。</li></ul><p>特朗普政府：</p><ul><li>技术禁运，华为等公司的制裁</li><li>人才回归审查</li><li>关税战（针对很多贸易伙伴），一般不能称为经济战，不是以政治为目的。</li></ul><h2 id="拿破仑的经济战"><a href="#拿破仑的经济战" class="headerlink" title="拿破仑的经济战"></a>拿破仑的经济战</h2><p>军事成功，大战略层面失败。打遍天下欧洲无敌手之后，发动对英国的经济战。因为海军被摧毁，不能通过暴力摧毁英国。</p><p>拿破仑犯了很多战略错误：</p><ul><li>对暴力管制迷信，对市场力量的轻视。管制内外形成巨大的价格落差。</li><li>不理解金融在国家间斗争的作用。没有把货币纳入考虑。<ul><li>允许英镑在欧洲流通。</li></ul></li><li>重商主义经济里面，拿破仑想把英国黄金挣完，使英国破产。<ul><li>在金本位下，谁黄金多谁东西贵。</li></ul></li><li>对自身的政治定位没有与时俱进。<ul><li>控制欧洲之后仍然把法国利益作为首位。</li></ul></li></ul><h2 id="中国古代经济战"><a href="#中国古代经济战" class="headerlink" title="中国古代经济战"></a>中国古代经济战</h2><p>管仲，服帛降鲁。倡导穿帛，但是不允许产丝，从鲁国进口丝，鲁国农民不种粮食了。后来禁止穿帛，同时禁止粮食出口鲁国。鲁国饥荒，通货膨胀，鲁王投降。</p><p>拿破仑应该低价出售粮食且高价收购英国棉布。之后同时宣布对英国禁运粮食，对英国公司债作废。</p><h2 id="如何经济战"><a href="#如何经济战" class="headerlink" title="如何经济战"></a>如何经济战</h2><p>经济战不要想挣钱，是会赔钱的。经济学家认为制裁要满足一致性原则、脆弱性原则（认为没有道理）。</p><p>经济战重要的不是压力的绝对值，而是压力的波动性。受打击对象短期收到极大压力无法有效调整。经济战要形成不利于对手，而操之在我的供求关系的巨幅波动。</p><h1 id="2021-01-27-美元与黄金脱钩的50年：富国债务率在变高，而穷国外汇储备却变大"><a href="#2021-01-27-美元与黄金脱钩的50年：富国债务率在变高，而穷国外汇储备却变大" class="headerlink" title="2021-01-27 美元与黄金脱钩的50年：富国债务率在变高，而穷国外汇储备却变大"></a>2021-01-27 美元与黄金脱钩的50年：富国债务率在变高，而穷国外汇储备却变大</h1><p>自由主义右翼经济学家喜欢用宏观杠杆率来衡量经济体的健康程度。中国宏观杠杆率高，变化太快。</p><h2 id="探讨债务问题要和货币联系起来"><a href="#探讨债务问题要和货币联系起来" class="headerlink" title="探讨债务问题要和货币联系起来"></a>探讨债务问题要和货币联系起来</h2><p>以di币为例。用自己可以发行印制的钞票界定你我之间的债权债务关系，那我借的钱越多，我占你的便宜越大。用黄金等自己不能印的货币借钱，哪怕借的少都是麻烦。</p><h2 id="1971年之后债务情况"><a href="#1971年之后债务情况" class="headerlink" title="1971年之后债务情况"></a>1971年之后债务情况</h2><p>富国债务率越来越高，穷国有越来越大外汇储备</p><p>毛轱辘的观点：这个是民主制度中，政治家欺负两类没有选票的利益相关者。外国人和后代。美欧日这几十年的积累债务就是在欺负外国人和后代。</p><p>翟老师观点：原因是金本位废除，主权债务的货币约束消失。而不是民主制度。1970年前美欧宏观杠杆率下行，之后持续上行。1971年之前民主制度没有带来杠杆率上升。</p><h2 id="黄金非货币化对世界运行方式的影响"><a href="#黄金非货币化对世界运行方式的影响" class="headerlink" title="黄金非货币化对世界运行方式的影响"></a>黄金非货币化对世界运行方式的影响</h2><p>意义重大，影响深远。1971年前，价值有黄金定义，像一个黄金铺满的平面，相对稳定。1971年之后，就像一个美元铺满的球面，各国央行往里充气。这个气是大国的主权债务（信用变成财富）。</p><p>个人和企业想挣到钱，宏观前提是全球主要大国愿意提供负债来支撑经济的信用。</p><h2 id="华尔街的观点"><a href="#华尔街的观点" class="headerlink" title="华尔街的观点"></a>华尔街的观点</h2><p>桥水基金Ray Dalio尝试用皇冠杠杆率解释金融市场经济波动国家兴衰。</p><ul><li>量纲不对，GDP流量，债务存量</li><li>1971之前和之后混为一谈。技术分析的假设，过去现实未来内在一致性，这个假设不一定成立。</li><li>单因素论，尝试找到一个因素。</li></ul><p>华尔街很多人把意识形态偏见当作定理科研学术。追求深刻惊悚带节奏，不追求真理。</p><p>某些华尔街资产管理人和民科思想家说的和中国有关的东西可信度很低。因为中国为大客户，他们要说一些讨喜或者耸人听闻的话。就像“邹忌讽齐王纳谏”。</p><h2 id="当今世界债务"><a href="#当今世界债务" class="headerlink" title="当今世界债务"></a>当今世界债务</h2><p>全球宏观风险宏观杠杆率逻辑关系，1980年以来：</p><ul><li>发达国家债务率走高，发展中国家比较低。</li><li>哪些因素导致经济体债务率高不出问题，债务耐受度。</li><li>债务率高低和宏观风险无关，非本币计价的主权债务与宏观风险高度相关。</li></ul><p>发展中国家用本币借不到债，导致发展中国家的原罪，金融体系中遭受某种先天制裁。</p><h1 id="2021-02-03-建设后疫情时代的“一带一路”，输出我们强政府的经验与能力"><a href="#2021-02-03-建设后疫情时代的“一带一路”，输出我们强政府的经验与能力" class="headerlink" title="2021-02-03 建设后疫情时代的“一带一路”，输出我们强政府的经验与能力"></a>2021-02-03 建设后疫情时代的“一带一路”，输出我们强政府的经验与能力</h1><p>后疫情时代一带一路怎么搞。面临一些变革的必要和升级换代的机遇。</p><h2 id="民本主义政治经济学"><a href="#民本主义政治经济学" class="headerlink" title="民本主义政治经济学"></a>民本主义政治经济学</h2><p>四组关系：</p><ul><li>中心和外围：中心剥削，但是中心更重要，供养昂贵的高精尖项目 </li><li>公共部门和私人部门的关系：公共部门提供公共产品，成本是综合税率</li><li>人和物的关系：财富不是物的堆积，人才是财富。人的生产消费创新才是财富的源头。</li><li>可贸易品和不可贸易品的关系：可贸易是财富的生产，不可贸易是财富的再分配。不可贸易品的定价区别来自于可贸易品的盈利能力。所以发展的时候是要发展可贸易品。</li></ul><p>如何理解美国梦，第五个命题：我们目前处于主权信用的大泡沫中。美国梦，他激动世界就繁荣，他低落世界就萧条。中华民族伟大复兴，我们自己做梦。</p><h2 id="一带一路背景下"><a href="#一带一路背景下" class="headerlink" title="一带一路背景下"></a>一带一路背景下</h2><p>美国梦即将苏醒，中国将进入中心。一般美国人民不愿意在进行全球化。西方世界变老了变小气了，也消费不动这么多东西了。不断印钱，不断稀释债权的游戏不可持续怎么办。</p><p>那我们和世界外围塑造一个新的泡泡，以前我们在别人的梦境，现在自己做梦。中国人消费是全球最大市场。旧泡泡我们是债主，但是用别人的货币定价。新泡泡我们是加之基准的确定者，定价货币人民币。</p><h2 id="外围国家为什么贫穷"><a href="#外围国家为什么贫穷" class="headerlink" title="外围国家为什么贫穷"></a>外围国家为什么贫穷</h2><p>自由主义认为原因是缺少资本，但是其实不是，王公贵族很有钱。原因在于当地缺少公共产品，因为没有强政府。强政府指国家机器对社会的管控识别动员。</p><p>要帮他们发展起来，要输出如何如何建立强政府的经验和能力。就是国家建设的现代化。</p><h2 id="一带一路的目的是什么"><a href="#一带一路的目的是什么" class="headerlink" title="一带一路的目的是什么"></a>一带一路的目的是什么</h2><p>追逐的是一带一路年轻生命。他们愿意消费但是没有钱。我们借钱给他们，他们有了现金流购买我们的产品。之后进一步投资借债，让这些年轻人进入我们的循环（但不是生活）。多种族聚居会带来惨重教训（欧美，五胡乱华）。</p><h2 id="一带一路三个倡议"><a href="#一带一路三个倡议" class="headerlink" title="一带一路三个倡议"></a>一带一路三个倡议</h2><ol><li>多投资，少放债。用股权发展人民币计价的全球二级市场。</li><li>多搞集中投资，少搞分散投资（管控减弱）。</li><li>帮助他们获得可贸易部门的繁荣。设立特区，有主权没有治权，工业化新城，种族多元文化一元。塑造新中国人群体。</li></ol><p>人类命运共同体实现过程就是漫长而曲折的。</p><h1 id="2021-02-24-如何诱导国家间的合作与互信？推演博弈论在外交策略上的应用"><a href="#2021-02-24-如何诱导国家间的合作与互信？推演博弈论在外交策略上的应用" class="headerlink" title="2021-02-24 如何诱导国家间的合作与互信？推演博弈论在外交策略上的应用"></a>2021-02-24 如何诱导国家间的合作与互信？推演博弈论在外交策略上的应用</h1><p>利用囚徒困境研究自私的人类何以可能合作和相互信任。个人国家来讲什么处世之道是合理的。</p><h2 id="囚徒困境"><a href="#囚徒困境" class="headerlink" title="囚徒困境"></a>囚徒困境</h2><p>关于囚徒困境：</p><ul><li>什么是囚徒困境？<ul><li>两个囚徒是否招供。</li><li>纳什均衡。</li></ul></li><li>人性假设，人是不是自私的？<ul><li>生物学来讲是自私的。保存基因。</li><li>但是也有例外，利他主义。蚂蚁蜜蜂，共享75%的基因</li></ul></li></ul><h2 id="生活中的囚徒困境"><a href="#生活中的囚徒困境" class="headerlink" title="生活中的囚徒困境"></a>生活中的囚徒困境</h2><p>明知合作可以共赢，但是理性的自私信任缺乏带来背叛。即使自己不贪心，但是难以相信对方不贪心。熟人社会，互相之间是友善的。</p><h2 id="是否存在最优的博弈策略"><a href="#是否存在最优的博弈策略" class="headerlink" title="是否存在最优的博弈策略"></a>是否存在最优的博弈策略</h2><p>在罗伯特·艾克斯洛德的实验中，他向各界征集了一些列博弈策略。第一轮策略评比中，“tit for tat”（一报还一报）策略在实验中获得了第一。第二轮评比中，告诉大家该策略最优，使大家改进。但是“tit for tat”策略仍旧获得了第一。</p><p><strong>“tit for tat”</strong>：第一步假设对方是好人，之后重复对方上一步的操作。这个策略可以最有效的鼓励其他程序与自己长期合作。这个简单的策略有以下品质：</p><ol><li>善良。假设世界是对自己好的。</li><li>可激怒的。会产生报复。</li><li>宽容。如果对方弃恶从善可以原谅对方。</li><li>不嫉妒。不争取高于对手的分数。</li></ol><p>建议阅读，《合作的进化》，被诺贝尔经济学家得主认为比其自身的研究更高明。</p><h2 id="从博弈论中得到的人生信条"><a href="#从博弈论中得到的人生信条" class="headerlink" title="从博弈论中得到的人生信条"></a>从博弈论中得到的人生信条</h2><ol><li>友善。</li><li>有原则。</li><li>宽容。</li><li>简单。</li><li>不妒忌朋友的成功。</li></ol><h2 id="一报还一报实操的问题"><a href="#一报还一报实操的问题" class="headerlink" title="一报还一报实操的问题"></a>一报还一报实操的问题</h2><p>朋友关系不一定是囚徒困境。很多时候高成本低回报。现实中很多时候回报的程度不确定。如果双方都采用这个策略，误解可以导致以恶报恶。</p><h2 id="利用圈子解决人际关系中的问题"><a href="#利用圈子解决人际关系中的问题" class="headerlink" title="利用圈子解决人际关系中的问题"></a>利用圈子解决人际关系中的问题</h2><p>双边关系中的回报放到多边关系中进行操作，很多问题可以引刃而解。用关系网分担了人心的自私带来的背叛的诱惑。</p><h2 id="国与国之间，对外战略的启示"><a href="#国与国之间，对外战略的启示" class="headerlink" title="国与国之间，对外战略的启示"></a>国与国之间，对外战略的启示</h2><p>多边主义外交，一个国家的背叛的潜在损失很大。背负信用压力的大国，越是利用自己的信用，别人越敢相信。</p><p>中国礼让谦虚大度，捍卫原则根本利益，不羡慕别人的成功。</p><p>国与国之间应该把一次性的囚徒困境转化为重复博弈，诱导国与国之间的信任和合作，培育出信任与和平。</p>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;翟老师与观视频合作的系列视频，内容可能较为广，此处总结要点便于回顾。
整个系列视频的时间为 2020-06 至 2021-02。
整个系列视频约有40节，内容涵盖政治经济教育文化等方面。
是翟老师其自身研究观点的整理与科普输出，值得思考。&lt;/p&gt;</summary>
    
    
    
    <category term="Economy &amp; Finance" scheme="https://superuier.github.io/categories/economy-finance/"/>
    
    
    <category term="翟东升" scheme="https://superuier.github.io/tags/%E7%BF%9F%E4%B8%9C%E5%8D%87/"/>
    
  </entry>
  
  <entry>
    <title>大师计划系列视频</title>
    <link href="https://superuier.github.io/economy-finance/didongsheng/dashi/"/>
    <id>https://superuier.github.io/economy-finance/didongsheng/dashi/</id>
    <published>2021-06-14T06:09:31.000Z</published>
    <updated>2021-07-13T16:42:31.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>记录一下翟老师《大师计划》视频的内容。<br>视频时间：2021.06</p><a id="more"></a><h1 id="1-当今世界格局与力量对比"><a href="#1-当今世界格局与力量对比" class="headerlink" title="1. 当今世界格局与力量对比"></a><strong>1. 当今世界格局与力量对比</strong></h1><p><strong>幂律</strong>现象：强者恒强弱者恒弱。</p><p>美国领先，中国紧随其后，但是远超其他。</p><ul><li>GDP规模</li><li>财政开支，军费开支</li><li>互联网巨头平台</li><li>独角兽企业数量</li><li>顶级大学</li><li>博士学位，高被引学者</li><li>专利产出，人工智能</li></ul><p>中国仍未到达第二的地方</p><ul><li>国际舆论话语权</li><li>国际组织影响力</li><li>货币国际化指标：美欧英日中</li></ul><p>超过或正在超过美国：</p><ul><li>出口贸易额</li><li>国内消费市场</li><li>理工科大学生人数</li><li>中等收入人群规模（4.5-5亿，是否具有小汽车）</li><li>汽车市场</li><li>智能手机销售</li><li>互联网接入用户</li><li>制造业产出增加值</li><li>能源资源消耗</li><li>500强数目</li><li>5g技术</li></ul><h1 id="2-大胆拥抱崛起的中国"><a href="#2-大胆拥抱崛起的中国" class="headerlink" title="2. 大胆拥抱崛起的中国"></a><strong>2. 大胆拥抱崛起的中国</strong></h1><p>至2013-2035年。</p><ul><li>目前大学生知识分子较多，能积极参与社会分工。</li><li>2012年前出口导向，央行资产负债表扩张（以美元储备作为基础）。</li><li>2014年前中国主权信用严重压抑，将来有巨大发展空间。</li><li>2013年以来，出口对外依赖度转变。提出并推动一带一路（非美经济体）。</li></ul><p>以美元信用为基础发行货币，无论你的质量科技含量再怎么高，都是加强美元的购买力。所以真正重要的不是跑的多努力，而是往什么方向跑。</p><h1 id="3-中国相对于美国“道”的优势"><a href="#3-中国相对于美国“道”的优势" class="headerlink" title="3. 中国相对于美国“道”的优势"></a><strong>3. 中国相对于美国“道”的优势</strong></h1><p>人类文明史上两种策略的竞争和对立（格劳秀斯的海洋文明流派 &amp; 康德的大陆文明流派）：</p><ul><li>美国：选取政治、三权分立、私有产权、自由市场</li><li>集体价值本位：个体服从整体，自上而下分配管控</li></ul><p>前者在市场竞争有优势（利益，贪婪），后者在军事上有优势（荣誉感，恐惧）</p><p>国家于市场的关系具有三角对立关系。国家发展的好取决于公共部门的能力和特质。底边越长越好。税率越低越好效率越高越高。</p><div style="width:60%; margin:auto"><img src="/economy-finance/didongsheng/dashi/public.png" class=""></div><div style="width:60%; margin:auto"><img src="/economy-finance/didongsheng/dashi/tax.png" class=""></div><p>学习苏联，把底边建立。（但是苏联的税点特别高）学习美国及盟友，降低综合税率，所以出现了繁荣的私人部门。两者不能相互否定，而是要正体反体合体（中庸之道）。</p><h1 id="4-中国相对于美国“治”的优势"><a href="#4-中国相对于美国“治”的优势" class="headerlink" title="4. 中国相对于美国“治”的优势"></a><strong>4. 中国相对于美国“治”的优势</strong></h1><p>两个重要任务：</p><ol><li>数字经济的转型：机遇和冲击<ul><li>数据所有权对社会的冲击，国内现在正在规范，之前也进行了对本土公司的保护。</li><li>社会政治影响，民众识字率大幅提升，年轻人只从互联网获取和传播再加工信息，大众政治由小众政治所取代。</li></ul></li><li>解决贫富分化。<ul><li>美国：印钱，加税</li><li>中国：反腐，对不合规资本系打击，财富再分配（精准扶贫）</li></ul></li></ol><h1 id="5-美国衰落的主要原因"><a href="#5-美国衰落的主要原因" class="headerlink" title="5. 美国衰落的主要原因"></a><strong>5. 美国衰落的主要原因</strong></h1><p>盎格鲁撒克逊王朝的帝国的生命周期末端的老年病：</p><ul><li>精神分裂：<ul><li>两党意识形态分歧变大，社会极化，攻势瓦解。</li><li>种族上也有投影。共和党白人，民主党有色人种。</li><li>建制派和民粹分裂。民众认为精英欺负大家。</li><li>四个所谓的美国：<ul><li>极左：民主党民粹派（桑德斯、沃伦，主张现代货币理论，发钱）</li><li>偏左：民主党建制派（拜登希拉里，拜登政策在讨好民粹派）</li><li>偏右：共和党精英（米奇·麦康奈尔，切尼，布什。正在失去共和党基本盘）</li><li>极右：共和党民粹（特朗普。占据共和党基本盘。）</li></ul></li></ul></li><li>脑瘤：<ul><li>华尔街的金融资本。70年代之后被一个特殊族群主导。高盛出产的精英在政界影响巨大。</li><li>内政上金融压倒实业。</li><li>外交上偏向于对中东伊斯兰的镇压征服。</li><li>经常制造泡沫。</li></ul></li><li>肝癌：<ul><li>肝癌导致有机体力量萎缩。</li><li>医疗医保医药构成的医疗系统挤占大量的财政空间。</li><li>美国是在医疗上制造支出最大的经济体，但是是OECD国家中人均寿命明显偏低的。</li><li>三医系统的政治捐赠最多。</li></ul></li></ul><h1 id="6-在中美的长期竞争中，我们如何赢得胜利"><a href="#6-在中美的长期竞争中，我们如何赢得胜利" class="headerlink" title="6. 在中美的长期竞争中，我们如何赢得胜利"></a><strong>6. 在中美的长期竞争中，我们如何赢得胜利</strong></h1><ol><li>要调整好心态。前倨而后恭。<ul><li>2035年前鼓起勇气敢于竞争。</li><li>2035年后美国可能经历惨烈的去杠杆。我们需要谦虚谨慎。</li><li>2035-2050如果美帝国出现崩盘，中俄关系可能也会出现不利于我们的翻转。</li></ul></li><li>赢得竞争不是两个国家的竞争，而是两个体系的竞争。<ul><li>美国同盟战略。源自罗马同盟大战略。</li><li>孟德斯鸠的研究，罗马在击败对手后吸纳盟友。但是盟友间不友好。而且会干涉盟友权力传承。</li><li>新的阵营浮现。美国赢得冷战后，应该避免使中俄结盟。</li><li>日本永远忠诚于最强大的国家。</li><li>印度对外政策，利用别国矛盾向双方索取好处。</li><li>我们对抗的是整个美国的同盟体系。</li><li>美国处境危险。多空双方对峙。多方中国没有加杠杆，空方拉盟友加杠杆才能均衡。如果中国加杠杆，空方可能爆仓。（如果未能兑现一个盟友的安全经济承诺。）</li><li>使他使用成本越来越高，用时间耗死他。</li></ul></li><li>做好国内再分配，和国际再分配。<ul><li>中央和地方的再分配。中央应该利用良好的信用敢于借钱，形成巨大的国债池。但是地方应该少借债。</li><li>地区再分配，东南沿海到中西部。</li><li>减少外汇储备，投到一带一路沿线的股权和债权。</li><li>积攒金钱是之前贫穷给我们带来的匮乏感的二次伤害。</li></ul></li></ol>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;记录一下翟老师《大师计划》视频的内容。&lt;br&gt;视频时间：2021.06&lt;/p&gt;</summary>
    
    
    
    <category term="Economy &amp; Finance" scheme="https://superuier.github.io/categories/economy-finance/"/>
    
    
    <category term="翟东升" scheme="https://superuier.github.io/tags/%E7%BF%9F%E4%B8%9C%E5%8D%87/"/>
    
  </entry>
  
  <entry>
    <title>Concerns for Academic Writing</title>
    <link href="https://superuier.github.io/research/academic-paper-writing/"/>
    <id>https://superuier.github.io/research/academic-paper-writing/</id>
    <published>2021-06-10T08:48:24.000Z</published>
    <updated>2021-06-10T08:48:24.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>A note about the precautions in academic writing. </p><a id="more"></a><h1 id="Try-to-avoid"><a href="#Try-to-avoid" class="headerlink" title="Try to avoid"></a>Try to avoid</h1><ul><li>Avoid and eliminate the fuzziness and subjectivity of language to the greatest extent.<ul><li>Avoid to use “we”, “I think”, etc.</li></ul></li></ul><h1 id="Try-to-do"><a href="#Try-to-do" class="headerlink" title="Try to do"></a>Try to do</h1><ul><li>Make the definitions clear.</li><li>Present the results in a legible way.<ul><li>Include both tables and figures.</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;A note about the precautions in academic writing. &lt;/p&gt;</summary>
    
    
    
    <category term="Research" scheme="https://superuier.github.io/categories/research/"/>
    
    
    <category term="Research" scheme="https://superuier.github.io/tags/Research/"/>
    
  </entry>
  
  <entry>
    <title>Hexo文档更新时间设置</title>
    <link href="https://superuier.github.io/software-tools/hexo/hexo-updated-time/"/>
    <id>https://superuier.github.io/software-tools/hexo/hexo-updated-time/</id>
    <published>2021-06-08T11:02:05.000Z</published>
    <updated>2021-06-08T11:02:05.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>Hexo 在使用远程部署时，默认 <code>update_option: mtime</code>, 即以最后修改时间作为更新时间。这个问题导致每次编译时，文章提交到远程，所有的文章都显示更新，且时间相同。具体解决方法则是在 Front-matter 中加入<code>updated:</code>项，则编译过后的更新时间以此为准。目前看来没有可以使其自动更新的方法。</p><a id="more"></a><h1 id="批量加入更新时间"><a href="#批量加入更新时间" class="headerlink" title="批量加入更新时间"></a>批量加入更新时间</h1><p>这里对之前原有的 post 我们可以按照生成时间 <code>date</code> 来批量添加 <code>updated：</code>具体事项过程即在 _post 目录下运行以下代码：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> `ls .`; <span class="keyword">do</span> value=`gawk <span class="string">&#x27;/date:/&#123;print &quot;updated: &quot;$2&quot; &quot;$3&#125;&#x27;</span> <span class="variable">$&#123;file&#125;</span>`; <span class="built_in">echo</span> <span class="variable">$&#123;value&#125;</span>; gsed <span class="string">&quot;3 a\\<span class="variable">$&#123;value&#125;</span>&quot;</span> -i <span class="variable">$&#123;file&#125;</span>; <span class="keyword">done</span>;</span><br></pre></td></tr></table></figure>需要注意的是，awk 和 sed 命令在 MacOS 下使用方法与 Linux 不同，此处应该使用 gawk 和 gsed。</p><h1 id="手动更新时间"><a href="#手动更新时间" class="headerlink" title="手动更新时间"></a>手动更新时间</h1><p>在 <code>_config.yml</code> 中设置 <code>update_option: date</code>，则更新时间与文章创建时间一致。虽然这样起不到“更新时间”的作用，但是至少不会无缘无故的对未更新的文件进行更新。如果出现一些重要的文章修改，手动更新时间添加 <code>updated:</code> 即可。</p>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;Hexo 在使用远程部署时，默认 &lt;code&gt;update_option: mtime&lt;/code&gt;, 即以最后修改时间作为更新时间。
这个问题导致每次编译时，文章提交到远程，所有的文章都显示更新，且时间相同。
具体解决方法则是在 Front-matter 中加入&lt;code&gt;updated:&lt;/code&gt;项，则编译过后的更新时间以此为准。
目前看来没有可以使其自动更新的方法。&lt;/p&gt;</summary>
    
    
    
    <category term="Software Tools" scheme="https://superuier.github.io/categories/software-tools/"/>
    
    
    <category term="Hexo" scheme="https://superuier.github.io/tags/Hexo/"/>
    
    <category term="Master Ma" scheme="https://superuier.github.io/tags/Master-Ma/"/>
    
  </entry>
  
  <entry>
    <title>翟东升教授文章摘录总结</title>
    <link href="https://superuier.github.io/economy-finance/didongsheng/articles/"/>
    <id>https://superuier.github.io/economy-finance/didongsheng/articles/</id>
    <published>2021-06-06T16:00:00.000Z</published>
    <updated>2021-06-06T16:00:00.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>作为翟老师的粉丝，翟老师的文章视频几乎每期必看。此处将翟老师一些微博，头条号中的文章进行观点记录，以便查阅。</p><a id="more"></a><p>翟（dí）东升：<br>中国人民大学国际关系学院副院长、博士、教授。<br>中国人民大学世界经济专业与国际政治经济学博士生导师。<br>中国人民大学国际货币所特聘研究员。  </p><p>研究方向： 货币与金融的国际政治经济学，中国对外经济关系美国政治经济</p><h1 id="人民币汇率、美元指数与全球通胀问题-2021-05-28"><a href="#人民币汇率、美元指数与全球通胀问题-2021-05-28" class="headerlink" title="人民币汇率、美元指数与全球通胀问题 2021-05-28"></a>人民币汇率、美元指数与全球通胀问题 2021-05-28</h1><ul><li>首先人民币汇率并不强，只是由于美元下跌看起来涨势明显。 </li><li>时至今日，美国仅仅是我们的第三大出口市场。要讨论汇率对实体经济的影响，必须看CFET指数，也就是人民币兑一揽子主要贸易伙伴的货币的加权平均值。 </li><li>人民币正在成为逆周期货币，且对美元应该有长期升值趋势 美元指数本身有17年左右的大周期，我们此刻处于新一轮下行周期的早期，考虑到新冠疫情冲击下美欧等国货币政策的现状和差异，明后两年内美元指数如果暴跌到70，无须惊讶。 </li><li>关于美联储货币政策是否会尽快转入收缩的问题，我的判断是，他们会口惠而实不至。美联储主席和总统的关系会影响到货币政策。 </li><li>鲍威尔若想获得连任，2022年之前很难加息缩表 我们此刻也许正在一个通胀率大时代的转折点上，未来三十年，也就是到2050年之前，全球主要经济体的通胀率也许是震荡上行的，或者说具有上涨容易下跌难的特点，各种预料之外的事情，比如疫情、战争、大国刺激生育和产业链调整等等，都会推动这个趋势的展开。</li></ul><h1 id="硕鼠的养成与美国的政治极化-2020-05-20"><a href="#硕鼠的养成与美国的政治极化-2020-05-20" class="headerlink" title="硕鼠的养成与美国的政治极化 2020-05-20"></a>硕鼠的养成与美国的政治极化 2020-05-20</h1><ul><li>任何一个政治经济体系都存在不完美或者有待弥补的漏洞。识别、利用并扩大这些漏洞，就能给某些聪明人快速收割巨额财富和权势的机会，但却是以国民和政府的损失为代价的。他们的财务成功不是因为创造了财富，不是因为发明了新的技术和工艺，而是利用了政治和经济体系中功能失调的那部分扭曲和漏洞。</li><li>尽管饱受批评和嘲讽，科赫兄弟仍然是美国的成功人士，或者说仍然在操纵着美国的政治经济乃至科研和教育，而他们在中国的模仿者如今纷纷落马。中国特色社会主义市场经济与美国资本主义市场经济的根本区别由此显现：事实雄辩地说明，中国走的是以人民为中心的路线，美国走的是以资本为中心的路线；我们是经得起考验的人民共和国，而美国越来越像一个“香蕉共和国”。哪个国家更有前途？我从2009年起就断言，中国更有前途。</li></ul>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;作为翟老师的粉丝，翟老师的文章视频几乎每期必看。
此处将翟老师一些微博，头条号中的文章进行观点记录，以便查阅。&lt;/p&gt;</summary>
    
    
    
    <category term="Economy &amp; Finance" scheme="https://superuier.github.io/categories/economy-finance/"/>
    
    
    <category term="翟东升" scheme="https://superuier.github.io/tags/%E7%BF%9F%E4%B8%9C%E5%8D%87/"/>
    
  </entry>
  
  <entry>
    <title>翟东升人民币国际化课程</title>
    <link href="https://superuier.github.io/economy-finance/didongsheng/rmb-courses/"/>
    <id>https://superuier.github.io/economy-finance/didongsheng/rmb-courses/</id>
    <published>2021-06-06T16:00:00.000Z</published>
    <updated>2021-07-05T16:00:37.000Z</updated>
    
    <content type="html"><![CDATA[<!-- omit in toc --><p>翟老师金融货币市场的课程。</p><a id="more"></a><h1 id="第一讲：为什么关于人民币汇率的悲观预测错了"><a href="#第一讲：为什么关于人民币汇率的悲观预测错了" class="headerlink" title="第一讲：为什么关于人民币汇率的悲观预测错了"></a>第一讲：为什么关于人民币汇率的悲观预测错了</h1><p>这一讲的目的是破除一定对于汇率的谬误。</p><p><strong>现象</strong>：<br>2014-2016汇率破7。所以多数持牌企业用人民币海外投资，资金成本昂贵但是收益低。只有当5%速度贬值时才能获利。</p><h2 id="看空人民币的5个错误理由："><a href="#看空人民币的5个错误理由：" class="headerlink" title="看空人民币的5个错误理由："></a>看空人民币的5个错误理由：</h2><ul><li>国内已过增长期</li><li>房地产泡沫不可持续，资产价格高估，只能通过汇率贬值调整</li><li>地方政府债务大，泡沫破裂会导致汇率下跌</li><li>过早的去工业化问题（东南亚，一带一路），产业和资本的转移。</li><li>中国GDP为50%，但是m2是美国的150%以上。</li></ul><h2 id="错在哪里（实证数据）："><a href="#错在哪里（实证数据）：" class="headerlink" title="错在哪里（实证数据）："></a>错在哪里（实证数据）：</h2><ul><li>经济增速和汇率相关吗？<ul><li>顺周期货币和逆周期货币表现不同。外围地区顺周期，中心地区逆周期。</li><li>中国变为顺周期。</li></ul></li><li>资产泡沫与汇率波动相关性？<ul><li>跨国资产价格并不具有一价定理（回归均值）</li><li>政府并不存在保汇率保房价问题</li></ul></li><li>地方政府债务并没有那么高。（债务/GDP）<ul><li>同时债务率和汇率之间负相关。只有有钱的人才能借到更多钱。</li><li>衡量债务风险的不是债务率，若以本币借债，本质上是一种隐形税收。</li><li>格林斯潘：“女士们先生们，大家不用瞎担心，这些债其实我们永远都不用还了。” （e.g.翟币）</li></ul></li><li>资本输出汇率不一定贬值。<ul><li>资本输出会让货币更加稳定，外部获取收益，导致长期强势。</li></ul></li><li>m2与汇率的关系。<ul><li>不公平的对比，评估美国要用m3（美国直接融资），中国m2（银行信贷）。</li><li>货币发行增速和汇率不相关。</li></ul></li><li>资本大鳄的做法既不道德也不慎重。</li></ul><h2 id="为什么人民币快速升值"><a href="#为什么人民币快速升值" class="headerlink" title="为什么人民币快速升值:"></a><strong>为什么人民币快速升值</strong>:</h2><ul><li>中国产业升级</li><li>中国老龄化</li></ul><h2 id="划重点，汇率并不是由以下这些点决定："><a href="#划重点，汇率并不是由以下这些点决定：" class="headerlink" title="划重点，汇率并不是由以下这些点决定："></a><strong>划重点</strong>，汇率并不是由以下这些点决定：</h2><ul><li>经济增速</li><li>房价波动</li><li>债务率</li><li>对外投资、产业迁移</li><li>货币发行增速</li></ul><h1 id="第二讲：长期汇率由什么决定"><a href="#第二讲：长期汇率由什么决定" class="headerlink" title="第二讲：长期汇率由什么决定"></a>第二讲：长期汇率由什么决定</h1><ul><li>短期汇率由市场情绪决定，不具有可预测性  </li><li>中期汇率（3-5年）由政府调控影响  </li><li>长期汇率（5-10）情况下，市场内在力量影响巨大</li></ul><h2 id="影响长期汇率的直接因素（实证数据验证）："><a href="#影响长期汇率的直接因素（实证数据验证）：" class="headerlink" title="影响长期汇率的直接因素（实证数据验证）："></a>影响长期汇率的直接因素（实证数据验证）：</h2><p>一个国家可贸易品的价格水平直接影响汇率。</p><p>可贸易品价格水平由什么决定（两个表层的经济因素）：</p><ol><li>技术水平（但只能解释20%左右的变化）</li><li>人口年龄结构，老龄化（能解释65%左右的变化）<ul><li>越老龄化，汇率越强。供给不变，需求下降，价格下降。（e.g.日本）</li></ul></li></ol><h2 id="影响汇率的六个深层因素"><a href="#影响汇率的六个深层因素" class="headerlink" title="影响汇率的六个深层因素"></a>影响汇率的六个深层因素</h2><p>（越高越强） </p><ol><li>国家能力：公共部门能力与效率问题</li><li>开放度：积极融入国际贸易（出口）<ol><li>能源大宗商品（以物为主）：明显顺周期特征 </li><li>制造业为主（以人为主）：逆周期特征明显</li></ol></li><li>要素特征<ul><li>人的核心竞争力（脑力/体力）</li><li>人还是物</li></ul></li><li>文明类型<ul><li>坚挺的文明有两大类<ul><li>新教文明</li><li>东亚文明：骨子里不信神（实用主义的神），努力证明自己的财富</li></ul></li><li>软的文明：<ul><li>南亚东南亚：小乘佛教，印度教，不鼓励生产也不鼓励消费（供给需求都萎缩）</li><li>天主教东正教伊斯兰教：不鼓励教徒寻求财富，鼓励寻求快乐（供给小于需求，则借债）</li></ul></li></ul></li><li>人群智商</li><li>宗教严肃度：询盘</li></ol><h2 id="海外投资的汇率风险"><a href="#海外投资的汇率风险" class="headerlink" title="海外投资的汇率风险"></a>海外投资的汇率风险</h2><p>哪些国家的汇率可能大规模贬值，详见深层因素。</p><h1 id="第三讲：为什么中国被动成为最大外汇储备国"><a href="#第三讲：为什么中国被动成为最大外汇储备国" class="headerlink" title="第三讲：为什么中国被动成为最大外汇储备国"></a>第三讲：为什么中国被动成为最大外汇储备国</h1><p>巨额的外汇储备不是大国的标志而是附庸国的象征。外汇储备源自对汇率波动的恐惧。发展中国家汇率波动和受到冲击非常大。</p><h2 id="中国的外汇储备是如何形成的"><a href="#中国的外汇储备是如何形成的" class="headerlink" title="中国的外汇储备是如何形成的"></a>中国的外汇储备是如何形成的</h2><p>为了快速工业化，招商引资，对人民币需求增多，有升值压力，但是又不能允许快速升值来保障制造业。于是中国人民银行使用外汇占款买入外汇，再由国家外汇管理局买其他国家国债。</p><p>对冲流动性（4万亿美金外汇储备30万亿人民币外汇占款，货币乘数为4的话就是120万亿广义货币，此时为了避免通胀），这些策略都是有成本的：</p><ul><li>发行央票</li><li>中央政府财政存款放在库底（利息低）</li><li>提高存款准备金率</li></ul><h2 id="央行拿外汇储备干什么去了"><a href="#央行拿外汇储备干什么去了" class="headerlink" title="央行拿外汇储备干什么去了"></a>央行拿外汇储备干什么去了</h2><p>买国债。中国外管局是美元的主要空头力量：</p><ul><li>猜想：外汇来自资本顺差和贸易顺差，还是要控制风险。</li><li>外管局将一部分美元换成其他货币进行投资降低风险。</li><li>以中国外汇储备来预测美欧元汇率</li></ul><h2 id="外汇储备无助于汇率稳定"><a href="#外汇储备无助于汇率稳定" class="headerlink" title="外汇储备无助于汇率稳定"></a>外汇储备无助于汇率稳定</h2><p>外汇储备是追求汇率稳定的结果。</p><p>持有巨额外汇（代价大，且并不安全）：</p><ul><li>巨量外汇占款，基础货币扩张，资产价格泡沫，央行对冲成本积累。</li><li>只能持有国债，低收益。高科技产品公司和资源无法买到。</li><li>并不能维护稳定例子：50亿美金做空巴西雷亚尔（5000亿美金储备）<ul><li>要不然汇率暴跌</li><li>要不然储备暴跌，通货紧缩，经济萧条</li><li>本国居民逃离</li></ul></li></ul><p>面对做空正确的策略：有序地贬值。</p><h2 id="中国不再需要外汇储备"><a href="#中国不再需要外汇储备" class="headerlink" title="中国不再需要外汇储备"></a>中国不再需要外汇储备</h2><p>不需外汇，只需要一些黄金。技术进步，产业升级，老龄化都可以减轻人民币汇率的贬值压力。我们需要担心的是汇率的长期强势。</p><h1 id="第四讲：政府意志与人民币汇率波动的历史"><a href="#第四讲：政府意志与人民币汇率波动的历史" class="headerlink" title="第四讲：政府意志与人民币汇率波动的历史"></a>第四讲：政府意志与人民币汇率波动的历史</h1><p>探讨政府在货币定价中扮演的角色及作用的方式方法。</p><h2 id="2005年前，汇率为出口导向型工业化提供支持"><a href="#2005年前，汇率为出口导向型工业化提供支持" class="headerlink" title="2005年前，汇率为出口导向型工业化提供支持"></a>2005年前，汇率为出口导向型工业化提供支持</h2><p>此时需要本币相对低估。以1:1.5（1980）到1:8.7（1994）。</p><p>见到实体产业迁移消长变迁的时候，要对以后几年潜在金融危机汇率动荡债务危机警惕。</p><p>1994-2005年，汇率相对稳定。亚洲金融危机时人民币拒绝贬值，导致工业化早期出现了出口困难。（用自身的经济困难换区地区稳定。）日本却因为主动贬值丧失了地区领导权。</p><h2 id="汇率主动温和升值是好事吗？"><a href="#汇率主动温和升值是好事吗？" class="headerlink" title="汇率主动温和升值是好事吗？"></a>汇率主动温和升值是好事吗？</h2><p>自主渐进可控的人民币升值背景：</p><ul><li>美国认为人民币低估，要求人民币升值。</li><li>人民银行如果要保持汇率，有通胀压力，要扩充外汇占款。</li><li>出口制造部门反对汇率升值。</li></ul><p>导致许多人搞套息交易。从香港搞廉价美元贷款，付给深圳公司，换成人民币，然后利差息差获利。巨量热钱流入中国。邀请全世界人来赌人民币升值，形成一定的泡沫，上证指数1000点到6000点。长期人为的扭曲付出了沉重代价。</p><p>后人视角给当时的决策者提建议：</p><ul><li>汇率波动走势具有随机性。</li><li>但是使月周级别不可预测。</li><li>减少热钱流动。</li></ul><h2 id="后金融危机时代，人民币汇率何去何从"><a href="#后金融危机时代，人民币汇率何去何从" class="headerlink" title="后金融危机时代，人民币汇率何去何从"></a>后金融危机时代，人民币汇率何去何从</h2><p>2008之后渐进升值，但是大大减缓。</p><ul><li>2014人民币被低估的程度不高了。2014-2016又贬值了4%左右（时间换空间，防止被做空）。</li><li>此时需要把热钱留出去。</li><li>外商持续投资减少，撤离中国（超国民待遇消失）。</li><li>2016不要对人民币贬值恐慌，认识到中国可贸易部门竞争力提升。2017人民币由市场升值</li><li>恐慌性升值，逼空2018贸易战，贬值预期</li><li>没有安抚市场，任由市场波动。</li><li>对美出口下降，但是人民币贬值，总出口稳定。2020-2021汇率明显上涨</li><li>新冠疫情，出口率先恢复。</li><li>CFETS汇率指数，与一篮子货币进行稳定化，与美国经济脱钩（换锚）。</li><li>汇率还是要基于经济基本面。</li></ul><h2 id="划重点"><a href="#划重点" class="headerlink" title="划重点"></a>划重点</h2><ol><li>东亚金融危机，中国赢得周边国家信任，为周边外交开拓了市场空间，日本丧失地区领导权。</li><li>汇率的渐进升值，把市场参与者的风险转移到了政府手里，市场调控应该注意。</li><li>政府塑造的中短期汇率及其波动方式的能力很强，但是长期人为扭曲将会付出沉重代价。</li><li>高明的汇率政策既要照顾实体经济的需要，又要充分考虑金融市场的内在规律。</li></ol><h1 id="第五讲：铸币税及其国际国内再分配效应"><a href="#第五讲：铸币税及其国际国内再分配效应" class="headerlink" title="第五讲：铸币税及其国际国内再分配效应"></a>第五讲：铸币税及其国际国内再分配效应</h1><h2 id="铸币税的前世今生"><a href="#铸币税的前世今生" class="headerlink" title="铸币税的前世今生"></a>铸币税的前世今生</h2><p>如今货币创造成本越来越低。</p><p>中国综合税率占GDP比例较低，但是如果把铸币税和土地财政纳入考量，则综合税率不再低。</p><h2 id="勤俭节约的美德“过时”了"><a href="#勤俭节约的美德“过时”了" class="headerlink" title="勤俭节约的美德“过时”了"></a>勤俭节约的美德“过时”了</h2><p>无锚货币时代，不应再痴迷储蓄：</p><ol><li>储蓄越多，向锚货币交的税越多。</li><li>2012年前，货币增速快，钱的购买力在不断降低。</li><li>无锚货币时代，用好杠杆做好投资。</li><li>中国人民存在海外的储蓄在不断地蒸发。</li><li>越难复制的资产，升值保值能力极高。学区房，高年份茅台，艺术品等的增值快于货币增发速度。</li></ol><h2 id="外汇占款带来的货币扩张帮助了谁？"><a href="#外汇占款带来的货币扩张帮助了谁？" class="headerlink" title="外汇占款带来的货币扩张帮助了谁？"></a>外汇占款带来的货币扩张帮助了谁？</h2><h3 id="剥夺了哪些阶层？补贴了哪些阶层？"><a href="#剥夺了哪些阶层？补贴了哪些阶层？" class="headerlink" title="剥夺了哪些阶层？补贴了哪些阶层？"></a>剥夺了哪些阶层？补贴了哪些阶层？</h3><p>出口部门获利，剥夺内地居民，补贴东南沿海。中央政府通过税收反哺内地是极其必要的。</p><p>中国央行从中国人民手中收取铸币税之后转手送给了美欧日央行，以此换取中国出口制造业的发展空间。为了扶持东南沿海企业家，才会有货币超发，才会有天量外汇储备。离开了政府提供的优质公共产品，这些企业家的表现不一定好。 2020年出口繁忙，人民币升值，出口企业不挣钱因为汇率变动。离开了政府保姆般的呵护，很多企业没本事挣到那么多钱。</p><p>很多先富起来的人将政府看成他们聚敛财富的负面因素，于是：</p><ul><li>2015看空人民币</li><li>投资移民走了</li></ul><p>外资企业也是明显的获益者群体：以折扣价获得土地。利用制造业服务业投资，通过土地获得资产升值的红利。</p><h3 id="中国底层消费力的重要变化"><a href="#中国底层消费力的重要变化" class="headerlink" title="中国底层消费力的重要变化"></a>中国底层消费力的重要变化</h3><p>外汇储备稳中有降，征收广义铸币税规模大大下降。金字塔底层的群体有了一定的消费能力。</p><h2 id="劫富济贫：美国梦的真相"><a href="#劫富济贫：美国梦的真相" class="headerlink" title="劫富济贫：美国梦的真相"></a>劫富济贫：美国梦的真相</h2><p>美国从全球征收了多少铸币税？</p><ul><li>资产负债表增量/GDP：较小</li><li>贸易逆差-海外投资金收益：通过财政收入产生数倍GDP</li></ul><ol><li>美联储扩表，剥夺全球储蓄者。<ul><li>0.9（2008前）=》2=》4.5（2014）=》缩表（2017初）=》扩表（2019.10）=》4.2（2020.2）=》无限量化宽松=》7.5（2020末）=》15（2025？推测）</li></ul></li><li>铸币税补贴了谁？<ul><li>小部分补贴了美国的穷人。</li><li>主要补贴了，美国的资本所有者，股市大涨。</li></ul></li></ol><p>所以要推动人民币国际化，征收全球铸币税，吸引全球储蓄者放进我们国债池。</p><h2 id="划重点-1"><a href="#划重点-1" class="headerlink" title="划重点"></a>划重点</h2><ol><li>2012年前，为了支持本国制造业国际竞争，用土地财政和铸币税补贴了出口产业，造成了不同资产收益率的巨大差异，也造成了不同地区发展水平和不同阶层收入水平的巨大差异，形成了强大的国内再分配效应。</li><li>美国的无限制量化宽松，伤害了美国底层和全球外围国家，但是令他最富有的人大大获益。</li><li>中国要通过人民币国际化获取一定的全球铸币税，保护自己的财富。</li></ol><h1 id="第六讲-人民币国际化的相关问题"><a href="#第六讲-人民币国际化的相关问题" class="headerlink" title="第六讲 人民币国际化的相关问题"></a>第六讲 人民币国际化的相关问题</h1><h2 id="不平衡的全球货币格局"><a href="#不平衡的全球货币格局" class="headerlink" title="不平衡的全球货币格局"></a>不平衡的全球货币格局</h2><p>美欧 GDP 占比42%左右，但是其货币却占83%。中日韩 GDP 占比27%左右，但是其货币却只占8%。</p><h2 id="人民币国际化的进展"><a href="#人民币国际化的进展" class="headerlink" title="人民币国际化的进展"></a>人民币国际化的进展</h2><p>2009年，周小川呼全球改革货币体系。人民币国际化提上日程。</p><p>人民币国际化指数在2010年左右只有0.02%，2020年已在4%左右。在贸易上和投资上增长比较快，储备较低。人民币面临的问题是国债池子太小。</p><p>随着中国高端制造业发展，人民币计价份额也会像日元一样上升。</p><h2 id="人民币国际化的改革努力"><a href="#人民币国际化的改革努力" class="headerlink" title="人民币国际化的改革努力"></a>人民币国际化的改革努力</h2><p>放松人民币跨境使用的管制。</p><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/rmb-courses/rmb-kuajing.png" class=""></div><p>政府的主动动作。</p><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/rmb-courses/rmb-active-1.png" class=""></div><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/rmb-courses/rmb-active-2.png" class=""></div><p>开放性大宗商品交易所，意味着全世界的资源出口国可以不必绕道美元进行交易。</p><ul><li>2015年前，人民币国际化路径和日元相似，提升本币在贸易结算份额，通过香港离岸市场发展日元海外市场。</li><li>2015年，香港市场人民币波动形成了人民币做空预期，中国政府不得不拉高隔夜拆借成本，抑制投机浪潮，意识到日本模式缺陷。</li><li>2015年后多管齐下，步步为营：<ul><li>人民币大宗商品交易</li><li>人民币对外直接投资</li><li>国债市场对外开放</li><li>上海金融中心建设</li><li>金融业外资准入放开</li><li>一带一路倡议</li><li>中资公司的跨国发展</li></ul></li><li>2020，稳慎推进人民币国际化。</li><li>未来：<ul><li>中央层面协调不同部门法规，让企业家乐于便于使用人民币</li><li>人民币上印全球通用语言</li><li>超大面额钞票，便于金融不发达地区</li><li>抛弃宏观债务率迷信，统一中国国债市场，增加交易活跃度和便利性</li><li>发行特别国债来置换人口流出区域地方政府的高息债务，节省利息成本</li><li>统计局统计公布数据时以人民币为单位，同时鼓励其他经济体使用人民币作为备用单位</li></ul></li></ul><h2 id="美国对人民币国际化的态度"><a href="#美国对人民币国际化的态度" class="headerlink" title="美国对人民币国际化的态度"></a>美国对人民币国际化的态度</h2><p>2010年之后最初几年，中国小心翼翼，“推动人民币跨境使用”。</p><ul><li>搞经济金融事务的人看好人民币国际化</li><li>政策和战略研究的人不看好</li><li>美国政策界乐于见到搞人民币国际化，但是认为人民币无法俘获信任，<ul><li>主权信用无法相信</li><li>必须放开货币汇率管制，实体经济可能受损</li></ul></li></ul><h2 id="数字货币带来人民币国际化弯道超车的机遇"><a href="#数字货币带来人民币国际化弯道超车的机遇" class="headerlink" title="数字货币带来人民币国际化弯道超车的机遇"></a>数字货币带来人民币国际化弯道超车的机遇</h2><p>中国推出数字人民币：在swift系统下，美国的制裁会造成挤兑破产。于是中国需要解决卡脖子问题。</p><p>猜想：</p><ul><li>如果能顺利推广到境外，可以农村包围城市，代替一部分美元现钞。</li><li>清晰坦率解释，取代地下流通的美元，提供资源帮助国家建设货币系统，实现货币现代化。</li><li>一带一路中，将数字人民币使用和援助相挂钩。</li><li>铸币税一部分好处恰当的分享給发展中国家人民</li></ul><h2 id="人民币国际化的前景及其影响"><a href="#人民币国际化的前景及其影响" class="headerlink" title="人民币国际化的前景及其影响"></a>人民币国际化的前景及其影响</h2><ol><li>货币网络效应对人民币国际化影响<ul><li>中心化网络导致强大货币存在兑换优势</li><li>人民币国际化需要先挤兑其他小币种的份额</li></ul></li><li>人民币国际化对中国经济的影响<ul><li>汇率水平上升，国际购买力翻倍</li><li>经济规模增大，本土消费市场会成为第一大市场</li></ul></li><li>人民币国际化代价<ul><li>汇率水平敏感的行业消亡</li></ul></li></ol><h2 id="划重点-2"><a href="#划重点-2" class="headerlink" title="划重点"></a>划重点</h2><ol><li>人民币国际化未来需要多管齐下，稳慎推进，久久为功。</li><li>全球货币市场存在赢家通吃的网络效应，人民币国际化在本世纪中期才会轮到挑战美元份额。</li><li>数字人民币将帮助人民币国际化弯道超车。 </li><li>人民币国际化会造成产业转移，这也是我国劳动力结构变化的内在要求，应该顺应这个市场规律而行。</li></ol><h1 id="实操课：全球货币体系知识如何转化为利润"><a href="#实操课：全球货币体系知识如何转化为利润" class="headerlink" title="实操课：全球货币体系知识如何转化为利润"></a>实操课：全球货币体系知识如何转化为利润</h1><h2 id="人民币汇率的预判及其分析方法"><a href="#人民币汇率的预判及其分析方法" class="headerlink" title="人民币汇率的预判及其分析方法"></a>人民币汇率的预判及其分析方法</h2><p>长期：中国快速迭代产业进步+老龄化趋势=坚定看多人民币</p><p>中期（3-5年）：中央政府政策导向以及与美国的关系。</p><p>短期（1年）：关注中国新生儿数量，数量下跌的话大胆做多人民币。如果政府鼓励生孩子且取得实际效果，可以适当看空一点人民币。</p><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/rmb-courses/birth-currency-rate.png" class=""></div><p>超短期（1-2月）：关注美元指数波动。</p><h2 id="美元在两种情况下会走强"><a href="#美元在两种情况下会走强" class="headerlink" title="美元在两种情况下会走强"></a>美元在两种情况下会走强</h2><p>两种情况：</p><ul><li>当美国率先与欧洲和东亚出现经济复苏和走出困境中。</li><li>当全球经济陷入大麻烦时。流动性躲到短期美元国债。</li></ul><p>如果美国出现政变，或者遭受核武器，或者资产泡沫破裂，导致股市暴跌，资产价格失序，那这个时候美元价格会涨还是会跌？其实是会大幅上涨。事情越大涨得越凶。次贷危机和新冠疫情为例。</p><h2 id="半球模型与全球资产配置的大周期理论"><a href="#半球模型与全球资产配置的大周期理论" class="headerlink" title="半球模型与全球资产配置的大周期理论"></a>半球模型与全球资产配置的大周期理论</h2><h3 id="半球模型（形似健身房物件）"><a href="#半球模型（形似健身房物件）" class="headerlink" title="半球模型（形似健身房物件）"></a>半球模型（形似健身房物件）</h3><p>可以用这个模型近似全球资本主义货币体系的构造。当一只脚踩下去，气体会像边缘扩散。</p><p>当中心出现减息，资金便宜了，资金向外围溢出，资本主义外围地区出现局部和暂时的繁荣。反过来的话，繁荣消失，金融危机。这只脚的踩踏和提起形成美元价格相对一篮子货币的波动（美元指数）。</p><p>过去50年，美元指数有着明确的周期，平均16-17年一次，下跌十年反弹六年。美元下行周期，可以从中心借入便宜资金追逐资本主义外围的高风险资产。美元上行周期，做空外围，把从外围挣到的钱连本带息还回中心，利润躲进无风险资产，美元短期国债。</p><h3 id="三位一体风险定价体系"><a href="#三位一体风险定价体系" class="headerlink" title="三位一体风险定价体系"></a>三位一体风险定价体系</h3><p>下列图中纵轴是资金的价格收益率risk</p><p>横轴是时间或者说债券的久期。连成一条线是美国国债收益率曲线。</p><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/rmb-courses/risk-time.png" class=""></div><p>横轴的单位从时间久期切换为地点，这个世界的中心外围体系，越中心资金价格越便宜。</p><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/rmb-courses/risk-location.png" class=""></div><p>横轴切换为资产类别。房地产风险最大收益率最大。</p><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/rmb-courses/risk-asset-type.png" class=""></div><h3 id="长周期资产配置蕴含较高收益率的模型"><a href="#长周期资产配置蕴含较高收益率的模型" class="headerlink" title="长周期资产配置蕴含较高收益率的模型"></a>长周期资产配置蕴含较高收益率的模型</h3><p>美元处于下行周期，把资金投入全球外围地区，在这一轮美元下行过程中最受益的工业化的经济体。指标上看哪一个国家的国际收支平衡表上堆积越来越多美元储备（热钱往哪里流）。所需做的去王公贵族居住的最最贵的房产和土地，然后不要动，操作周期在10年以上。当美元指数上涨的时候，所需做的是把当地房产卖掉，获得当地货币，卖掉换回美元，换回美国短期国债，只追求活下去。</p><h3 id="回溯过去50年的美元周期"><a href="#回溯过去50年的美元周期" class="headerlink" title="回溯过去50年的美元周期"></a>回溯过去50年的美元周期</h3><p>1970年中期跑到拉美，在布宜诺斯艾利斯、里约热内卢、墨西哥城，买入最贵的房地产。1980年代初，卖出当地房产和货币，回归短期国债。1980年代中，指数再次下行，做右肩交易（趋势确定），在东亚日韩港台新加坡买入最贵的房产。1990年代中，美元上涨，卖出当地房产和货币，回归短期国债。2001到03年开始，又大幅下行，在中国投资，北京四合院，上海小洋楼。2015年美元指数上涨，周而复始。</p><h2 id="那些外围国家的房地产值得投资"><a href="#那些外围国家的房地产值得投资" class="headerlink" title="那些外围国家的房地产值得投资"></a>那些外围国家的房地产值得投资</h2><p>问题切换成，那些外围国家在美元下行周期能成为工业化浪潮的赢家。</p><p>最理想的候选国家，应该具有以下条件：</p><ul><li>强政府，最好不要玩选举。民主制度是奢侈品，只有实现工业化才可以享受，否则劣质民主或者民主倒退。</li><li>外交采用务实的外交政策，与各个大国搞好关系（同时中美）。</li><li>经济上重商主义，鼓励出口，让自己家制造业越办越好。</li><li>人口劳动力素质比较好且有一定规模，人均智商偏高。</li><li>文化上最好不怎么信教，追求世俗的财富和成就。</li><li>地理上最好有一些值得开发的港口，因为海运成本较低，腐败与否不重要。</li></ul><p>如果找到一两个发展中国家符合一两个或者大部分条件，需要长期关注其政治变革和政策组合。这种国家一旦进入工业化轨道，办工厂的人未必能积攒巨大财富。这种工业化积攒的财富最终以各种形式流入当地统治阶级和王公贵族。导致统治阶级聚集的大城市资产价格大幅上涨。</p><h2 id="如何识别泡沫"><a href="#如何识别泡沫" class="headerlink" title="如何识别泡沫"></a>如何识别泡沫</h2><h3 id="反身性"><a href="#反身性" class="headerlink" title="反身性"></a>反身性</h3><p>索罗斯的操盘策略很多时候是在美元指数上涨周期做空外围泡沫比较严重的经济体。索罗斯是用主观认知和客观事实之间的循环联动性，寻找自我实现的预言，自我增强的趋势，利用反身性来识别泡沫。</p><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/rmb-courses/reflexive.png" class=""></div><h3 id="实体经济和虚拟经济之间的背离问题"><a href="#实体经济和虚拟经济之间的背离问题" class="headerlink" title="实体经济和虚拟经济之间的背离问题"></a>实体经济和虚拟经济之间的背离问题</h3><p>两个案例，注意关注类似情形，警惕危机：</p><ol><li>东南亚制造业在1990年遭到中国出口工业的竞争而被掏空，而房价股价持续走高。而汇率却铆住美元，结果产生金融泡沫。</li><li>21世纪头十年，中东欧地区经济体融入欧盟统一市场，抢夺原本属于南欧实体产业的市场空间，南欧货币融入欧元无法通过汇率调整反应经济基本面变迁，出现泡沫，欧债危机。</li></ol><p>为什么很多发展中国家喜欢搞固定汇率和联系汇率制度？出口制造行业低端，无法经受汇率波动，于是铆定主要的贸易对象。注意未来也会有国家与人民币挂钩。</p><h2 id="从发达工业国还是发展中穷国，哪个挣钱容易？"><a href="#从发达工业国还是发展中穷国，哪个挣钱容易？" class="headerlink" title="从发达工业国还是发展中穷国，哪个挣钱容易？"></a>从发达工业国还是发展中穷国，哪个挣钱容易？</h2><p>两种国家：</p><ul><li>发达国家钱多富裕但是人精明。中国人挣辛苦钱。</li><li>穷国穷，没什么钱，人单纯，知识匮乏，政策漏洞多，定价错误多。</li></ul><p>在发展中国家布局相对更容易。但是道德上不建议这样做。</p><h3 id="两种策略"><a href="#两种策略" class="headerlink" title="两种策略"></a>两种策略</h3><p>有一些发展中国家有大企业，具有国际性业务。例如能源、采矿、服务业制造业。他们有公允价值。在他们本币大幅贬值时，这些企业股票价格或迟或早等比例上涨。如果判断出本币贬值过程，及时进场加杠杆做多这些大企业。本币贬值会带来某些无风险收益。</p><h2 id="划重点-3"><a href="#划重点-3" class="headerlink" title="划重点"></a>划重点</h2><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/rmb-courses/important1.png" class=""></div><div style="width:60%;margin:auto"><img src="/economy-finance/didongsheng/rmb-courses/important2.png" class=""></div>]]></content>
    
    
    <summary type="html">&lt;!-- omit in toc --&gt;
&lt;p&gt;翟老师金融货币市场的课程。&lt;/p&gt;</summary>
    
    
    
    <category term="Economy &amp; Finance" scheme="https://superuier.github.io/categories/economy-finance/"/>
    
    
    <category term="翟东升" scheme="https://superuier.github.io/tags/%E7%BF%9F%E4%B8%9C%E5%8D%87/"/>
    
  </entry>
  
  <entry>
    <title>糖盐</title>
    <link href="https://superuier.github.io/knowledge-from-growth/sugar-salt/"/>
    <id>https://superuier.github.io/knowledge-from-growth/sugar-salt/</id>
    <published>2021-01-21T11:44:50.000Z</published>
    <updated>2021-01-21T11:44:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录一些内容并未不适合公开的糖盐。个人觉得还比较有趣，亦不免寓教于乐。</p><a id="more"></a><h2 id="Archive"><a href="#Archive" class="headerlink" title="Archive"></a>Archive</h2><ul><li>“虚无的充实感”<ul><li>描述忙活了半天却不知道做出什么进展但却心满意足的样子。</li><li>个人认为旨在期望你清楚地认识到自己的进步在哪里，或者说到底有没有进步。</li><li>eg. 埋头写了1000行代码，好充实的一天。</li></ul></li><li>“那你/我还不如去买彩票呢”<ul><li>描述一种期望可能性很小的状态，一般会有上一句作为铺垫。</li><li>个人认为旨在期望你认识到此事的荒谬。</li><li>eg. “那我要只期望我的学生XXX（此处为动词），那我还不如去买彩票呢。”</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;记录一些内容并未不适合公开的糖盐。
个人觉得还比较有趣，亦不免寓教于乐。&lt;/p&gt;</summary>
    
    
    
    <category term="Knowledge from Growth" scheme="https://superuier.github.io/categories/knowledge-from-growth/"/>
    
    
    <category term="感悟总结" scheme="https://superuier.github.io/tags/%E6%84%9F%E6%82%9F%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>A Survey of Deep Active Learning</title>
    <link href="https://superuier.github.io/paper-reading/deep-AL-survey/"/>
    <id>https://superuier.github.io/paper-reading/deep-AL-survey/</id>
    <published>2020-12-20T10:47:07.000Z</published>
    <updated>2020-12-20T10:47:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>记录一篇深度主动学习的调研文章。2020年，这篇 survey 被挂在了 arxiv 上， 见此<a href="https://arxiv.org/abs/2009.00236">链接</a>。此篇 survey 全30页，引用189篇参考文献。</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Background: A rich variety of related work has been published, DeepAL still lacks a unified classification framework.</p><p>Challenges of DeepAL:</p><ul><li>Insufficient data for label samples<ul><li>The labeled training samples provided by the classic AL are insufficient to support the training of traditional DL.</li><li>The non-batch sample query method commonly used in AL is not applicable in the DL context.</li></ul></li><li>Model uncertainty<ul><li>The softmax response of the final output is unreliable as a measure of confidence, and the performance of this method will thus be even worse than that of random sampling. DL model can be too confident about the output results.</li></ul></li><li>Processing pipeline inconsistency<ul><li>AL: fixed features + learned classifiers</li><li>DL: learned features + learned classifiers</li><li>Only fine-tuning the DL models in the AL framework, or treating them as two separate problems, may thus cause divergent issues.</li></ul></li></ul><h2 id="Approaches"><a href="#Approaches" class="headerlink" title="Approaches"></a>Approaches</h2><p>In general they summarized the corresponding approaches for the challenges.</p><ul><li>Insufficient data for label samples<ul><li>data augmentation</li><li>assigning pseudo-labels</li><li>combine supervised and semi-supervised training</li><li>batch sample</li></ul></li><li>Model uncertainty<ul><li>Bayesian deep learning</li></ul></li><li>Processing pipeline inconsistency<ul><li>Modify the combined framework of AL and DL to make the proposed DeepAL model as general as possible</li></ul></li></ul><h2 id="Query-strategies"><a href="#Query-strategies" class="headerlink" title="Query strategies"></a>Query strategies</h2><p>Batch Mode DAL should be satisfied.The reference indexes are copied.</p><ol><li>Uncertainty-based and hybrid query strategies<ul><li>directly use uncertainty sampling (non-batch) [9, 59, 114, 123]</li><li>query batch sample set representative to the distribution [177:Exploration-P, 183:DBAL, 99:WI-DL]<ul><li>the richer the category content of the dataset, the larger the batch size, and the better the effect of diversity-based methods;</li></ul></li><li>query batch sample set representative to the gradient embedded space [10:BADGE]</li><li>query batch sample set representative by adversarial method [146:WWAL, 150:VAAL, 81:TA-VAAL]</li></ul></li><li>Deep Bayesian Active Learning <ul><li>obtain the posterior distribution of network prediction [47:DBAL, 118:DEBAL, 28:DPEs, 114:ActiveLink]</li><li>obtain the mutual information between the batch samples and the model parameters [84: BatchBALD]</li><li>reconstructed the batch structure to optimize the sparse subset approximation [117:ACS-FW]</li><li>other [54, 105, 126, 147, 175, 179]</li></ul></li><li>Density-based Methods<ul><li>core-set approach [49:FF-Active, 138:core-set]</li><li>discriminative approach [35:Active Palmprint Recognition, 51:DAL]</li></ul></li><li>Other <ul><li>RL approach [37]</li><li>Adversarial examples[36:DFAL]</li><li>ensemble [14]</li><li>flexible acquisition function [57:RAL, 100:DRAL]</li><li>with NAS [50:Active-iNAS]</li></ul></li></ol><h2 id="Model-training"><a href="#Model-training" class="headerlink" title="Model training"></a>Model training</h2><p>Normally accompanied with insufficient data (under the view of DAL).</p><ol><li>assigning pseudo-labels [166:CEAL]</li><li>combine unsupervised feature learning [99:WI-DL]</li><li>data augmentation [187:GAAL, 162:BGADL, 107:ARAL]</li><li>add adversarial task [150:VAAL, 81:TA-VAAL]</li></ol><h2 id="DAL-on-the-generalization-of-the-model"><a href="#DAL-on-the-generalization-of-the-model" class="headerlink" title="DAL on the generalization of the model"></a>DAL on the generalization of the model</h2><ol><li>Use the final layer output to make query</li><li>ALso use the middle layer output to make query [59:AL-MV, 178:LLAL, 182]</li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;记录一篇深度主动学习的调研文章。
2020年，这篇 survey 被挂在了 arxiv 上， 见此&lt;a href=&quot;https://arxiv.org/abs/2009.00236&quot;&gt;链接&lt;/a&gt;。
此篇 survey 全30页，引用189篇参考文献。&lt;/p&gt;</summary>
    
    
    
    <category term="Paper Reading" scheme="https://superuier.github.io/categories/paper-reading/"/>
    
    
    <category term="active-learning" scheme="https://superuier.github.io/tags/active-learning/"/>
    
    <category term="survey" scheme="https://superuier.github.io/tags/survey/"/>
    
  </entry>
  
  <entry>
    <title>黑苹果使用记录</title>
    <link href="https://superuier.github.io/software-tools/macos/hackintosh/"/>
    <id>https://superuier.github.io/software-tools/macos/hackintosh/</id>
    <published>2020-12-20T07:47:07.000Z</published>
    <updated>2020-12-20T07:47:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>黑苹果使用记录，很多功能调教起来还是较为麻烦的。</p><a id="more"></a><h2 id="win-与-macOS-下显卡切换"><a href="#win-与-macOS-下显卡切换" class="headerlink" title="win 与 macOS 下显卡切换"></a>win 与 macOS 下显卡切换</h2><p>之前一直使用的是 RX470 显卡，由于最近玩2077实在是带不动，所以考虑换卡。由于最近狂潮，AMD 显卡大涨一波价，于是乎买A卡总觉得很不值。于是入手了索泰1080至尊Plus（1800rmb， 2020年12月）。</p><p>但是由于N卡不支持黑果，所以目前目标为</p><ul><li>黑苹果下把显卡独立显卡屏蔽并使用集成显卡 UHD630 作为显卡。</li><li>Windows 下正常使用1080。</li></ul><p>其实操作很简单，理论上只需要在 config 文件中加入<code>wegnoegpu</code>屏蔽外部显卡即可。</p><p>简述一下现在的设置情况：</p><ul><li>clover 的 config 下，添加 whatevergreen 驱动里的命令 <code>wegnoegpu</code>，便可正常进入 macOS。</li><li>主板 bios 设置为集显优先，则用集成显卡进入 clover。</li><li>主板插 DP 线，显卡插 HDMI 线（这个很重要且很玄学！我最开始是相反方式插着，结果无法进 macOS 系统。（显示器是有一个DP口））</li><li>windows 下进入会有两个信号源，集显和独显。在设备管理器中关闭集显不管用（集显的DP口仍有输出。）<ul><li>在显示设置中设置只显示特定桌面（只显示独显连接的桌面）</li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;黑苹果使用记录，很多功能调教起来还是较为麻烦的。&lt;/p&gt;</summary>
    
    
    
    <category term="Software Tools" scheme="https://superuier.github.io/categories/software-tools/"/>
    
    
    <category term="MacOS" scheme="https://superuier.github.io/tags/MacOS/"/>
    
    <category term="Hackintosh" scheme="https://superuier.github.io/tags/Hackintosh/"/>
    
  </entry>
  
</feed>
