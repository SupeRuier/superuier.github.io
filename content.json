{"pages":[],"posts":[{"title":"讲座及研讨班记录","text":"本篇Blog用于记录参加的讲座，不定期更新。 搜索引擎的技术趋势和精准度提高【常毅】 主讲人：常毅教授，吉林大学人工智能学院院长 日期：2020/11/23 1. Introduction搜索引擎架构： 网页爬虫 倒排索引（word &gt; document 拿空间换时间的一个过程） 网页检索&amp;网页排序（0.2s内） PageRank 算法： Google最先提出的一种网页排名算法。 大众一般误认为google精准是仅仅是因为PageRank这一项技术，事实上是因为其很多黑科技的结合。 搜索引擎进化史： 1994-1998 Syntactic matching关键字匹配 1998-2006 PageRank，外加利用指向信息，点击信息等 2006-2014 垂直搜索，知识图谱 2014～ mobile search，私人助手，聊天机器人等 搜索引擎后发劣势（追赶者很难超越）：用户数据积累较少导致效果较差。 深度学习在搜索引擎里的优势远远小于我们所期待的（至少在2016年）。 2. Web search ranking review排序问题的变换： pointwise：退化为回归问题，用gradient boosting类回归 pairwise：排错对数越来越少 listwise 3. Yahoo web search ranking practice简要介绍了2015年KDD best paper的工作。 这是一个系统性的工作，并不仅仅单一的提出了一个算法。 Practical challenges： avoid ugly result on the top Semantic gap between query and document Search queues follows a long tail distributions 3.1. 排序学习的算法2010 Yahoo learning rank challenge中前十名都是使用tree-based算法。 2010年得出的结果中lambdaMart &gt; logisticRank。 2015年KDD best paper中logisticRank则好于lambdaMart和2010年结果相悖。 根据2015年的结果总结得出造成这一现象的原因是以前的community都或多或少忽略了”over 99% query-url pairs are bad”这一现象。 3.2. 查询改写的方法机器翻译需要平行语料库，需要以用户反馈来学习查询语料对训练翻译模型。 举个例子： Tesla Price =&gt; How much is a Tesla 使用用户查询关键字与点击情况进行匹配。 3.3. Click similarity feature当前存在很多feature types，如图所示： Click similarity feature是一种特征提取的方法。 通过二分图对Query进行特征构建 比deep learning效果好。 在特征提取的问题上DL不会dominant，为什么？ Web search ranking is an ‘easy’ and well studies task -李航 DL更适用数据初识表示和解决问题的合适表示相距甚远时 -周志华 DL适用于信息complete的时候，WSR中这一条件并不总是成立。","link":"/computer-science-engineering/seminar-talk/"},{"title":"摸爬滚打","text":"增强对这个世界各种属性的认识。 Archive 人和人的体质不能一概而论。 eg. 本科的同学们发了很多顶会文章。 重要的个人利益不能放弃，如果可以放弃的话，自由一定会导致强者对弱者的剥削。这就是为什么我们的伦理道德认为一个人是不能处分自己最重要的利益的，因为自由不能以彻底放弃自由为代价。 from 罗翔 eg. 代孕 很多复杂问题是更高维度简单问题的投影，比如说打篮球动作变形、速度慢、配合差是很多时候体力不行；写程序烂、bug 多、时间长是抽象分解问题做的不好","link":"/knowledge-from-growth/mud-down/"},{"title":"自我提升","text":"一些平时总是注意不到但是应该要注意的东西。 阅读 阅读时要寻找作者的观点（能代表这一篇文章的东西），不要总是无重点的阅读，否则较难形成知识或经验。 表达方式 陈述观点或者描述事物时先简要说结论，再重新以背景、问题、分析、归纳的顺序铺展开来补充。 在打断对方并对自己观点加以补充时需要注意（这种情况一般是发生于你认为对方没有听明白）。 首先你并不清楚是否对方理解了你的观点，可能需要听完对方的表述。 如果对方理解，你加以打断，或是一种自大表现。 如果对方不理解，完全可以等他说完，再补充表述。 如果听完还是不清楚对方是否理解，则可以寻求对方复述一遍。大部分人可能不会介意重复一遍自己的理解，但是介意对方认为自己不理解对方的观点。 思维方式 避免想当然。 一点不要含糊，含糊代表着侥幸、代表着自我欺骗、代表着自我感觉飘然","link":"/knowledge-from-growth/self-improvement/"},{"title":"糖盐","text":"记录一些内容并未不适合公开的糖盐。 个人觉得还比较有趣，亦不免寓教于乐。 Archive “虚无的充实感” 描述忙活了半天却不知道做出什么进展但却心满意足的样子。 个人认为旨在期望你清楚地认识到自己的进步在哪里，或者说到底有没有进步。 eg. 埋头写了1000行代码，好充实的一天。 “那你/我还不如去买彩票呢” 描述一种期望可能性很小的状态，一般会有上一句作为铺垫。 个人认为旨在期望你认识到此事的荒谬。 eg. “那我要只期望我的学生XXX（此处为动词），那我还不如去买彩票呢。”","link":"/knowledge-from-growth/sugar-salt/"},{"title":"强化学习 &amp; 模仿学习基础知识","text":"总是会看到强化学习及模仿学习的内容，每次看完都会忘记，此处把之前的小笔记总结一下。 基本术语 Terminologies以下术语在强化学习和模仿学习中都经常见到。 agent: the intelligent individual environment: The agent is acting in an environment. state: Current condition. The agent can stay in one of many states of the environment action: The agent chooses to take one of many actions under the certain states. reward: Once an action is taken, the environment delivers a reward as feedback. policy: Agents’ behavior.(s =&gt; a) The agent’s policy π provides the guideline on what is the optimal action to take in a certain state with the goal to maximize the total rewards. value: (s =&gt; value) Each state is associated with a value function V(s) predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy. state-value of a state s is the expected return if we are in this state at time t. action-value (“Q-value”; Q as “Quality” I believe?) of a state-action pair is expected return if we are in this state at time t and take action a. A-value: The difference between action-value and state-value is the action advantage function (“A-value”): model: Transition and reward. (s,a =&gt; s’ &amp; r) How the environment reacts to certain actions (we may or may not know). 强化学习1. 马尔科夫决策过程In more formal terms, almost all the RL problems can be framed as Markov Decision Processes (MDPs). All states in MDP has “Markov” property, referring to the fact that the future only depends on the current state, not the history. The goal is to react on each state to maximize the total reward. 如果所有 MDP 成分都已知，我们便可以较容易的训练出来一个 agent。 但是现实情况是很多时候，我们的 agent 对 transition function $P$ 和 reward function $R$ 一无所知，所有的信息都来自于同环境的交互。 1.1. 强化学习方法分类 以是否对环境建模分类: Doesn’t model the environment: Model-free RL: Doesn’t need to know the transition function (“model”), neither the real function nor a learned function. Model the environment: Model-based RL: Need to know the transition function (“model”), either the real function or a learned function. Inverse reinforcement learning: Need to learn a value function for a state. (Imitation learning) 以行动策略和评估策略是否相同分类： On-policy: Use the deterministic outcomes or samples from the target policy to train the algorithm. 行动策略和评估策略相同 Off-policy: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy. 行动策略和评估策略不同 1.2. 对价值函数进行评估和分解强化学习的目标是可以使最终价值最大化，所以需要对其进行评估。 Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values. \\begin{aligned} V_{\\pi}(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s) Q_{\\pi}(s, a) \\\\ Q_{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_{\\pi} (s') \\\\ V_{\\pi}(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s) \\big( R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_{\\pi} (s') \\big) \\\\ Q_{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\vert s') Q_{\\pi} (s', a') \\end{aligned}2. Common Approaches 2.1. Dynamic ProgrammingWhen the model is fully known, following Bellman equations, we can use Dynamic Programming (DP) to iteratively evaluate value functions and improve policy. The policy would greedy based on the Q-value. Iteratively update the state value and the Q-value. Generalized Policy Iteration (GPI) adaptive dynamic process \\pi_0 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_0} \\xrightarrow[]{\\text{improve}} \\pi_1 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_1} \\xrightarrow[]{\\text{improve}} \\pi_2 \\xrightarrow[]{\\text{evaluation}} \\dots \\xrightarrow[]{\\text{improve}} \\pi_* \\xrightarrow[]{\\text{evaluation}} V_*2.2. Monte-Carlo MethodsModel-free method. It learns from episodes of raw experience without modeling the environmental dynamics and computes the observed mean return as an approximation of the expected return 2.3. Temporal-Difference LearningTD Learning is model-free and learns from episodes of experience. However, TD learning can learn from incomplete episodes and hence we don’t need to track the episode up to termination. 主要思想是将效用估计朝着理想均衡方向调整: TD调整一个状态与已观察到的后继状态相一致 ADP调整一个状态与可能出现的的后继状态相一致 TD可视为对ADP的一个粗略有效的一阶近似 \\begin{aligned} V(S_t) &\\leftarrow (1- \\alpha) V(S_t) + \\alpha G_t \\\\ V(S_t) &\\leftarrow V(S_t) + \\alpha (G_t - V(S_t)) \\\\ V(S_t) &\\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)) \\\\ Q(S_t, A_t) &\\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) \\end{aligned}To learn optimal policy: SARSA: On-Policy TD control “SARSA” refers to the procedure for updating the Q-value. Same routine of GPI. In each step of SARSA, we need to choose the next action according to the current policy. Q-Learning: Off-policy TD control The key difference from SARSA is that Q-learning does not follow the current policy to pick the second action (off-policy). Q-learning may suffer from instability and divergence when combined with an nonlinear Q-value function approximation Deep Q-Network It quickly becomes computationally infeasible to memorize Q-table when the state and action space are large. Use functions (i.e. a machine learning model) to approximate Q values and this is called function approximation. greatly improve and stabilize the training procedure of Q-learning by two innovative mechanisms: Experience Replay: improves data efficiency, removes correlations in the observation sequences, and smooths over changes in the data distribution. Periodically Updated Target: only periodically updated, overcomes the short-term oscillations 2.4. Combining TD and MC LearningIn TD learning, we only trace one step further down the action chain when calculating the TD target. One can easily extend it to take multiple steps to estimate the return. 2.5. Policy GradientAll the methods we have introduced above aim to learn the state/action value function and then to select actions accordingly. Policy Gradient methods instead learn the policy directly with a parameterized function. Measure the quality of a policy with the policy score function. Use policy gradient ascent to find the best parameter that improves the policy. 2.6. Asynchronous Advantage Actor-Critic (A3C) Asynchronous: Several agents are trained in it’s own copy of the environment and the model form these agent’s are gathered in a master agent. The reason behind this idea, is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse. Advantage: Similarly to PG where the update rule used the dicounted returns from a set of experiences in order to tell the agent which actions were “good” or “bad”. Actor-critic: combines the benefits of both approaches from policy-iteration method as PG and value-iteration method as Q-learning (See below). The network will estimate both a value function V(s) (how good a certain state is to be in) and a policy π(s). Agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods. 3. Known Problems Exploration-Exploitation Dilemma Deadly Triad Issue: off-policy, nonlinear function approximation, and bootstrapping are combined in one RL algorithm, the training could be unstable and hard to converge. 模仿学习1. 背景Background: Given: demonstrations or demonstrator Goal: train a policy to mimic demonstrations Intuition: 人们并不总是知道执行某项任务所获得的报酬 但是人们可能会知道“做什么是正确的事情（最佳策略） Rollout: sequentially execute $\\pi(s_0)$ on an initial state Produce trajectory $\\mathcal{T}=(s_0,a_0,s_1,a_1,…)$ 2. 模仿学习分类 Behavior cloning Direct policy learning (multiple step BC) Inverse reinforcement learning (assume learning R is statistically easier) 2.1. Behavioral Cloning (simplest Imitation Learning setting)Treat experts’ states-actions pairs i.i.d and as training example use supervised learning (from state to action). Distribution provided exogenously. When to use BC? 2.2. Direct Policy LearningLearning reduction: Reduce “harder” learning problem to “easier” one Idea: Construct a sequence of distributions or sequence of supervised learning problems. Query interactive oracle about the state and construct a loss function according to our action and expert action on this state. 2.3. Inverse reinforcement learningInverse RL指我们需要对环境的reward进行建模。 RL与IRL的对比如下图所示： In a traditional RL setting, the goal is to learn a decision process to produce behavior that maximizes some predefined reward function. Inverse reinforcement learning (IRL), flips the problem and instead attempts to extract the reward function from the observed behavior of an agent. IRL seeks the reward functions that ‘explains’ the demonstrations. 此时同样存在是否依赖 transition function 的情况 References https://en.wikipedia.org/wiki/Reinforcement_learning https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html 人工智能：一种现代的方法 https://sites.google.com/view/icml2018-imitation-learning/","link":"/machine-learning/reinforcement-learning/"},{"title":"A Survey of Deep Active Learning","text":"记录一篇深度主动学习的调研文章。 2020年，这篇 survey 被挂在了 arxiv 上， 见此链接。 此篇 survey 全30页，引用189篇参考文献。 IntroductionBackground: A rich variety of related work has been published, DeepAL still lacks a unified classification framework. Challenges of DeepAL: Insufficient data for label samples The labeled training samples provided by the classic AL are insufficient to support the training of traditional DL. The non-batch sample query method commonly used in AL is not applicable in the DL context. Model uncertainty The softmax response of the final output is unreliable as a measure of confidence, and the performance of this method will thus be even worse than that of random sampling. DL model can be too confident about the output results. Processing pipeline inconsistency AL: fixed features + learned classifiers DL: learned features + learned classifiers Only fine-tuning the DL models in the AL framework, or treating them as two separate problems, may thus cause divergent issues. ApproachesIn general they summarized the corresponding approaches for the challenges. Insufficient data for label samples data augmentation assigning pseudo-labels combine supervised and semi-supervised training batch sample Model uncertainty Bayesian deep learning Processing pipeline inconsistency Modify the combined framework of AL and DL to make the proposed DeepAL model as general as possible Query strategiesBatch Mode DAL should be satisfied. The reference indexes are copied. Uncertainty-based and hybrid query strategies directly use uncertainty sampling (non-batch) [9, 59, 114, 123] query batch sample set representative to the distribution [177:Exploration-P, 183:DBAL, 99:WI-DL] the richer the category content of the dataset, the larger the batch size, and the better the effect of diversity-based methods; query batch sample set representative to the gradient embedded space [10:BADGE] query batch sample set representative by adversarial method [146:WWAL, 150:VAAL, 81:TA-VAAL] Deep Bayesian Active Learning obtain the posterior distribution of network prediction [47:DBAL, 118:DEBAL, 28:DPEs, 114:ActiveLink] obtain the mutual information between the batch samples and the model parameters [84: BatchBALD] reconstructed the batch structure to optimize the sparse subset approximation [117:ACS-FW] other [54, 105, 126, 147, 175, 179] Density-based Methods core-set approach [49:FF-Active, 138:core-set] discriminative approach [35:Active Palmprint Recognition, 51:DAL] Other RL approach [37] Adversarial examples[36:DFAL] ensemble [14] flexible acquisition function [57:RAL, 100:DRAL] with NAS [50:Active-iNAS] Model trainingNormally accompanied with insufficient data (under the view of DAL). assigning pseudo-labels [166:CEAL] combine unsupervised feature learning [99:WI-DL] data augmentation [187:GAAL, 162:BGADL, 107:ARAL] add adversarial task [150:VAAL, 81:TA-VAAL] DAL on the generalization of the model Use the final layer output to make query ALso use the middle layer output to make query [59:AL-MV, 178:LLAL, 182]","link":"/paper-reading/deep-AL-survey/"},{"title":"Cuda 使用记录","text":"在服务器上使用cuda遇到一些问题，在此记录。 在 torch.cuda.is_available() 返回 False最主要的问题是在 python 中输入torch.cuda.is_available()时，返回 False。 猜测应该是某种版本不匹配造成的问题。 于是先查询版本。 通过nvidia-smi 查询到的版本为 CUDA Version: 11.0， 和我安装的pytorch对应的cuda10.0不兼容。 于是下载对应版本即可解决，注意与 python 发行版的冲突。(下载贼慢) 1conda install pytorch torchvision cudatoolkit=11.0 -c pytorch 查询版本查询版本有三种方法。 1nvidia-smi 这个命令既可以查cuda的驱动API版本，也可以查看GPU运行状态。 查询到的版本为： NVIDIA-SMI 450.57 Driver Version: 450.57 CUDA Version: 11.0。最终需要匹配的版本以此命令为准。 1cat /usr/local/cuda/version.txt CUDA Version 10.0.130 1nvcc --version Cuda compilation tools, release 10.0, V10.0.130 有时会显示command not found，解决方法见此。 nvcc —version command not found首先查看 1ls /usr/local/cuda/bin 存在nvcc命令，此时配置环境变量即可。 .bashrc12export PATH=/usr/local/cuda/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH 然后更新配置文件。 source ~/.bashrc","link":"/programming/cuda/"},{"title":"Docker","text":"Docker 使用过程中的一点记录。大部分内容来源于此教程。 简单介绍Docker 包括三个基本概念: 镜像（Image）：Docker 镜像（Image），就相当于是一个 root 文件系统。比如官方镜像 ubuntu:16.04 就包含了完整的一套 Ubuntu16.04 最小系统的 root 文件系统。 容器（Container）：镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的类和实例一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 仓库（Repository）：仓库可看成一个代码控制中心，用来保存镜像。 Docker 容器通过 Docker 镜像来创建。 容器与镜像的关系类似于面向对象编程中的对象与类。 基础命令Docker run 可以在容器内运行一个应用程序。 1$ docker run ubuntu /bin/echo &quot;Hello world&quot; 以上命令完整的意思可以解释为：Docker 以 ubuntu15.10 镜像创建一个新容器，然后在容器里执行 bin/echo “Hello world”，然后输出结果。 12$ docker images #检查有哪些镜像$ docker ps –a #检查有哪些容器 12345678910$ docker run -it ubuntu /bin/bash #运行交互式容器$ docker run -itd ubuntu /bin/bash #后台启动容器$ exit (ctrl+D) #容器内退出容器$ docker stop [id/name] #停止容器（在容器外）$ docker start [id/name] 启用停止的容器$ docker restart [id/name] #重启容器$ docker exec -it [id/name] #进入容器$ docker attach [id/name] #进入容器，退出终端导致容器停止$ docker rm [id/name] #删除容器$ docker rmi [id/name] #删除镜像 创建镜像更新镜像当我们从 docker 镜像仓库中下载的镜像不能满足我们的需求时，需要对镜像进行更改。 一般有两种方法： 从已经创建的容器中更新镜像，并且提交这个镜像 使用 Dockerfile 指令来创建一个新的镜像 此处我们只考虑更新镜像。 123456$ docker run -it ubuntu /bin/bash # Open a container$ /# # Do modification$ exit # exit container# Create image$ docker commit -m=&quot;has update&quot; -a=&quot;user&quot; container_id image_name 镜像备份1docker save -o image_name.tar image_name 执行后，运行 ls 命令即可看到打成的 tar 包 镜像恢复与迁移首先我们先删除掉 image_name 镜像，然后执行以下命令进行恢复。 1docker load -i image_name.tar 执行后再次查看镜像，可以看到镜像已经恢复。","link":"/programming/docker/"},{"title":"Linux 常用命令","text":"常用到的 linux 命令语句 功能进程相关使用 ps 使得命令查询任务。 12# user's running processesps r -ef | grep username 终止进程 1kill PID 高级终止 1234# 将用户colin115下的所有进程名以av_开头的进程终止:ps -u colin115 | awk '/av_/ {print &quot;kill -9 &quot; $1}' | sh# 将用户colin115下所有进程名中包含HOST的进程终止ps -fe| grep colin115 | grep HOST |awk '{print $2}' | xargs kill -9; 后台运行使用 nohup 使得命令后台运行，同时断网和关闭终端都不会终止任务，适合用来作为任务提交方式。 1nohup /root/start.sh &amp; 此外，python 中自带的 subprocess.popen() 方法同样可以起到后台运行命令且关闭终端不退出的作用，所以不必要使用 nohup。","link":"/programming/linux-commands/"},{"title":"Miniconda","text":"Miniconda 与 Anaconda 为 conda 的发行版，主要用于包管理，其中 miniconda 更轻量级。 由于日常使用总是会忘记，所以此处记录一些常用的命令。 安装首先从清华源下载安装包并安装。 123wget -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda2-4.5.11-Linux-x86_64.shbash Miniconda2-4.5.11-Linux-x86_64.shsource ~/.bashrc Conda 默认的软件源在国外,速度非常的慢,我们可以将其更换为清华源。 可以直接在.condarc中添加 .condarc1234- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/- https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/- https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ 之后需要使用 conda clean -i 清除索引缓存，保证用的是镜像站提供的索引。] 注意有的时候会显示 url 错误， 此时换成 https 可能会解决问题。 配置创建环境创建所需要环境 1conda create -n py3.8 python=3.8 激活环境 1conda activate py3.8 然后下包 1conda install XXX 退出环境 1conda deactivate py3.8 批量导入导出组件导出 1conda list -e &gt; requirements.txt 导入有两种方式 12conda create --name py3.8 --file requirements.txt # 顺便创建环境conda install --yes --file requirements.txt","link":"/programming/miniconda/"},{"title":"Python","text":"常用到但是总忘记的的 Python 知识/语句 功能格式化字符串12345678910111213141516171819202122232425## Basics ## use &quot;·&quot; to visualize whitespace&quot;{} {}&quot;.format(1, 2) ## &quot;1·2&quot;f&quot;{1} {2}&quot; ## &quot;1·2&quot;## Padding and alignmenta = &quot;test&quot;f&quot;{a:10}&quot; ## &quot;test······&quot;f&quot;{a:&lt;10}&quot; ## &quot;test······&quot;f&quot;{a:&gt;10}&quot; ## &quot;······test&quot;f&quot;{a:^10}&quot; ## &quot;···test···&quot;f&quot;{a:_&lt;10}&quot; ## &quot;test______&quot;f&quot;{a!s}&quot; ## equals to f&quot;{str(a)}&quot;f&quot;{a!r:10}&quot; ## f&quot;{repr(a):10}&quot;## Floatsb = 0.5f&quot;{b:5}&quot; ## &quot;··0.5&quot; !!!f&quot;{b:&lt;5}&quot; ## &quot;0.5··&quot;f&quot;{b!s:5}&quot; ## &quot;0.5··&quot;f&quot;{b:05}&quot; ## &quot;000.5&quot;f&quot;{b:.3f}&quot; ## &quot;0.500&quot;f&quot;{b:.3e}&quot; ## &quot;5.000e-01&quot;f&quot;{b:.2%}&quot; ## &quot;50.00%&quot; Copy from Yu’s blog. 将 Traceback 信息保存到 log 文件中有时候要跑一个运行时间很长的文件，中间会创建 log 文件。 如果跑到一半报错终止，文件中却没有相应记录则很难追踪错误。 所以需要加入报错信息。 1234567import tracebacktry: main() # The code need to be executed.except Exception as e: logger.error(f&quot;Main program error: {e}&quot;) logger.error(traceback.format_exc())","link":"/programming/python/"},{"title":"Pytorch 踩坑","text":"使用 Pytorch 时学到的一些知识 1. 用法1.1. 随机种子 在导入文件之前，先导入与随机种子相关的包，这样导入的文件随机数也被确定。 在文件的开头添加以下代码： 1234567891011def seed_torch(seed=1029): random.seed(seed) os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现 np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) # if you are using multi-GPU. torch.backends.cudnn.benchmark = False torch.backends.cudnn.deterministic = Trueseed_torch() 1.2. zero_grad optimizer or net？ model.zero_grad() and optimizer.zero_grad() are the same IF all your model parameters are in that optimizer. It is safer to call model.zero_grad() to make sure all grads are zero. e.g. if you have two or more optimizers for one model. 1.3. 初始化网络网络参数初始化会对模型表现产生影响，一般通过一些随机的方式初始化参数。具体的影响可以见这篇博文。 具体如何实现网络权重初始化，可以通过对模型每一层遍历赋值实现，参见如下代码。 12params = list(net.parameters())torch.nn.init.xavier_uniform(layer) for layer in params 1.4. nn.module 中 __call__ vs forward call 方法中调用了 forward 函数，区别主要在于如果使用 forward 函数来进行前向传播，则无法使用 pytorch 提供的 hook 功能。 1.5. NLLLoss &amp; CrossEntropyLoss从文档中： This CrossEntropyLoss criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. 可以简单理解为： Softmax + CrossEntropyLoss == LogSoftmax + NLLLoss 那我们为什么要用 LogSoftmax 呢？ 因为在实现上，算log值更加便捷，如果直接计算指数值，可能会出现极大或者极其接近0的情况。 所以使用 LogSoftmax 的话数值稳定性可能会更好。 参考此链接。 1.6. tensor 非 contiguous 导致无法使用 view()当使用 tensor 操作时，新建了一份 tensor 元信息，并重新制定 stride，导致其不连续，无法使用 view()。 最简单的解决方法是使用tensor.contiguous(), 此时会重新开辟一块内存储存底层数据。 若不介意底层数据是否使用了新的内存，用reshape()则更方便。 这篇文章提供了一个非常完善的解释。 1.7. pytorch 中 hook 的使用Pytorch 中的 hook 为我们提供了一个较为方便的方式来访问网络某一层的输入与输出（前向的话返回 feature，反向的话返回梯度。） 具体的使用方法，首先要在相应的层上打开前向或者反向的 hook： 1234567891011121314151617181920212223242526272829# forward# 定义 forward hook functiondef hook_fn_forward(module, input, output): print(module) # 用于区分模块 print('input', input) # 首先打印出来 print('output', output) total_feat_out.append(output) # 然后分别存入全局 list 中 total_feat_in.append(input)# Add hook on the layer you wantmodules = model.named_children() # for name, module in modules: module.register_forward_hook(hook_fn_forward)################################################ backwarddef hook_fn_backward(module, grad_input, grad_output): print(module) # 为了区分模块 # 为了符合反向传播的顺序，我们先打印 grad_output print('grad_output', grad_output) # 再打印 grad_input print('grad_input', grad_input) # 保存到全局变量 total_grad_in.append(grad_input) total_grad_out.append(grad_output)modules = model.named_children()for name, module in modules: module.register_backward_hook(hook_fn_backward) 注意 register 函数接受的是一个函数，会为传入的函数传递三个参数 module， grad_input， grad_output。 这里的 input 和 output 都是以前向网络的方向来进行标记的。 反向传播中对于线性模块：o=W*x+b ，它的输入端包括了W、x 和 b 三部分，因此 grad_input 就是一个包含三个元素的 tuple。 而在 forward hook 中，input 是 x，而不包括 W 和 b。 详见这篇非常好的讲解。 1.8. 查看某一层梯度hook 是一种提取梯度的方法，同样的，还有其他方法可以提取梯度。 12# 一个全链接层举例list(model.modules())[5].weight.grad 1.9. 计算某一层梯度其实如果使用 loss.backward() 然后再利用 hook来提取梯度会有一些耗费时间，因为反向传播是要从尾到头的，如果你只需要倒数几层的梯度的话，其实可以直接计算。 torch.autograd.grad 方法提供了一个计算梯度的方式，可以看以下例子，此方法返回的对象是一个元祖。 123# 计算梯度# 如果需要多次计算的话记得保留计算图grad= torch.autograd.grad(outputs=loss, inputs=W, retain_graph=True, only_inputs=True)[0] 1.10. 计算梯度的时间在我的实验终有一个计算每一样本对梯度贡献的需求，有两种方法计算： 将 batch_size 设为1，然后使用 torch.autograd.grad 计算梯度。 用非1的 batch_size，计算 loss 时，不 reduce，这样得出来的 loss 是一个向量。遍历这个向量，对向量中每一个tensor使用 torch.autograd.grad 计算梯度。 但是发现一个问题。 在 batch 下，平均每个样本的前向时间是要远小于不使用 batch。 但是平均每个样本的后向时间是要远大于不使用 batch。 （这里远小远大是指数量级）。 推测原因为如果遍历向量的话的话，获得的 tensor 中 grad_fn 是 UnbindBackward 而不是 nlllossbackward。 所以尝试在计算 loss 之前就对样本进行遍历，但是其实时间上和遍历 loss 是一样的。 因为是用 loss 计算梯度是要使用之前的计算图，遍历网络输出会使遍历的每一个输出的 grad_fn 变化。 用这种方式虽然看起来 loss 的 grad_fn 还是 nlllossbackward，但是在梯度的计算过程中还是会遇到 UnbindBackward。 所以这个问题没有想到具体的解决方法，就选取了耗费时间相对较短的方法。 2. 设置2.1. Dataloader 中的 num_workers 造成训练循环缓慢在本地跑实验，一个简单的网络的训练，发现 Dataloader 中 num_workers 设置的数目越大，在 batch 中训练越耗时，表示莫名其妙。在我的情形下将其设为8要比将其设为0慢了百倍以上。 仔细看了一下 mini-batch 的训练过程并且记录了一下时间，发现主要的时间开销发生于 for 循环遍历 loader 之后退出循环时。 所还还是将其设为了0。 造成这个的主要原因可能是 IO 耗时和模型前/后传耗时之间的 GAP 太大，导致进程间造成了阻塞，详见这篇文章。 3. 报错3.1. RuntimeError: CUDA error: device-side assert triggered参考此篇文章。 一般来说这个错误出现的原因是数据中的类标记label和网络中的类标记label不匹配。包括但不限于以下几种问题。 pytorch识别的类别 数据中的类别 [0,1,2,3] [1,2,3,4] [0,1] [0,1,2,3] 解决方法只要找到矛盾发生的地方，对数据中类别的标签进行改动即可。当然有的时候也可能是网络格式写错。 3.2. RuntimeError: CUDA out of memory起因在于丢了49000张 mnist 数据进去没有分 batch，本来以为数据的大小只占了450m内存应该不会有问题，但是发现跑了一个前向就加了七八个g的显存，甚至一个模型直接把24g的显卡显存跑炸了。 分析原因应该是因为 batch size 较大的时候，前向输入模型，在某一层计算时申请了很大的 tensor 导致消耗了成倍与数据大小的显存。 这个在小 batch 的情况下应该并不会有太大影响，所以说还是需要使用 batch。 当然还是可以在需要的时候释放缓存，治标不治本。 1torch.cuda.empty_cache() 这篇文章简要介绍了 pytorch 的缓存机制。","link":"/programming/pytorch/"},{"title":"亲密关系","text":"《亲密关系》 克里斯多福·孟 极简简介这是一本介绍亲密关系的书，两性关系为主（relationship）。 其重点讲的是从作者角度出发亲密关系中不愉快的成因及少数他认为的解决办法。 个人看法间断地浏览了一遍这本书，或许遗漏了重点，此处仅以自己印象来进行评价。 可能有些地方记错记漏，但是无所谓，不喜勿喷。 以下所有仅从个人角度出发。 极简评价并不是一本可以学到知识的书，但是如果比较善于推广反思，倒是可能会有一些收获。 简单理性人寻求知识不建议阅读。 如果对自己的感情状况毫无头绪无从下手可以适当阅读。 缺点（喜爱挑毛病） 自己造定义，但是讲述又不严谨。 所以这本书不能以工具书的角度来看，但也不能以闲书的角度来看，我就因为总忘了定义导致可能理解有出入。 几乎把所有的问题归因到了原生家庭和小时候的痛苦。 不失有一定道理，但是看了整本书都是这么一套 cliche ，便觉得有点点肤浅了。 很多问题都是由他自己的经验出发举例说明。或者说讲了很多有的没的的故事。 举例论述其实并不很有说服力，且不具有覆盖性。 给人一种看起来这个人怎么这么善于反思这么厉害的样子。（成功学大师的影子） 很多说得很绝对的话，虽然可能有一定道理，但是明明是 opinion 却以一种 fact 的样子讲出来，让人不舒服。 举例：“我们所看到的每件事，其实都是我们内心的投射，我们怎么评论别人就是我们怎么看待自己。” 其实没有提到有效的解决方案。 只是很乐观的提沟通，提去爱，神神叨叨的讲什么灵魂真理。 他其实提倡对伴侣没有期望。 Ridiculous 感想/看法 这个作者每次讲观点总是说的不是很一致（说的不清楚，可能他自己也没搞清楚），而且最后也总是引到奇奇怪怪的结论。所以我此处给他的观点总结一下，这也可能是这个作者唠唠叨叨写了一本书想讲的东西。反正我总结完觉得还是有点道理。 其实很多亲密关系中面临的问题都只是导火索，究其根本，悲伤失落或者生气的原因都不是导火索看起来的那样子。【背景】 这件事其实很多人意识不到，以为只是导火索这件事的问题（或者说很多过去的没点燃的导火索的问题）。【背景】 稍微深一点的原因在于不同的人生活上需求可能不同（eg. 不同的生活习惯）。【过度】 更深层的原因在于心理上的需求，这一点不同人可能是相同的（eg. 都希望被尊重）。【原因】 对于心理上的需求可沟通解决。【解决】 其实面对很多问题的时候，跳出事件本身，客观的想一想自己要什么可能会好一些。 我不认为要无期望或者低期望。 我觉得人与人之间的相处总是以期望为前提的，无期望这件事太理想化了。 但是对人的期望最好同时也是对自己的期望。比如你期望伴侣让你快乐，为什么不可以自己快乐呢？当自己内心强大的时候，对别人期望的落空其实并没有那么难以接受。 句段摘录可能是一些我觉得可能有点道理的话。 最悲哀的是，在得到满足前，我们不愿意去爱自己的伴侣，紧抱着需求不放手，又不让自己去爱。 我们真正需要的，没有人能给。 如果对别人取悦我们的能力抱有太大的期望，那么失望便会是必然的结果。 不满意时，问问自己此时此刻希望从伴侣身上得到什么。 愿不愿意放弃这项期望，是由自己满足还是由伴侣满足。 我们宁愿争吵也不愿面对伤口，是因为生气比承受心碎简单的多。 过去的创伤并不会随着时间逝去。每个自我局限的信念，都来自于过去的创伤。 找出这些信念在我们心中驻足的所在，将会很有帮助。 亲密关系让我们有机会面对并治好旧伤，从而改变衍生自伤痛的错误想法。 当被卷进权力斗争的漩涡时，你一定要切记，我生气的原因不是我自己想的那回事。 对伴侣发怒的原因 麻痹自身心中的痛 是对方有罪恶感，有效控制对方的行为 如果无力掌控大局又不想感觉能力不足，或没安全感，最快的解决方法就是证明自己是对的。 如果你相信对亲密关系，你只需要付一半的责任，那么，即使你能付出百分之百，实际付出的，却只有50%。既然你所看到的一切都是你内心的投射。你就会发现伴侣，也只是付出50%。这样一来，你们两人都会坚持自己已经做了自己该做的那一份，却指责对方不肯尽全力。 只有当你愿意为发生在自己身上的事情完全负责的时候，你才能得到选择的力量。 如果你为了伴侣牺牲，那么你就会把对方看成是利用你的人，因为他们没有尽他们应尽的力量。 虽然弥补伴侣的放纵不是你的责任，但对方的放纵行为的确是你的责任。这句话的意思是你有能力对伴侣的行为作出响应，而不是采取牺牲的方式。 我们所看到的每件事，其实都是我们内心的投射，我们怎么评论别人就是我们怎么看待自己。","link":"/reading-note/intimate-relationship/"},{"title":"Concerns for Academic Writing","text":"A note about the precautions in academic writing. Try to avoid Avoid and eliminate the fuzziness and subjectivity of language to the greatest extent. Avoid to use “we”, “I think”, etc. Try to do Make the definitions clear. Present the results in a legible way. Include both tables and figures.","link":"/research/academic-paper-writing/"},{"title":"翟东升教授文章摘录总结","text":"作为翟老师的粉丝，翟老师的文章视频几乎每期必看。 此处将翟老师一些微博，头条号中的文章进行观点记录，以便查阅。 翟（dí）东升：中国人民大学国际关系学院副院长、博士、教授。中国人民大学世界经济专业与国际政治经济学博士生导师。中国人民大学国际货币所特聘研究员。 研究方向： 货币与金融的国际政治经济学，中国对外经济关系美国政治经济 人民币汇率、美元指数与全球通胀问题 2021-05-28 首先人民币汇率并不强，只是由于美元下跌看起来涨势明显。 时至今日，美国仅仅是我们的第三大出口市场。要讨论汇率对实体经济的影响，必须看CFET指数，也就是人民币兑一揽子主要贸易伙伴的货币的加权平均值。 人民币正在成为逆周期货币，且对美元应该有长期升值趋势 美元指数本身有17年左右的大周期，我们此刻处于新一轮下行周期的早期，考虑到新冠疫情冲击下美欧等国货币政策的现状和差异，明后两年内美元指数如果暴跌到70，无须惊讶。 关于美联储货币政策是否会尽快转入收缩的问题，我的判断是，他们会口惠而实不至。美联储主席和总统的关系会影响到货币政策。 鲍威尔若想获得连任，2022年之前很难加息缩表 我们此刻也许正在一个通胀率大时代的转折点上，未来三十年，也就是到2050年之前，全球主要经济体的通胀率也许是震荡上行的，或者说具有上涨容易下跌难的特点，各种预料之外的事情，比如疫情、战争、大国刺激生育和产业链调整等等，都会推动这个趋势的展开。 硕鼠的养成与美国的政治极化 2020-05-20 任何一个政治经济体系都存在不完美或者有待弥补的漏洞。识别、利用并扩大这些漏洞，就能给某些聪明人快速收割巨额财富和权势的机会，但却是以国民和政府的损失为代价的。他们的财务成功不是因为创造了财富，不是因为发明了新的技术和工艺，而是利用了政治和经济体系中功能失调的那部分扭曲和漏洞。 尽管饱受批评和嘲讽，科赫兄弟仍然是美国的成功人士，或者说仍然在操纵着美国的政治经济乃至科研和教育，而他们在中国的模仿者如今纷纷落马。中国特色社会主义市场经济与美国资本主义市场经济的根本区别由此显现：事实雄辩地说明，中国走的是以人民为中心的路线，美国走的是以资本为中心的路线；我们是经得起考验的人民共和国，而美国越来越像一个“香蕉共和国”。哪个国家更有前途？我从2009年起就断言，中国更有前途。","link":"/economy-finance/didongsheng/articles/"},{"title":"大师计划系列视频","text":"记录一下翟老师《大师计划》视频的内容。视频时间：2021.06 1. 当今世界格局与力量对比 幂律现象：强者恒强弱者恒弱。 美国领先，中国紧随其后，但是远超其他。 GDP规模 财政开支，军费开支 互联网巨头平台 独角兽企业数量 顶级大学 博士学位，高被引学者 专利产出，人工智能 中国仍未到达第二的地方 国际舆论话语权 国际组织影响力 货币国际化指标：美欧英日中 超过或正在超过美国： 出口贸易额 国内消费市场 理工科大学生人数 中等收入人群规模（4.5-5亿，是否具有小汽车） 汽车市场 智能手机销售 互联网接入用户 制造业产出增加值 能源资源消耗 500强数目 5g技术 2. 大胆拥抱崛起的中国 至2013-2035年。 目前大学生知识分子较多，能积极参与社会分工。 2012年前出口导向，央行资产负债表扩张（以美元储备作为基础）。 2014年前中国主权信用严重压抑，将来有巨大发展空间。 2013年以来，出口对外依赖度转变。提出并推动一带一路（非美经济体）。 以美元信用为基础发行货币，无论你的质量科技含量再怎么高，都是加强美元的购买力。 所以真正重要的不是跑的多努力，而是往什么方向跑。 3. 中国相对于美国“道”的优势 人类文明史上两种策略的竞争和对立（格劳秀斯的海洋文明流派 &amp; 康德的大陆文明流派）： 美国：选取政治、三权分立、私有产权、自由市场 集体价值本位：个体服从整体，自上而下分配管控 前者在市场竞争有优势（利益，贪婪），后者在军事上有优势（荣誉感，恐惧） 国家于市场的关系具有三角对立关系。 国家发展的好取决于公共部门的能力和特质。 底边越长越好。税率越低越好效率越高越高。 学习苏联，把底边建立。（但是苏联的税点特别高） 学习美国及盟友，降低综合税率，所以出现了繁荣的私人部门。 两者不能相互否定，而是要正体反体合体（中庸之道）。 4. 中国相对于美国“治”的优势 两个重要任务： 数字经济的转型：机遇和冲击 数据所有权对社会的冲击，国内现在正在规范，之前也进行了对本土公司的保护。 社会政治影响，民众识字率大幅提升，年轻人只从互联网获取和传播再加工信息，大众政治由小众政治所取代。 解决贫富分化。 美国：印钱，加税 中国：反腐，对不合规资本系打击，财富再分配（精准扶贫）","link":"/economy-finance/didongsheng/dashi/"},{"title":"肖钢讲座：我国多层次资本市场建设","text":"我国多层次资本市场建设主讲人：肖刚（中国证监会前主席） 日期：2020-11-19 十四五金融工作八大任务： 建设现代央行制度（1983建立，农业改革到企业改革，为金融发展提供历史性条件） 构建金融支持实体经济的体制机制 国有商业银行改革 市场枢纽，注册制，常态化退市，提高融资比重 金融双向开放 完善监管体系、透明度、法制化 整治乱象 防范风险 1. 主要内容（PPT总结归纳而来）1.1. 重要意义多层次资本市场强调资本市场枢纽功能（举例2013钱荒)： 微观主体活力、稳健货币政策、资本市场功能 $\\rightarrow$ 相互支撑 社会的需要推动政策的变动。 多层次资本市场是我国特有术语： 以金融工具类别来分：股债衍生品 以交易场所来分：场内场外市场。交易所市场，产权交易市场（中央最新文件，并列为新市场体系） 以发行方式来分：公募大众的市场，私募小众的市场 多层次资本市场重要意义： 改善融资结构服务实体经济 促进科技创新 满足财富管理需求 提高直接融资比重防范金融风险 扩大开放提升影响力 助力内循环与双循环： 资源配置能力 风险分担能力 市场约束能力 “技术创新+资本市场”是产业升级的强大推动力。 存在两大问题： 科技成果转化专业化服务体系不健全， 高校、科研与市场供需对接不畅。 信用评价体系缺乏差异化、针对性。知识产权融资有限、质押率偏低、不良率较高。 投贷联动有待完善。 1.2. 私募市场私募基金服务实体经济（2019年3季度末） 累计投资境内未上市未挂牌企业股权、新三板企业股权和再融 资项目数量达到11.18万个，形成股权资本金6.36万亿元。所投资账面价值分布，股权类资产占比46.2%。 2018年向境内未上市挂牌企业股权本金新增1.22万亿元，相当于同期新增社会融资规模的6.3%。 在投中小企业项目5.98万个，本金2万亿；在投高新技术企业 3.1万个，本金1.27万亿。 资金主要来源于：企业&gt;资管计划&gt;居民 1.3. 交易所市场中国上市公司海外收入比较低。 A股再融资政策调整： 非公开发行对象数量上限分别由10名和5名（创业板） 统一调 整到35名。 非公开发行价格由原先不得低于定价基准日前20个交易 日均价的90%，改成80%，吸引投资者，降低发行难度。 非公开发行锁定期缩短，原规定控股股东36个月、普通 投资者2个月不得转入，分别减半，改为18个月和6个月，不再受减持规则限制。 A股行业表现： 高增长、不稳定：5G、新能源、军工、半导体、互联 网、券商、创业板、中证1000。牛市涨得多，熊市跌 得多。 高增长、很稳定：医疗、医药、消费、白酒、食品饮 料、家电。牛市涨得多，熊市跌得少。 低增长，不景气：煤炭、纺织、有色、钢铁、能源、 化工、交通。牛市涨得少，熊市跌得多。 高股息、低增长：机械设备、电力、金融、地产、上 证50。牛市涨得少，熊市跌得少。 北向资金： 北向资金占外资持股近七成（2019年底）。 对A股有明显领先性，多次精准抄底“聪明钱”，作为投资决策的一个参考。 北向资金类型： 配置型资金，主要跟踪MSCI等指数 套利型资金（个股、指数、汇率间的利差），如离岸人民币汇率走高，北向资金流出，A股下跌 北向资金择股： 行业发展成熟，在产业链上具有高溢价。 所处行业壁垒高，短时间很难有新的竞争者。 持续稳定的高ROE. 什么是股票发行注册制： 注册制不是登记备案制，股票发行上市仍要经过审核同意，但与现行核准制不同。 只要不违背国家利益和公众利益，企业能不能发行、何时发行、以什么价格发行，均应由企业和市场自主决定。 以信息披露为中心，企业必须披露充分和必要的投资决策信息，审批部门不对企业资产质量和投资价值进行判断，更不“背书”，也不对信息披露的真实性负责，但要对招股说明书的齐备性、一致性和可理解性负责。 发行人是信息披露第一责任人，中介机构承担对发行人信息披露的把关责任，投资者自主作出投资决策并自担投资风险。 实行宽进严管，政府职责重在事中事后监管，严惩违法违规，保护投资者合法权益。 推进注册制改革的核心在于理顺市场与政府的关系。 注册制的巧妙之处就在于既能较好地解决发行人与投资者信息不对称所引发的问题，又可以规范监管部门的职责边界，避免监管部门的过度干预，不再对发行人“背书”，企业业绩与价值、未来发展前景均交由投资者判断和选择，股票发行数量与价格由市场各方博弈，让市场发挥资源配置的决定性作用。 监管部门则集中精力履行好事中事后监管职责，维护好市场秩序。 创业板面临挑战： 发行、定价市场化程度 再融资、退市效率，优胜劣汰功能 前沿产业与未来科技企业不足，缺乏顶尖巨头新兴产业。 “易进易出、快进快出、大进大出+明星企业” 1.4. 上市公司相关制度我国退市的现状：退市率低 成因分析： 强制退市标准尚不完备，且可执行性较差。 退市执行力度不够，为规避退市行为提供了条件。 股票发行被人为调控，造成上市资源稀缺。 投资者权益保障不足，加大了退市难度。 直接融资渠道有限，上市公司退市后生存困难。 1.5. 债券市场防范化解债券市场风险。 两大风险：企业违约风险、部分中小金融机构和非法人产品的杠杆风险。 总体思路：坚持市场化、法治化原则，有序可控打破刚性兑付，打击逃废债行为，加强政策协调，完善监管制度，切实保护投资者利益。 政策措施： 健全市场化、法治化信用风险处置机制。如银行理财、券商资管计划等非法人产品管理人在破产重整中的法律地位。 合理控制债券市场杠杆水平。 规范债券市场信用评级，统一准入管理，建立以投资者为主导的市场化评介制度，推动投资人付费服务模式，引入评级机构强制退出机制。 加强监管协调，监测预警，信息共享和失信联合惩戒。 1.6. 衍生品市场增强实体企业利用衍生品抵御风险的能力 我国商品期货共70个品种，自2013年以来，成交量连续五年占全球一半以上。 研究表明，实体企业运用衍生品套期保值，能降低风险，提高企业价值，在经济下行期更为明显。 持续推进衍生品市场改革，扩宽服务实体经济范围。 提升企业参与衍生品市场的意愿与能力，加快国际化步伐，提升我国定价影响力。 1.7. 资本市场基础设施狭义：证券市场参机构之间用于清算、结算或记录、支付证 券的多边系统。 支付系统、中央托管机构、证券结算机构、中央对手 方、交易数据库。 广义：上述五类以外扩大到：证券、期货、保险、黄金交易 所、征信系统、法律和监管环境、公司治理、会计准 则、反洗钱以及金融安全网。 1.7.1. 如何提高基础设施建设与改革重点难点：平衡安全、效率、成本以及证券行业发展的相互关 系。安全是前提，效率是关键，成本是基础，行业 发展是支撑。四者有机整体，相辅相成。 改革内容： 基础设施集约化：债券市场3家托管结算机构 场外交易数据库建设 区域性股权市场登记、托管、评估系统。 托管模式：直接持有模式和间接持有模式。间接持有法律关系：共有权（德国），信托关系（英国），证券权益（美国） 基础设施开放：与国际结算规则接轨、DVP制度境内外联结风险评估与防范，基础设施走出去。 基础设施风险防控：跨境跨市场风险，股票/债券质押、回购、期权市场，场内场外市场。流动性风险与交收违约风险。研究债券中央借贷机制，引入安全、效率高的流动性工具，完善担保品管理，全面推广逐日盯市，精细化，标准化，自动化管理，完善法律，建立折价过户、拍卖、债务抵消等快速处理机制 基础设施统筹监管 2. 给肖主席个机会讲一讲（没时间讲PPT） 我们的资本市场是在1990年建立，（1989动乱之后），那个时候资本主义国家封锁中国经济。 成立交易所释放了坚持改革开放的信念。 我国金融学过多的运用了西方金融学的理论，但是我国金融学的内涵与西方并不一样。应当创造性地研究提出中国特色的体制。 国外学界对中国经济发展肯定，但是对金融制度抱有负面看法。那么为什么经济成功金融不行？ 3. Q&amp;A 社会需求推动政策改革还是国家改革推动社会进步？ 社会的需要推动政策的变动。 怎么能不让科创板走创业板老路？ 上交所科创板针对战略新兴产业。 创业板起初主办化，包容性较差。 创业板在科创板上市时政策变化风险较大，先看科创板改革结果。 创业板也增强了保荐责任（推荐亏损企业必须跟投） 深交所所长：什么时候能恢复深交所主板上市？ 八卦很久没有实质性内容。 政府会不会拉一个牛市。 不能出现08、15年的情形，否则会影响改革的进行。 Andy：人生经验故事？ 没干货，终身学习 如何和香港制度接轨？合作？（这个人问得不清不楚） 建设国际金融枢纽 创新要素流动方面存在障碍（人才资本土地知识产权等）","link":"/economy-finance/seminar-talks/xiaogang/"},{"title":"Icarus 主题设置","text":"本文内容都与当前使用的主题 Icarus 相关。 具体的主题设置可以见此链接，对应的 markdown 源代码可以见此链接。 布局设施侧边栏设置sidebar中某个侧边栏的sticky为true来让它的位置固定而不跟随页面滚动。 _config.icarus.yml12345sidebar: left: sticky: false right: sticky: true","link":"/software-tools/hexo/Icarus/"},{"title":"Hexo常用功能说明","text":"本文档会用作Hexo基本用法记录演示。 Create a new post1$ hexo new &quot;My New Post&quot; More info: Writing Generate static files + Run server123$ hexo clean # Clean local files$ hexo generate # Generate static files$ hexo server # Build local server More info: Server More info: Generating Deploy to remote sites (use Gitbuh Action)The procedure is in .github/workflows/deploy.yml When this local git folder has been uploaded to github, the deploy.yml would be executed. More info: Deployment 文章折叠1&lt;!-- more --&gt; 文章间引用站内文章引用语法如下。 1{% post_link file_name Title_of_link %} Insert figures不同于markdown的图片引用方法，Hexo有着自己的语法。 图片文件夹位于_post目录下 1{% asset_img test.jpg%} 同时可以自定义图片大小。 语法与html语法相同。 1&lt;div style=&quot;width:70%;margin:auto&quot;&gt;{% asset_img test.jpg%}&lt;/div&gt; markdown的语法需要配置之后才可以使用，具体配置的方法见这篇文章 1![Test](test.jpg)","link":"/software-tools/hexo/hexo-functions/"},{"title":"Hexo-Trials","text":"本文记录使用Hexo中遇到的很多坑，以后可能会重复遇到，记录在此，以观后效。 功能&amp;支持Markdown image insertion grammar support具体配置的方法 123456_config.ymlpost_asset_folder: truemarked: prependRoot: true postAsset: true 运行&amp;测试坑Local server存在local server无法打开或打开极慢，但是GitHub Action正常部署的情况。 此时使用全局代理可解决。 经查原因为在编译HTML时，难以获取all.css文件，fontawesome.com需要代理访问。 将该网站加入代理规则即可解决。 写作排版坑公式支持使用 \\\\ 时不换行，原因是hexo使用的markdown引擎造成的。 其将第一个 \\ 识别为转义符号。 解决方法是换一个引擎，如下： 12npm uninstall --save hexo-renderer-markednpm install --save hexo-renderer-kramed 参考： https://jdhao.github.io/2017/10/06/hexo-markdown-latex-equation/ https://lanlan2017.github.io/blog/eb86e892/","link":"/software-tools/hexo/hexo-trials/"},{"title":"Hexo文档更新时间设置","text":"Hexo 在使用远程部署时，默认 update_option: mtime, 即以最后修改时间作为更新时间。 这个问题导致每次编译时，文章提交到远程，所有的文章都显示更新，且时间相同。 具体解决方法则是在 Front-matter 中加入updated:项，则编译过后的更新时间以此为准。 目前看来没有可以使其自动更新的方法。 批量加入更新时间这里对之前原有的 post 我们可以按照生成时间 date 来批量添加 updated： 具体事项过程即在 _post 目录下运行以下代码： 1for file in `ls .`; do value=`gawk '/date:/{print &quot;updated: &quot;$2&quot; &quot;$3}' ${file}`; echo ${value}; gsed &quot;3 a\\\\${value}&quot; -i ${file}; done; 需要注意的是，awk 和 sed 命令在 MacOS 下使用方法与 Linux 不同，此处应该使用 gawk 和 gsed。 手动更新时间在 _config.yml 中设置 update_option: date，则更新时间与文章创建时间一致。 虽然这样起不到“更新时间”的作用，但是至少不会无缘无故的对未更新的文件进行更新。 如果出现一些重要的文章修改，手动更新时间添加 updated: 即可。","link":"/software-tools/hexo/hexo-updated-time/"},{"title":"黑苹果使用记录","text":"黑苹果使用记录，很多功能调教起来还是较为麻烦的。 win 与 macOS 下显卡切换之前一直使用的是 RX470 显卡，由于最近玩2077实在是带不动，所以考虑换卡。 由于最近狂潮，AMD 显卡大涨一波价，于是乎买A卡总觉得很不值。 于是入手了索泰1080至尊Plus（1800rmb， 2020年12月）。 但是由于N卡不支持黑果，所以目前目标为 黑苹果下把显卡独立显卡屏蔽并使用集成显卡 UHD630 作为显卡。 Windows 下正常使用1080。 其实操作很简单，理论上只需要在 config 文件中加入wegnoegpu屏蔽外部显卡即可。 简述一下现在的设置情况： clover 的 config 下，添加 whatevergreen 驱动里的命令 wegnoegpu，便可正常进入 macOS。 主板 bios 设置为集显优先，则用集成显卡进入 clover。 主板插 DP 线，显卡插 HDMI 线（这个很重要且很玄学！我最开始是相反方式插着，结果无法进 macOS 系统。（显示器是有一个DP口）） windows 下进入会有两个信号源，集显和独显。在设备管理器中关闭集显不管用（集显的DP口仍有输出。） 在显示设置中设置只显示特定桌面（只显示独显连接的桌面）","link":"/software-tools/macos/hackintosh/"},{"title":"MacOS 功能与设置","text":"不得不说在多年的MacOS使用过程中还是有很多不方便的地方。 本帖记录一些经常遇到的问题与解决方法。 外接显示器音频设置MacOS是不可以通过HDMI/DP控制外接显示器的音量输出的。 但是黑苹果是一定要外接显示器，同时使用显示器自带的扬声器是极为方便的，所以这是一个需要解决的问题。 目前来看有三种解决方案： Soundflower配套方案 Monitor Control插件 Sound Control插件 其中Monitor Control在我的黑苹果上只可以调节亮度无法调节声音。 Sound Control试用期内可以使用（单独条件app声音），但是试用期过后无法通过调节全局声音来调整显示器输出。 Soundflower方案之前在黑苹果上一直使用，后来重装系统之后出现了蜂鸣现象。 考虑到SF方案体验很不错，于是尝试解决蜂鸣现象。 先简要介绍，SF方案需要下载Soundflower插件与相应的客户端Soundflower Bed（菜单栏插件）。 简单配置即可使用，此处省略，可以使用全局的音量调节调整显示器音量。 但是使用过程中存在播放一定时间后，出现蜂鸣Humming/Bizz，在之后便无音量输出，由于SF项目已经多年未维护，所以在issue里看大家讨论的结果。 问题出现的直接原因是Buffer size过小，溢出时就会出现此问题。 治标不治本的方法时直接选择最大的Buffer size，但是时间过长时还是会出现此类问题。 从大家的讨论中得出，问题出现的根本原因是Soundflower可能存在内置时钟与设备时钟不符，才导致缓冲区最终会溢出。 一篇2014年的帖子给出了方案. 新建一个”聚合设备”（名称必须用英文），时钟源设置为”Soundflower(2ch)”，实体设备显示器勾上“漂移修正”，然后就可以拿给Soundflower用了。 问题解决。 外接2K显示屏开启HiDPIMacbook外接大部分2K显示器时并不会开启HiDPI，需要手动开启。 开启方法很多帖子写的很详细了，详见此链接。 印象中是有一些坑，待下次再设置的时候更新。 Google Software Update 自启动详见这篇文章。","link":"/software-tools/macos/macos-functions/"},{"title":"主动学习与交互决策过程","text":"之前导师让调研一下“基于主动学习技术的可交互策略学习方法”，此处整理成文便于留存。 调研背景介绍调研关键词：主动学习，可交互，策略学习，降低标注成本 相关领域主动学习首先我们指出广义场景下主动学习有两种含义，技术面和任务面（不冲突，侧重点不同）。 但是不论是指技术还是任务，当我们提到主动学习的时候，总是伴随着对某项成本降低的期望。 技术面：一种以某种原则选取数据的技术（此种含义不必须与人交互） 任务面：一种以某种原则选取数据从而节省监督学习标记成本的任务（标记任务需要与人交互） 策略学习一般来说，策略是在指定任务中为完成任务目标而实行的行动准则。 目前来看，有两种被广泛使用的策略学习方法： 强化学习（需要与环境交互） 模仿学习（需要与专家系统交互） 具体调研的方向故此调研的调研重点置于主动学习AL与强化学习RL与模仿学习IL的交叉方向上面(以降低成本为目的)。 此处的分析及本文的调研以问题为导向，不特别深入到具体技术的归类。 问题分类（调研结构以此为标准）： 传统AL数据标记任务下的问题（监督学习中设计相关策略减少标记成本）（标记任务中与人交互） 使用RL/IL的policy作为AL标记任务的选取策略（AL的任务面） RL场景下的问题（有时与环境交互复杂昂贵） 定义一种数据选取的原则来指导policy的学习（AL的技术面） IL场景下的问题（与专家系统交互复杂昂贵） 定义一种数据选取的原则来指导policy的学习（AL的技术面） 另一种分类（在此调研中可找到相应模块）： 在策略学习中无人类参与 Active Reinforcement Learning 在策略学习中有人类参与 Imitation/Reinforcement Learning in Conventional AL Process Interactive Reinforcement Learning Active Imitation Learning 三类相关方向1. 传统AL数据标记任务背景简介在AL的任务面下， AL与RL有着相似的任务框架，都可以看作MDP。 两类问题描述如下： RL AL（任务面） Task Decision tasks Instance selection tasks with supervised learning Purpose Learn best policy to maximize the total return Construct best labeled condition (coming with best prediction results) Goal Policy Labeled condition 我们将两类问题的相关概念进行对比如下： RL AL（任务面） agent Apply [current state =&gt; action] selector Apply [current labeled state =&gt; selection] state Describe current condition labeled state Current labeled and unlabeled data action The response from the agent under the certain state selection The response from the selector under the curtain labeled state. (A set of unlabeled data) environment Apply [state + action =&gt; next state + reward] oracle/interaction Apply [labeled state + selection =&gt; next labeled state]. Oracle annotate the selection set. reward Given by the environment after an action is taken policy The guideline for the agent to take optimal actions query strategy The guideline for the selector to make optimal selections value Evaluation of the current state/action/state-action-pair score Evaluation of the current potential selections model (not necessary) The learned formula of the environment model The learned formula of the oracle 我们发现RL中的Policy其实对应着AL中的选取策略。 在此方向上，我们可以发现两者的区别 相似处：RL policy和AL strategy都应该同时考虑exploration&amp;exploitation。 不同处：RL学习policy，但是AL通常使用预先启发式设定的strategy。 问题与思路存在的问题： AL通常使用预先启发式设定的strategy其实很多时候并不是最优，而且对不同场景不一定适用。 这就带来了一个问题，这个预先设定好的heuristic有时候并不是最优，对不同场景不一定适用。 直观的方法： 从RL与AL的相似性出发，我们或许可以运用强化学习学习policy的方法来学习一个strategy。 目前主要有三类工作（数据标记任务，所以需要与人简单交互）： 在一个相关的领域学习策略 之后不伴随策略在数据集间的迁移 之后伴随策略在数据集间的迁移 直接在目标领域学习策略 KEY WORDS： Learning to actively learn, Data-driven AL, Meta-active learning (ability to work with all kinds of data) 现有工作列表在一个相关的领域学习策略，之后不伴随策略在数据集间的迁移： Learning active learning from data [2017, NIPS]: LAL. In pool-based AL setting. Random forest as basic classifiers. The state consist the current classifier parameters and a randomly selected data (Monte-Carlo). Train a random forest regressor that predicts the expected error reduction for a candidate sample in a particular learning state. The regressor works as the value function of the state action pair. Train the policy on the representative dataset and use it on the target dataset (without updating). Learning How to Actively Learn: A Deep Imitation Learning Approach [2018, Annual Meeting of the Association for Computational Linguistics]: ALIL. In pool-based AL setting. The task is named entity recognition in a cross lingual setting. The state consists of the labeled and unlabelled datasets paired with the parameters of the currently trained model. An action corresponds to the selection of a query data point, and the reward function is the loss of the trained model on the hold-out evaluation set. Imitation Learning directly learns the map (policy) from state to action in a supervised manner. The policy is then used on the target dataset (without updating). Learning to Actively Learn: A Robust Approach [2020]: Address that previous works in this area learn a policy by optimizing with respect to data observed through prior experience (e.g., metalearning or transfer learning). This approach makes no assumptions about what parameters are likely to be encountered at test time, and therefore produces algorithms that do not suffer from a potential mismatch of priors. 在一个相关的领域学习策略，之后伴随策略在数据集间的迁移： Learning how to Active Learn: A Deep Reinforcement Learning Approach [2017, Arxiv]: PAL(Policy based Active Learning). In stream-based AL setting where the policy/strategy is to decide whether to query the current instance. The task is named entity recognition in a cross lingual setting, where transfer the learned strategy/policy to the target domain (where no enough data to learn a strategy). The policy is learned by deep Q-network. The state is the current instance the previous constructed dataset. The reward is given by a hold-out set. Then the learned policy is transferred to the target dataset with policy updating. 直接在目标领域学习策略： RALF: A reinforced active learning formulation for object class recognition [2012, CVPR]: First to consider AL as a MDL process. They use Q-learning to learn the adaptive combination between exploration and exploitation. Use the overall entropy as the reward in each iteration. The state is the strategy combination condition and the action is the trade-off parameter. They also used a guided initialization for Q-table. Active Learning by Learning [2015, AAAI]: ALBL. By Hsuan-Tien Lin (NTU). Connect AL with the well-known multi-armed bandit problem. Choose strategies in the AL process by estimating the performance of different strategies on the fly. Use EXP4.P as the core solver. The action is to choose a specific strategy. Learning Loss for Active Learning [2019, CVPR]: Attach a small parametric module, named “loss prediction module”, to a target network, and learn it to predict target losses of unlabeled inputs. The “loss prediction module” could be considered as the value function of the state-action pair. The state is the current model parameters and the action is the selected instance. The idea to directly predict the loss is similar to [Learning active learning from data [2017, NIPS]], but this work doesn’t need to train the policy in advance. The policy is trained during the AL process. Learning How to Active Learn by Dreaming [2020, ACL]: The follow-up work for [Learning How to Actively Learn: A Deep Imitation Learning Approach]. Recent data-driven AL policy learning methods are also restricted to learn from closely related domains. This method fine-tune the initial AL policy directly on the target domain of interest by using wake and dream cycles. Cross-domain and cross-lingual text classification and named entity recognition tasks. Wake learning is an AL process, and the dream learn is a policy updating process. The current weak model was used as a weak annotator to train the policy in the dream phase. 此类方法在其他类型标注任务中的相关应用: Multi-modal Active Learning From Human Data: A Deep Reinforcement Learning Approach [2019, ICMI] Reinforced active learning for image segmentation [2020, ICLR] Learning to Actively Learn Neural Machine Translation [2020, CoNLL] Deep Reinforcement Active Learning for Medical Image Classification [2020, MICCAI] 分析与看法首先这一节我们面对的还是传统的AL场景，面对的主要是数据标注的任务。 相比于传统的AL启发式方法，此类学习Policy的方法对算力的要求很大，虽然标称的表现也会好，但是如果真的实际使用则需要权衡。 浅见： 对不同任务设计特异性的AL策略（不论是启发还是学习一个策略）是当下的趋势。 对于一些复杂任务，基于RL策略的迁移或许是一种良好的解决方案。 2. Reinforcement Learning 场景背景简介强化学习相关知识与概念在此不赘述。 具体可以参见这篇文章。 强化学习面对的问题场景就是一个广义的学习一个智能 agent 的场景。 一般通过与环境的交互来学习 agent 的 policy。 环境交互的过程中一般会黑盒地出状态的转移以及相应的奖励。 Policy 的训练依赖于观测的结果。 一般是在观测到的 trajectory 中进行训练: \\{S_0,A_0,R_0,S_1,A_1,R_1,S_2,A_2,R_2,...,S_{n-1},A_{n-1},R_{n-1},S_n,R_n\\} （主流的强化学习方法分为 model-based 与 model-free 两种，区别在 policy 的训练是否依赖对环境的状态转移建模。） 训练的过程中存在一个问题：若持续以当前的学到的策略与环境进行交互（exploitation），那么学习的结果则容易陷入一个局部最优当中。 所以在强化学习中 exploitation 需要伴随 exploration，这样可以避免陷入局部最优并且找到更好的策略。 最简单的分配方式是 $\\epsilon-greedy$，按照一定比例分配 exploitation 和 exploration。 如果 exploitation 过多，则容易陷入局部最优，如果 exploration 过多，则较难有充足的数据训练一个较好的policy。 所以说 exploitation 之外的 exploration 也十分重要。 通常情况下RL假设与环境的交互是即时完成且没有花费的。 然而现实情况下，与环境的交互，通常是需要付出代价的（时间或算力或实际成本）。 在 large, high-dimensional environment 中，这个成本更为高昂（复杂的模拟计算等）。 此时我们希望我们每一次的交互都能被更好的利用起来： 在 exploitation 中用更好的训练方式（大部分强化学习在研究的的） 在 exploration 中选择更好的交互样本，更高效探索（active） 于是RL则与AL选取更优样本的逻辑产生了交叉，主要是聚焦于探索的场景下。 《人工智能：一种现代的方法》21.3中的主动强化学习提到了这一现象。 我们想更优更高效地 exploration（Agent 不必关心那些它知道不需要的而且可以避开的状态的具体效用）。 此处我们称这种选取为 active reinforcement learning。 此处的 Active 不一定一定要有人的存在。 这里的 Active 含义: the agent seeks out novelty based on its own “internal” estimate of what action sequences will lead to interesting transitions. 问题与思路存在的问题： RL与环境交互会产生交互成本。 较多的探索会影响模型的训练，同时也会造成一定的cost损失。 直观的方法： 借鉴AL的思路，制定或学习探索的方式，更高效的探索，使相同数量的探索下训练的policy更好。 目前主要有以下几类工作： ARL: 整体框架仍是RL的框架。加入探索的heuristic。（不与人交互，只与环境交互） ARL-1: 与环境交互获取状态转移和奖励时产生cost ARL_2: 只在与环境交互获取奖励时产生cost，状态转移不产生cost。 IRL: 以RL的框架为基础，添加interactive的部分。（需要与人交互） KEY WORDS： active exploration, active reinforcement learning, interactive reinforcement learning 现有工作ARL-1:与环境交互获取状态转移和奖励时产生cost。(No human interactions) 由于需要主动的选择来与环境进行交互，一般来说是要选取我们认为环境能反馈更多信息的样例。 通常的做法是在对环境的建模上来衡量informativeness。 具体衡量的方法与AL中的方法相似（QBC/Entropy/Information Gain, etc.） 所以说绝大部分的工作都是基于Model-based RL。 Active Reinforcement Learning [ICML, 2008]: Before this work, many ideas are trying to explore the actions which agents has experienced the fewest number of times in the past. In this paper, the agent determines the sensitivity of the optimal policy to changes in transitions and rewards. It then focuses its exploration on the regions of space to which the optimal policy is most sensitive. The process is similar to ADP. Use Taylor’s approximation to model the local sensitivity of the utility on current policy. VIME: Variational Information Maximizing Exploration [2016, NIPS]: Agent should take actions that maximize the reduction in uncertainty about the dynamics (transition). Use BNN to learn the model. This can be formalized as maximizing the sum of reductions in entropy of transition probability. The exploration to the uncertain state of the models should reward more. Model-based active exploration [2019, ICML]: Better state-action pair makes the model ensemble changes more. The utility of the state action pair is the disagreement among the next-state distributions given s and a, in terms of JSD (Jensen-Shannon Divergence), of all possible transition functions weighted by their probability. (用可能的transaction分布变化作为criteria。) Self-supervised exploration via disagreement [2019, ICML]: The idea is similar to Model-based active exploration. Generate an intrinsic reward, defined as some difference measure (i.e., KL-divergence, total variation) across the output of different models in the ensemble drives exploration. (Such exploration might be ineffective, as the policy may visit regions of the state space which have no relation to solving the task.) Explicit explore-exploit algorithms in continuous state spaces [2019, NIPS]: Applicable in large or infinite state spaces. Exploration and exploitation are controlled by a threshold. Exploration is guided by the disagreement in the model set. And the models updated after get the exploration trajectories. Exploitation is guided by one of the model in the model set. Ready Policy One: World Building Through Active Learning [2020, Arxiv]: View MBRL exploration as an active learning problem (select trajectory from trajectory space), where we aim to improve the world model in the fewest samples possible. Acquiring data that most likely leads to subsequent improvement in the model. The exploration metric is based on reward variance computed from a (finite) collection of models. SAMBA: Safe Model-Based &amp; Active Reinforcement Learning [2020, Arxiv]: From Huawei UK. Safe-RL setting, every trajectory comes with a cost. Aim at minimizing cost, maximizing active exploration (bi-objective), and meeting safety constraints. Increases in regions with dense training-data (due to the usage of CVaR constraint) to aid agents in exploring novel yet safe tuples. The exploration use an expected leave-one-out semi-metric between two Gaussian processes defined, for a one query state-action pair. (Model change) ARL-2:只在与环境交互获取奖励时产生cost，状态转移不产生cost。 Specify the cost C: Active reinforcement learning with monte-carlo tree search [2018]: The reward needs to be paid to observe. The received reward has the discount for the querying cost. Active Measure Reinforcement Learning for Observation Cost Minimization [2020]: The policy applied with Q-learning. Add whether to query as part of the action in Q-table, so the number of action would be twice as large as the ordinary Q-table. After each query, it jump to the queried state and update a transition model. Otherwise, it go to the state according to the transition model. (没太懂这篇文章的intuition) IRL:与环境交互获取状态转移和奖励时产生cost，人类交互不产生cost 此类问题的背景仍旧是搜索空间大，搜索耗时且复杂。 所以仍然是在与环境交互时产生cost，希望使用人类专家在训练时的介入来缓解这个问题。 人类专家的交互仍然是为了使exploration更高效。 但是需要强调，此处的问题并没有对与人类专家交互的数量与质量进行假设。 Exploration from Demonstration for Interactive Reinforcement Learning [2016, aamas]: A model-free policy-based approach, uses human demonstrations to guide search space exploration. The demonstrations are used to learn an exploration policy that actively guides the agent towards important aspects of the problem. In the proceeding of a episode, when the agent reaches a informative state (computed by leverage and discrepancy), it queries the action from the state with the closest leverage to the current state, then update the policy parameters. A self-play with the environment followed in this episode, and the parameters are keep updating. Interactive Teaching Strategies for Agent Training [2016]: A work by Microsoft. Include two agent: student(learn from RL) and teacher(fixed human policy). Contains two module: student initiated and teacher initiated. Query when the student has a low Q-value difference on the current state. (the student is uncertain about which action to take) Agent-Agnostic Human-in-the-Loop Reinforcement Learning [2016, NIPS]: Develop a framework for human-agent interaction that is agent-agnostic and can capture a wide range of ways a human can help an RL agent (e.g. Q-values, action optimality, or the true reward.) 分析与看法ARL和普通RL的工作界限其实并不是很明显，因为很多 RL 的工作也是需要处理 exploitation 和 exploration。 此处只是挑了一些很明显使用到 AL 相关思路的文章，如果要详尽调研则较为困难。 IRL 其实比较有趣，它主要是想通过人的参与让与环境的交互更高效，人在这里只是起到画龙点睛的作用，或许这种交互可以用到很多 RL 系统。 3. Imitation Learning 场景背景简介模仿学习相关知识与概念在此不赘述。 具体可以参见这篇文章。 简单来说，模仿学习会在学习的过程中与专家交互，同时获得专家对指定 state 所作出的 action \\pi_*(s)。 模仿学习的目标为学习一个 policy，使其与专家的 policy 尽可能接近（模型生成的状态-动作轨迹分布和输入的轨迹分布相匹配/获得同样的 reward 等）。 通常情况下，根据应用场景的不同，IL可以分为以下几类（By ICML 2018 IL Tutorial）: Behavior cloning Direct policy learning (multiple step BC) Inverse reinforcement learning (assume learning R is statistically easier) 与RL类似，IL同样是在观测到的trajectory中进行训练: \\{S_0,A_0,S_1,A_1,S_2,A_2,...,S_{n-1},A_{n-1},S_n\\}。 不同的是，在 trajectory 中并没有每一步的 reward。 问题与思路存在的问题： 当我们想要通过模仿来学习策略的时候，唯一的学习来源来自于 Expert 对所询问的 stat e的回答（相应 action）。 这种与 Expert 的交互是昂贵的。 直观的方法： 询问专家时，挑选更有价值的 state 来询问相应的 action。 目前的工作也是基于 IL 的分类，存在于以下两类： Direct Policy Learning Inverse reinforcement learning 在 IL 中，一部分工作使用一定数量与专家交互下 policy 的表现作为评估，类似于 AL 中的学习曲线。 所以说一部分 IL 天然的可以看作 AL 的问题（以 AL 选取策略降低与专家交互成本）。 （另一部分使用一定数量与环境交互下 policy 的表现作为评估，是以人的参与降低与环境交互成本，类似之前提到的 IRL。） KEY WORDS： imitation learning, active imitation learning, active inverse reinforcement learning 现有工作Under Direct Policy Learning: 此类工作的目标是学一个 state 到 action 的 map，是一个监督学习的过程，选取的方式基于学习到的 supervised model。 Active Imitation Learning: Formal and Practical Reductions to I.I.D. Learning [2014, JMLR]: Under the subclass Direct Policy Learning. Construct a committee by bootstrap sampling (posterior over policies). Select states by density weighted QBC strategy and get the corresponding action. The density is from trajectory from the simulated environment. Maintain a supervised labeled state-action pairs. Query-Efﬁcient Imitation Learning for End-to-End Simulated Driving [2017, AAAI]: Maintain a safety classifier to decide which state needs to be queried. Use supervised loss between the action given by trained policy and reference policy on the visited states. Under Inverse reinforcement learning: 由于目标之一是学习 value function，所以通常是以当前的 value function 来构建选取 state 的逻辑。 Active Learning for Reward Estimation in Inverse Reinforcement Learning [2009, ECML PKDD]: Take the entropy of the value prediction on the state as the sample strategy (where the reward is unsure). i.e. the disagreement on the learned reward functions. It query the specific action from the oracle on the selected state. (162) Where to Add Actions in Human-in-the-Loop Reinforcement Learning [2017, AAAI]: Selecting the state s to query the action where adding the next action most improves the estimated value of state. Policy Optimization with Demonstrations [2018, ICML]: Use discriminator between human demonstrations and trajectory from environment to make the exploration close to the human’s action. The purpose is to use the current demonstration to train a policy as good as possible. (Demonstration is collected in loops, could be considered as an AL process.) Interactive Teaching Algorithms for Inverse Reinforcement Learning [2019, IJCAI]: Focus on how could a teacher provide an informative sequence of demonstrations to an IRL learner to speed up the learning process. The main idea is to pick a demonstration with maximal discrepancy between the learner’s current policy and the teacher’s policy in terms of expected reward. 分析与看法Imitation Learning 天然和 AL 有很多相似的地方。 AL 是想模仿 oracle 的黑盒模型，IL 是想模仿 expert 的黑盒策略。 人在 IL 中的重要程度就像人在 AL 中的重要程度一样，因为人类是系统唯一可以获得监督信息的地方。 所以说本节所在意的并不是环境的交互成本，而是人类的交互成本，这就使 AIL 与 IRL 产生了区别（问题的定义不同）。 总结本文以两种维度介绍了主动学习与可交互策略相关的问题，并以第一种维度进行了展开（第二种维度同样可以找到相应模块）。 其中对于人类专家存在的交互式的策略学习，当前已经有了一些综述或者前瞻的文章。 这些工作主要介绍了不同交互的方式与目的，并不一定包含减少标记成本的模块。 Power to the people: The role of humans in interactive machine learning [2014, AI Magazine] Scalable agent alignment via reward modeling: a research direction [2018]: From Deepmind. Leveraging Human Guidance for Deep Reinforcement Learning Tasks [2019, IJCAI] A Survey on Interactive Reinforcement Learning: Design Principles and Open Challenges [2020, ACM DIS] 最新进展&amp;发展趋势本文并未对相关技术的最新进展展开探讨，但是从问题的角度来讲的确出现了一些新问题。 包括但不限于： 由 active RL 到 active safe RL IRL 中考虑交互的方式（不同形式的指导，或者相同形式指导的不同展现方式） 但是现在来看问题的大类并没有发生什么改变。 交互的成本只有两类：与环境交互成本，与人类专家交互成本。 在这次简要的调查中并没有发现其他类的问题。","link":"/machine-learning/active-reinforcement/"},{"title":"翟东升人民币国际化课程","text":"翟老师金融货币市场的课程。 第一讲：为什么关于人民币汇率的悲观预测错了这一讲的目的是破除一定对于汇率的谬误。 现象：2014-2016汇率破7。所以多数持牌企业用人民币海外投资，资金成本昂贵但是收益低。只有当5%速度贬值时才能获利。 看空人民币的5个错误理由： 国内已过增长期 房地产泡沫不可持续，资产价格高估，只能通过汇率贬值调整 地方政府债务大，泡沫破裂会导致汇率下跌 过早的去工业化问题（东南亚，一带一路），产业和资本的转移。 中国GDP为50%，但是m2是美国的150%以上。 错在哪里（实证数据）： 经济增速和汇率相关吗？ 顺周期货币和逆周期货币表现不同。外围地区顺周期，中心地区逆周期。 中国变为顺周期。 资产泡沫与汇率波动相关性？ 跨国资产价格并不具有一价定理（回归均值） 政府并不存在保汇率保房价问题 地方政府债务并没有那么高。（债务/GDP） 同时债务率和汇率之间负相关。只有有钱的人才能借到更多钱。 衡量债务风险的不是债务率，若以本币借债，本质上是一种隐形税收。 格林斯潘：“女士们先生们，大家不用瞎担心，这些债其实我们永远都不用还了。” （e.g.翟币） 资本输出汇率不一定贬值。 资本输出会让货币更加稳定，外部获取收益，导致长期强势。 m2与汇率的关系。 不公平的对比，评估美国要用m3（美国直接融资），中国m2（银行信贷）。 货币发行增速和汇率不相关。 资本大鳄的做法既不道德也不慎重。 为什么人民币快速升值: 中国产业升级 中国老龄化 划重点，汇率并不是由以下这些点决定： 经济增速 房价波动 债务率 对外投资、产业迁移 货币发行增速 第二讲：长期汇率由什么决定 短期汇率由市场情绪决定，不具有可预测性 中期汇率（3-5年）由政府调控影响 长期汇率（5-10）情况下，市场内在力量影响巨大 影响长期汇率的直接因素（实证数据验证）：一个国家可贸易品的价格水平直接影响汇率。 可贸易品价格水平由什么决定（两个表层的经济因素）： 技术水平（但只能解释20%左右的变化） 人口年龄结构，老龄化（能解释65%左右的变化） 越老龄化，汇率越强。供给不变，需求下降，价格下降。（e.g.日本） 影响汇率的六个深层因素（越高越强） 国家能力：公共部门能力与效率问题 开放度：积极融入国际贸易（出口） 能源大宗商品（以物为主）：明显顺周期特征 制造业为主（以人为主）：逆周期特征明显 要素特征 人的核心竞争力（脑力/体力） 人还是物 文明类型 坚挺的文明有两大类 新教文明 东亚文明：骨子里不信神（实用主义的神），努力证明自己的财富 软的文明： 南亚东南亚：小乘佛教，印度教，不鼓励生产也不鼓励消费（供给需求都萎缩） 天主教东正教伊斯兰教：不鼓励教徒寻求财富，鼓励寻求快乐（供给小于需求，则借债） 人群智商 宗教严肃度：询盘 海外投资的汇率风险哪些国家的汇率可能大规模贬值，详见深层因素。 第三讲：为什么中国被动成为最大外汇储备国巨额的外汇储备不是大国的标志而是附庸国的象征。 外汇储备源自对汇率波动的恐惧。 发展中国家汇率波动和受到冲击非常大。 中国的外汇储备是如何形成的为了快速工业化，招商引资，对人民币需求增多，有升值压力，但是又不能允许快速升值来保障制造业。 于是中国人民银行使用外汇占款买入外汇，再由国家外汇管理局买其他国家国债。 对冲流动性（4万亿美金外汇储备30万亿人民币外汇占款，货币乘数为4的话就是120万亿广义货币，此时为了避免通胀），这些策略都是有成本的： 发行央票 中央政府财政存款放在库底（利息低） 提高存款准备金率 央行拿外汇储备干什么去了买国债。中国外管局是美元的主要空头力量： 猜想：外汇来自资本顺差和贸易顺差，还是要控制风险。 外管局将一部分美元换成其他货币进行投资降低风险。 以中国外汇储备来预测美欧元汇率 外汇储备无助于汇率稳定外汇储备是追求汇率稳定的结果。 持有巨额外汇（代价大，且并不安全）： 巨量外汇占款，基础货币扩张，资产价格泡沫，央行对冲成本积累。 只能持有国债，低收益。高科技产品公司和资源无法买到。 并不能维护稳定例子：50亿美金做空巴西雷亚尔（5000亿美金储备） 要不然汇率暴跌 要不然储备暴跌，通货紧缩，经济萧条 本国居民逃离 面对做空正确的策略：有序地贬值。 中国不再需要外汇储备不需外汇，只需要一些黄金。 技术进步，产业升级，老龄化都可以减轻人民币汇率的贬值压力。 我们需要担心的是汇率的长期强势。 第四讲：政府意志与人民币汇率波动的历史探讨政府在货币定价中扮演的角色及作用的方式方法。 2005年前，汇率为出口导向型工业化提供支持此时需要本币相对低估。以1:1.5（1980）到1:8.7（1994）。 见到实体产业迁移消长变迁的时候，要对以后几年潜在金融危机汇率动荡债务危机警惕。 1994-2005年，汇率相对稳定。 亚洲金融危机时人民币拒绝贬值，导致工业化早期出现了出口困难。 （用自身的经济困难换区地区稳定。） 日本却因为主动贬值丧失了地区领导权。 汇率主动温和升值是好事吗？自主渐进可控的人民币升值背景： 美国认为人民币低估，要求人民币升值。 人民银行如果要保持汇率，有通胀压力，要扩充外汇占款。 出口制造部门反对汇率升值。 导致许多人搞套息交易。 从香港搞廉价美元贷款，付给深圳公司，换成人民币，然后利差息差获利。 巨量热钱流入中国。 邀请全世界人来赌人民币升值，形成一定的泡沫，上证指数1000点到6000点。 长期人为的扭曲付出了沉重代价。 后人视角给当时的决策者提建议： 汇率波动走势具有随机性。 但是使月周级别不可预测。 减少热钱流动。 后金融危机时代，人民币汇率何去何从2008之后渐进升值，但是大大减缓。 2014人民币被低估的程度不高了。 2014-2016又贬值了4%左右（时间换空间，防止被做空）。 此时需要把热钱留出去。 外商持续投资减少，撤离中国（超国民待遇消失）。 2016不要对人民币贬值恐慌，认识到中国可贸易部门竞争力提升。 2017人民币由市场升值 恐慌性升值，逼空 2018贸易战，贬值预期 没有安抚市场，任由市场波动。 对美出口下降，但是人民币贬值，总出口稳定。 2020-2021汇率明显上涨 新冠疫情，出口率先恢复。 CFETS汇率指数，与一篮子货币进行稳定化，与美国经济脱钩（换锚）。 汇率还是要基于经济基本面。 划重点 东亚金融危机，中国赢得周边国家信任，为周边外交开拓了市场空间，日本丧失地区领导权。 汇率的渐进升值，把市场参与者的风险转移到了政府手里，市场调控应该注意。 政府塑造的中短期汇率及其波动方式的能力很强，但是长期人为扭曲将会付出沉重代价。 高明的汇率政策既要照顾实体经济的需要，又要充分考虑金融市场的内在规律。 第五讲：铸币税及其国际国内再分配效应铸币税的前世今生如今货币创造成本越来越低。 中国综合税率占GDP比例较低，但是如果把铸币税和土地财政纳入考量，则综合税率不再低。 勤俭节约的美德“过时”了无锚货币时代，不应再痴迷储蓄： 储蓄越多，向锚货币交的税越多。 2012年前，货币增速快，钱的购买力在不断降低。 无锚货币时代，用好杠杆做好投资。 中国人民存在海外的储蓄在不断地蒸发。 越难复制的资产，升值保值能力极高。学区房，高年份茅台，艺术品等的增值快于货币增发速度。 外汇占款带来的货币扩张帮助了谁？剥夺了哪些阶层？补贴了哪些阶层？出口部门获利，剥夺内地居民，补贴东南沿海。 中央政府通过税收反哺内地是极其必要的。 中国央行从中国人民手中收取铸币税之后转手送给了美欧日央行，以此换取中国出口制造业的发展空间。 为了扶持东南沿海企业家，才会有货币超发，才会有天量外汇储备。 离开了政府提供的优质公共产品，这些企业家的表现不一定好。 2020年出口繁忙，人民币升值，出口企业不挣钱因为汇率变动。 离开了政府保姆般的呵护，很多企业没本事挣到那么多钱。 很多先富起来的人将政府看成他们聚敛财富的负面因素，于是： 2015看空人民币 投资移民走了 外资企业也是明显的获益者群体：以折扣价获得土地。 利用制造业服务业投资，通过土地获得资产升值的红利。 中国底层消费力的重要变化外汇储备稳中有降，征收广义铸币税规模大大下降。 金字塔底层的群体有了一定的消费能力。 劫富济贫：美国梦的真相美国从全球征收了多少铸币税？ 资产负债表增量/GDP：较小 贸易逆差-海外投资金收益：通过财政收入产生数倍GDP 美联储扩表，剥夺全球储蓄者。 0.9（2008前）=》2=》4.5（2014）=》缩表（2017初）=》扩表（2019.10）=》4.2（2020.2）=》无限量化宽松=》7.5（2020末）=》15（2025？推测） 铸币税补贴了谁？ 小部分补贴了美国的穷人。 主要补贴了，美国的资本所有者，股市大涨。 所以要推动人民币国际化，征收全球铸币税，吸引全球储蓄者放进我们国债池。 划重点 2012年前，为了支持本国制造业国际竞争，用土地财政和铸币税补贴了出口产业，造成了不同资产收益率的巨大差异，也造成了不同地区发展水平和不同阶层收入水平的巨大差异，形成了强大的国内再分配效应。 美国的无限制量化宽松，伤害了美国底层和全球外围国家，但是令他最富有的人大大获益。 中国要通过人民币国际化获取一定的全球铸币税，保护自己的财富。 第六讲 人民币国际化的相关问题不平衡的全球货币格局美欧 GDP 占比42%左右，但是其货币却占83%。 中日韩 GDP 占比27%左右，但是其货币却只占8%。 人民币国际化的进展2009年，周小川呼全球改革货币体系。 人民币国际化提上日程。 人民币国际化指数在2010年左右只有0.02%，2020年已在4%左右。 在贸易上和投资上增长比较快，储备较低。 人民币面临的问题是国债池子太小。 随着中国高端制造业发展，人民币计价份额也会像日元一样上升。 人民币国际化的改革努力放松人民币跨境使用的管制。 政府的主动动作。 开放性大宗商品交易所，意味着全世界的资源出口国可以不必绕道美元进行交易。 2015年前，人民币国际化路径和日元相似，提升本币在贸易结算份额，通过香港离岸市场发展日元海外市场。 2015年，香港市场人民币波动形成了人民币做空预期，中国政府不得不拉高隔夜拆借成本，抑制投机浪潮，意识到日本模式缺陷。 2015年后多管齐下，步步为营： 人民币大宗商品交易 人民币对外直接投资 国债市场对外开放 上海金融中心建设 金融业外资准入放开 一带一路倡议 中资公司的跨国发展 2020，稳慎推进人民币国际化。 未来： 中央层面协调不同部门法规，让企业家乐于便于使用人民币 人民币上印全球通用语言 超大面额钞票，便于金融不发达地区 抛弃宏观债务率迷信，统一中国国债市场，增加交易活跃度和便利性 发行特别国债来置换人口流出区域地方政府的高息债务，节省利息成本 统计局统计公布数据时以人民币为单位，同时鼓励其他经济体使用人民币作为备用单位 美国对人民币国际化的态度2010年之后最初几年，中国小心翼翼，“推动人民币跨境使用”。 搞经济金融事务的人看好人民币国际化 政策和战略研究的人不看好 美国政策界乐于见到搞人民币国际化，但是认为人民币无法俘获信任， 主权信用无法相信 必须放开货币汇率管制，实体经济可能受损 数字货币带来人民币国际化弯道超车的机遇中国推出数字人民币： 在swift系统下，美国的制裁会造成挤兑破产。 于是中国需要解决卡脖子问题。 猜想： 如果能顺利推广到境外，可以农村包围城市，代替一部分美元现钞。 清晰坦率解释，取代地下流通的美元，提供资源帮助国家建设货币系统，实现货币现代化。 一带一路中，将数字人民币使用和援助相挂钩。 铸币税一部分好处恰当的分享給发展中国家人民 人民币国际化的前景及其影响 货币网络效应对人民币国际化影响 中心化网络导致强大货币存在兑换优势 人民币国际化需要先挤兑其他小币种的份额 人民币国际化对中国经济的影响 汇率水平上升，国际购买力翻倍 经济规模增大，本土消费市场会成为第一大市场 人民币国际化代价 汇率水平敏感的行业消亡 划重点 人民币国际化未来需要多管齐下，稳慎推进，久久为功。 全球货币市场存在赢家通吃的网络效应，人民币国际化在本世纪中期才会轮到挑战美元份额。 数字人民币将帮助人民币国际化弯道超车。 人民币国际化会造成产业转移，这也是我国劳动力结构变化的内在要求，应该顺应这个市场规律而行。 实操课：全球货币体系知识如何转化为利润人民币汇率的预判及其分析方法长期： 中国快速迭代产业进步+老龄化趋势=坚定看多人民币 中期（3-5年）： 中央政府政策导向以及与美国的关系。 短期（1年）： 关注中国新生儿数量，数量下跌的话大胆做多人民币。 如果政府鼓励生孩子且取得实际效果，可以适当看空一点人民币。 超短期（1-2月）： 关注美元指数波动。 美元在两种情况下会走强两种情况： 当美国率先与欧洲和东亚出现经济复苏和走出困境中。 当全球经济陷入大麻烦时。流动性躲到短期美元国债。 如果美国出现政变，或者遭受核武器，或者资产泡沫破裂，导致股市暴跌，资产价格失序，那这个时候美元价格会涨还是会跌？ 其实是会大幅上涨。事情越大涨得越凶。 次贷危机和新冠疫情为例。 半球模型与全球资产配置的大周期理论半球模型（形似健身房物件）可以用这个模型近似全球资本主义货币体系的构造。 当一只脚踩下去，气体会像边缘扩散。 当中心出现减息，资金便宜了，资金向外围溢出，资本主义外围地区出现局部和暂时的繁荣。 反过来的话，繁荣消失，金融危机。 这只脚的踩踏和提起形成美元价格相对一篮子货币的波动（美元指数）。 过去50年，美元指数有着明确的周期，平均16-17年一次，下跌十年反弹六年。 美元下行周期，可以从中心借入便宜资金追逐资本主义外围的高风险资产。 美元上行周期，做空外围，把从外围挣到的钱连本带息还回中心，利润躲进无风险资产，美元短期国债。 三位一体风险定价体系下列图中纵轴是资金的价格收益率risk 横轴是时间或者说债券的久期。连成一条线是美国国债收益率曲线。 横轴的单位从时间久期切换为地点，这个世界的中心外围体系，越中心资金价格越便宜。 横轴切换为资产类别。房地产风险最大收益率最大。 长周期资产配置蕴含较高收益率的模型美元处于下行周期，把资金投入全球外围地区，在这一轮美元下行过程中最受益的工业化的经济体。 指标上看哪一个国家的国际收支平衡表上堆积越来越多美元储备（热钱往哪里流）。 所需做的去王公贵族居住的最最贵的房产和土地，然后不要动，操作周期在10年以上。 当美元指数上涨的时候，所需做的是把当地房产卖掉，获得当地货币，卖掉换回美元，换回美国短期国债，只追求活下去。 回溯过去50年的美元周期1970年中期跑到拉美，在布宜诺斯艾利斯、里约热内卢、墨西哥城，买入最贵的房地产。 1980年代初，卖出当地房产和货币，回归短期国债。 1980年代中，指数再次下行，做右肩交易（趋势确定），在东亚日韩港台新加坡买入最贵的房产。 1990年代中，美元上涨，卖出当地房产和货币，回归短期国债。 2001到03年开始，又大幅下行，在中国投资，北京四合院，上海小洋楼。 2015年美元指数上涨，周而复始。 那些外围国家的房地产值得投资问题切换成，那些外围国家在美元下行周期能成为工业化浪潮的赢家。 最理想的候选国家，应该具有以下条件： 强政府，最好不要玩选举。民主制度是奢侈品，只有实现工业化才可以享受，否则劣质民主或者民主倒退。 外交采用务实的外交政策，与各个大国搞好关系（同时中美）。 经济上重商主义，鼓励出口，让自己家制造业越办越好。 人口劳动力素质比较好且有一定规模，人均智商偏高。 文化上最好不怎么信教，追求世俗的财富和成就。 地理上最好有一些值得开发的港口，因为海运成本较低，腐败与否不重要。 如果找到一两个发展中国家符合一两个或者大部分条件，需要长期关注其政治变革和政策组合。 这种国家一旦进入工业化轨道，办工厂的人未必能积攒巨大财富。 这种工业化积攒的财富最终以各种形式流入当地统治阶级和王公贵族。 导致统治阶级聚集的大城市资产价格大幅上涨。 如何识别泡沫反身性索罗斯的操盘策略很多时候是在美元指数上涨周期做空外围泡沫比较严重的经济体。 索罗斯是用主观认知和客观事实之间的循环联动性，寻找自我实现的预言，自我增强的趋势，利用反身性来识别泡沫。 实体经济和虚拟经济之间的背离问题两个案例，注意关注类似情形，警惕危机： 东南亚制造业在1990年遭到中国出口工业的竞争而被掏空，而房价股价持续走高。而汇率却铆住美元，结果产生金融泡沫。 21世纪头十年，中东欧地区经济体融入欧盟统一市场，抢夺原本属于南欧实体产业的市场空间，南欧货币融入欧元无法通过汇率调整反应经济基本面变迁，出现泡沫，欧债危机。 为什么很多发展中国家喜欢搞固定汇率和联系汇率制度？ 出口制造行业低端，无法经受汇率波动，于是铆定主要的贸易对象。 注意未来也会有国家与人民币挂钩。 从发达工业国还是发展中穷国，哪个挣钱容易？两种国家： 发达国家钱多富裕但是人精明。中国人挣辛苦钱。 穷国穷，没什么钱，人单纯，知识匮乏，政策漏洞多，定价错误多。 在发展中国家布局相对更容易。 但是道德上不建议这样做。 两种策略有一些发展中国家有大企业，具有国际性业务。 例如能源、采矿、服务业制造业。 他们有公允价值。 在他们本币大幅贬值时，这些企业股票价格或迟或早等比例上涨。 如果判断出本币贬值过程，及时进场加杠杆做多这些大企业。 本币贬值会带来某些无风险收益。 划重点","link":"/economy-finance/didongsheng/rmb-courses/"},{"title":"翟东升政经启翟系列视频","text":"翟老师与观视频合作的系列视频，内容可能较为广，此处总结要点便于回顾。 整个系列视频的时间为 2020-06 至 2021-01 2020-06-03 雄安，你就是中国的学园都市了！如何在新型举国体制下处理政府和市场关系 市场是政府的创造物，是一种基础性公共产品，目的是覆盖消费者。 政府要帮助新的商业模式探清道路。 科学由政府来做，技术由市场和企业来做 政府需要向国民提供安全。 政府可以为企业分担小部分的技术研发风险。 科技创新： 技术路线不能单主体决策 适当的远离市场，“养一批闲人”在雄安，创造一个良好的科研氛围。 2020-06-10 美国以后不让我们摸了，中国该怎么自己过河？创新人才基础存在（智商较高）。 中国文化抑制创新的因素： 重物而轻人，不愿意为人才买单只愿意为物品买单。 对说谎造假的惩罚和仇视不够，机会不能给到有真才实学的人。 好面子怕出丑怕红脸，尊老。 不喜辩论不喜批判。 教育考试体系求全，要求平衡发展。 人才之间跨国交流，目前被抑制。 2020-06-17 曾经的华尔街之狼，为什么在特朗普时代成了哈士奇？美国金融业对美国内政外交的影响力产生持续大规模的衰落。 1960年代，华尔街背后家族有多个族群，基本都是犹太人。 1970年代前，美国国家利益就是有利于通用公司的利益。 1980年代，什么有利于华尔街什么就有利于美国。 上位过程： 华尔街通过捐赠逐步上位。同时搞人员渗透（1960开始持续上升，最高达到60%）。 华尔街搞意识形态渗透，逐步上位。例如格林斯潘。 华尔街大而不能倒，绑架内政外交。 下位过程： 08年金融危机之后政治声望一落千丈 和其他医药协会、步枪协会比不占上风 世界资本主义利益分配结构： 美国产业通过股权债券外包给欧日韩 欧日韩FDI（外商直接投资）产业迁移到中国实现中国工业化（要付给外商15%） 中国买美国国债，和华尔街有紧密联系（3%收益率） 3%-15%是快速工业化的代价 中方和华尔街关系紧密（互利），2016年之后华尔街影响力急剧衰弱。 负利率时代，受伤最严重的就是金融系统，所以华尔街影响力可能还会持续下降。 2020-06-24 中国年轻人压力这么大，怎么才能让他们有钱消费？《美国真相》：政府市场关系的分析。 市场私人部门繁荣是因为政府能提供有效公共产品： 和平安全 基础设施 国际贸易条约 基础科研基础教育 知识产权保护 在保证公共产品的情况下，综合税率越低越好。 为何瑞典和北欧区域如此成功？不是因为税收高，企业就不干活，而是政府有钱可以转化为有效公共产品，导致私人部门的繁荣。 国内主要矛盾： 全球总需求本国总需求增速偏低，但是供给能力偏强，供给过剩。 所以： 内循环，让中国年轻人有钱去消费。 中国综合税率和美国差不太多，并不适合给企业减税，降税是增加了后代债务，给全球消费者输出了通货紧缩，加强了中国与外部的竞争力度。 鼓励年轻人生孩子。 2020-07-01 数据时代中国真正的对手还是只有一个：美国新技术带来的挑战：科技进步对中国对世界的冲击和影响。 科技进步利于再分配而不是创造新财富。 以更低成本以满足原有需求。 现象： 垄断性平台剥削生产商 商家相对消费者能力更大了 科技进步后果： 贫富分化进一步增大 大企业和政府谈判地位更强 负利率时代，人工智能发展时代，金融业能容纳的就业人数未来将会萎缩。 教师等细分领域的人员也会需求降低。 英语汉语西语对其他语言的文明的冲击可能十分明显。 如何建设新时代中国特色社会主义，不能刻舟求剑。 重新思考教育制度，终身教育。 退休制度，福利制度。 养老育儿制度。 2020-07-08 黑人运动在美国翻不了天，真正会使美国蛮族化的力量是…美国文化革命的根源： 种族的结构性变化 贫富分化 美版文革特点：种族党派贫富分化叠加一起。 共和党几乎全部白人。民主党五花八门。 美国文化革命发展缓慢，呈现周期性，需要社会动能去刺激。持续时间长。 运动的前景： 虽然以黑人为导火索，拉丁人真正动员起来才是真正的高潮。 平权的悖论：肤色，性别，年龄段。是要求机会的平等还是结果的平等？ 如何评判革命是否成功？（是否通过斗争达到团结？）美国正在出现古罗马的蛮族化。 对中国的启示： 要对中美竞争有信心。 信心源于，我专而敌分。 美国种族主义的原罪。 加强对拉丁裔文化的研究。什么样的新美国化？对美政策有的放矢。 2020-07-15 翟老师：下代霸主应该是个十亿人口规模的国家…印度：正是在下全球化的周期（贸易全球化指标：出口/GDP）： 1870-1914 上升 1914-1945 下降：英帝国的衰落 1945-1979 平行竞争，半球化时代 1979-2008 全球化时代，中国融入世界市场，英美提出新自由主义全球化。 2008-至今 全球化指标下行，逆全球化。 为什么全球化会有一个波动？全球化是历史的必然趋势吗？ 广义全球化：大航海突破地理隔阂。 狭义全球化： 霸权周期就是全球化周期。 几波全球化： 荷兰大航海，使商品跨境流通。人口大规模增长。百万人口。 英国工业文明诞生，FDI跨界流通。人均GDP跨境流通。千万人口。 美国，信息革命，货币跨界流动。亿级人口。 十亿人口规模？中国？印度？欧盟？ 2020-07-22 统治世界400年的新教，就要被我们“筷子教“打败了？看起来是霸权更替，但是荷兰英国美国都是新教文明。 12年前，指标上出现重大变化。2019年，东亚文化制造业总产出超过新教文明圈。 东亚文明圈的共同点，都在用筷子。 过去两百年形成的大众政治体系在逐渐过时。 小众政治时代，对全人类的政治稳定提出巨大挑战。 2020-07-29 欧盟又开始追求大一统了？中国先笑出了声……全球市场对欧洲一体化前景乐观。 需要中央政府对利益进行再分配。因为内部不同省份竞争力变化。 在欧盟下看，汇率对德国低谷，对南欧高估，所以强者愈强，弱者恒弱。 但是以前欧盟的转移支付能力有限。 新冠疫情下的转移支付法案，让他家看到了一体化的希望。 欧盟对中国的定位（疫情之前）： 系统性对手：政治和社会制度方面 谈判伙伴：国际问题 经济竞争 未来三角游戏： 军事：中美俄 经济：中美欧 欧盟体系：成员国太多，成事不足败事有余。 2020-08-05 眼看和平演变马上要成功..公知和特朗普：快！“救”中国黑格尔：重要的历史往往上演两次，第一次是悲剧，第二次是闹剧。 美国对华派系 老一代中国问题专家，知华派愿意接触中国，白人为主。 新一代中国问题专家，很多由中国去美国，对中国文化抱有仇恨态度，认为接触派是错的。坚持有原则的现实主义，施压中国。 进入美国体系，经济上的获益，是以独立性丧失为代价。 虽然特朗普政府短期带来的损失比较多，但是戳破了过去四十年美国对中国实施的和平演变政策，而且客观的太高了中国的国际地位。 两个时代的中美关系： 奥巴马时代：不平等，不对抗 特朗普时代：平等，对抗 2020-08-12 美国：我得癌症了，急需医生！美国医保：可肿瘤就是我美帝国为什么相对衰弱了？ 过度扩张穷兵黩武？这个观点站不住脚。国防占比占财务支出规模减少。 美帝国体系最大的问题是内耗。医疗医保支出持续扩大，医保体系存在巨大的系统性问题。 美国医疗产业越来越垄断，形成强大利益集团。 药品福利管理局也在合并垄断，推荐高价格药品。 保险业不受联邦反垄断法管理。 且这一系列链条向政府提供了很多的政治献金。 猫鼠沆瀣一气。 如何游说的？ 华盛顿K街，很多游说公司。 和国会议员助理沟通，再简短的和议员见面写支票，进入他下一次的竞选资金池。 2020-08-19 从小父母告诉我读书可以改变命运！美国学生：我酸了…两种教育思潮的撕扯： 自由主义教育思想：爱与自由，自然生长。重视文科艺术。个人价值本位。 非自由主义教育思想：调用本民族教育资源。从严要求约束。重视理工科。集体价值本位。 美国美国自由主义教育好不好？ 公立教育质量非常差。 很早放学。 体育娱乐活动多。 理工科难度比较低。 中小学老师待遇低，素质差。 读书改变命运的观念不存在。读书无用论。 美国高等教育存在巨大泡沫。 人文领域人才来自私立教育，理工人才来自欧亚大陆。 日本看到日本年轻人综合素质比较低下。举例： 中文专业无法用中文英文交流。 旅游专业无法安排行程。日语也无法讲解。 1980教育改革，让孩子自由生长。 造成平成废宅。 百年前英德自由主义和非自由主义教育理念之争，百年前英德已经完美演绎了 1870-1914： 英国 小学入学率偏低（50%），中学更低（25%） 着重于发现天赋 嘲讽对技能的培训 德国作为一个落后国家 重视教育，认为教育是义务 强调纪律和礼貌 重视理工科 中国不能学习英美的自由主义教育。 坚持为中华民族崛起而读书。 应该继续重视理工科。 在高考下，寒门仍能出贵子。 针对目前教育的问题，可以通过更实际的方式解决。 2020-08-26 压低老百姓的福利来发展本国经济，这不是必须滴！以德意志第二帝国的成功经验为例。 发达的公共产品： 市场经济 福利制度 全序列高质量教育 基础设施建设 立法保护创新 德意志帝国产业部门兼并联合。 真实世界中垄断不一定带来低效，可能会以全球消费者的福利损失来通过工资和福利分享给本国国民。 德国金融为实体产业服务，鼓励金融和实体产业股权人事上高度交叉。 对中国启示： 加大教育研发投入，保护知识产权 搞好社会再分配，实现社会团结 不必压低本国福利来谋求经济发展 2020-09-02 这个盛世帝国的灭亡，给中国复兴之路留下的5个经验教训以德意志第二帝国的灭亡为例。 二十世纪早期的德意志帝国和当今中国有相似之处： 后发的工业国，通过新一轮工业革命的基于实现对原有大国的赶超 与原有大国是盟友关系，但趋于反目 陆海复合型国家，且安于陆权 想过与原有大国重温旧梦（英德，中美G2） 新型大国韬光养晦 反对老大国的自由主义经济学，强调国情独特，强调个体服从整体，爱国主义价值观 国内搭建铁路，海外延伸，要实现大陆体系互联互通 模仿发达国家再技术反超 面临外国邻国的仇怨 工业化之后通过财富再分配缓解国内分化 认为老霸主是全球帝国，不会集中所有力量，对方会上门来打，自己有主场优势。把问题想象的相对简单。 会重蹈覆辙吗？有若干点关键不同。 政治制度不同。世袭封建和资本主义的嫁接vs民主集中制度。 名族个性不同。中国人实用唯物主义。德意志浪漫主义，内在的自杀倾向，悲剧之美。 人口规模不同。中国相当于美欧日只和。人口质量长期来看不同。 外交制度不同。中国不搞扩张主义。 中国不穷兵黩武。 对我们的教训： 不要过度刺激民族主义。 给本国民众提供平等发展空间。 不能热衷操纵民意。 联盟战略。 少搞存量博弈，多搞增量博弈。 军事力量建设，不能面面俱到要有专长。 在一两个维度有绝对优势。 充分战略规划和估计，不能太过乐观。 2020-09-09 讨论“中国GDP何时超美”没意义，这不是时间能决定的问题用线性外推推测中国GDP合适超过美国是存在很严重的错误的。 大国GDP相对力量变化背后结构性因素逻辑是什么。 中国经济超过美国不是客观趋势而是一种选择。 背后的逻辑结构： 过去100年绝大多数国家GDP难以长期超过美国。 中心国家比外围国家富有且稳定。 大国人均GDP占美国比例会不断上移趋进却无法达到美国经济。 美国的GDP和其他国家的GDP不一样。美国提供流动性，其经济是虚的。 美国经济的“虚”决定其他经济的“实”。 美国GDP占全球1/4： 非美经济规模应该等于美经济规模（生产=消费） 美元占全球货币市场份额50%多。 所以： 中国如果以美元储备为基础，永远无法超过美国GDP。 出口换美元存在局限 2020-09-16 美帝国自以为一切尽在掌握，不料失控而让这里再度崛起东亚供应链的政治经济学含义。 目前三大供应链：东亚，美加墨，欧洲。 日本是龙头，其成功之后低端产业转移到四小龙，之后再转移到中国，现在正在转移到越南。 为什么东亚能成功？龙腾文化。 东亚具有强政府，对于国民比较强势。工业化往往是由强政府推动。强政府一般是由战争而来。 东亚民众智商高。全球人均智商最高的区域。 东亚文化总体不信神。（升官发财生儿子。交易心态。）高储蓄率。 东亚模式缺陷：依附性的出口导向的发展，以本国民众的血汗换取别的国家的主权信用。 美国用投资收益顺差来支付贸易逆差。 经济表现不错，通胀低位。 但是以中国加入东亚供应链而转变。 于是美国提出TPP想把中国踢出，但是特朗普推出。 所以RCEP获得发展。 2020-09-23 中国工业化搞全产业链通吃？这是违背经济规律的幼稚做法！中国实现最大规模最快的工业化，主要原因1992年之后： 区别于日韩，中国欢迎别的国家来投资。（超国民待遇） 这一系列工业化浪潮，打乱了日本的雁行计划。 1994年人命币贬值，与美元非正式挂钩。 开放欢迎制造业外资。 中国角色的变化，从低端到终端。 本土品牌，本土供应链崛起。 目前全产业链都有。 中国要保持全产业链吗？不能。 汇率定价，要素价格配置不能同时适应低附加值和高附加值的商品。 通过产业升级改造，可以产生贸易顺差，人民币汇率会进一步升高。 此时劳动密集型产品没有生存空间。 中国劳动力总人数萎缩，劳动力成本越来越高（这是我们发展的目的。）。 我们不是要保持全产业链，而是要保留高附加值的产业。 不能有卡脖子的事情。 低端的污染的产业要让别人承担一部分。 低端产业也不能转移到一个国家，非东亚国家。 新冠会导致去中国化加速吗？会强化中国制造业的地位。 虽然东亚制造业先断裂再快速恢复，介入到了很多原先排斥我们的地方。 中国在东亚取得了中心地位。 政治影响之前日本经地区首要地位时，政治上十分软弱。 而中国的地区领导力令人刮目相看。 2020-09-30 日本究竟毁在哪里？日本政策界：我们也反思了35年…中国汇率提升会不会重蹈日本的覆辙？ 日本有什么问题和德国相比，日元升值很晚。 大企业财阀推动少升值晚升值。 导致产业升级得慢。 德国不靠汇率低估来获取竞争力。 所以德国央行不需要大规模放水导致泡沫。 同时日本生育率持续低迷。 广场协议日元升值。 广场协议之前日本大幅低息放债。 这种快速升值，伤害了其制造业。 日本通过资本项目放开和离岸中心来拉动经济。 导致资产泡沫化。 问题所在：开始拒绝升值，后来短期大幅升值。 对于中国2005年开始升值，每年6%左右。 政府给予纺织等行业补贴。 但是去除补贴他们的利益也创新高。 2020-10-07 中国低端制造业能不能转移到内陆？取决于这两个关键因素观点：不可行 云南行想要将江浙沪玩具产业转移到云南的园区。 但是价差很微小。 长期来看这些制造业难以生存。 人民币汇率长期看涨。 且老龄化形势严峻。 云南的要素价格难以与东南亚等国PK。 劳动力产业东南沿海的原因，运输成本低。 云南运输成本很高。 重庆黄奇帆引入其他地方的产业，补贴成本。 仍然没有提高重庆占国内的制造业占比。 我们发展的目的让人民过上好日子，而不是老板过上好日子。 不是为了拥有这些制造业。 一些无关国家安全的产业可以放弃。 2020-10-14 听说欧洲人有好为人师的习惯，那就给他个机会，顺便打击美国反华势力新时代中国应该向谁学习？欧洲。 过去的中国，先学习苏联，再进行中国特色社会主义探索，再学习美国及其盟友美日韩。 为什么一定要向他人学习呢？ 向自己学习不吉利（lol），中国之前向谁学习谁就变差了。 师心自用和我们传统文化相悖，我们虚心好学。 和世界态势有关。国外势力新冷战，中国也没有意愿推广中国模式。以美国为首对中国模式敌视。如果抬高欧洲有利于制造统一战线。 向欧洲学习： 我们依旧谦虚 不再向美国学习 寻求政治共识，孤立分化共和党反华 学欧洲什么？欧洲不是一个统一的模式： 莱茵模式，以德国为代表 斯堪的纳维亚半岛北欧模式 地中海模式 盎格鲁撒克逊模式（比较市场化与美国类似），出口导向 我们想要学的是前两种模式以及作为一个整体的模式 德国的资本主义：重视国有企业在命脉产业，大陆法系，发挥国家市场两种力量 利益相关者资本主义模式（不是stockholder） 欧洲各国在专利发展，人口阶级纵向流动，绿色发展等方面都十分领先。 欧洲最大的公共产品是和平，其实现了一个人类命运共同体。 学习欧洲需要避免的教训： 移民政策 福利政策，给老年人福利偏高 个人价值本位的人权政策，抽象人权。人权不能神圣化，我们要搞集体价值本位的人权。 2020-10-21 打破人口外流与经济衰退之间的恶性循环，振兴东北奥里给！东北有什么问题？东北为什么不行？ 并不是因为讲人情搞腐败，90年代的广东也有这些问题。不能用普遍因素解释区域性的特殊现象。 和地理区域有关，冬天工作时间少。但是这个观点也不合理，北欧和美国东海岸都冷。 全球化和老龄化带来的人口流动产业变迁才是根本问题。 全球化： 解放前是工业化最早的地区，解放后学苏联也是很领先全中国的。 1992年之后，中国成为美欧体系外围，东南沿海区位优势，人口优势处于这个体系中。东南沿海的枝叶嫁接在别人的根上，而放弃了自己的根东北。 东北军工科技和其他地区脱钩。 老龄化： 老龄化社会产出不会产少，可以用机器 但是老龄化社会消费萎缩，人的生命是有欲望周期，48岁是欲望高峰。 下岗潮和老龄化浪潮导致东北和其他地区拉开差距。 这两个因素导致人口自由流动，年轻人出去回不来，加剧了东北老龄化趋势。 本地的就业进一步下降。 人不是物以稀为贵。 人越密集不是竞争更激烈更难，而是每个年轻人发展机会越大。 认识群居动物，人聚在一起分工规模扩大，交易机会增加，合作成本降低。 为什么东北成为中国老龄化最快的区域？ 因为东北是现代化城市化工业化最早的区域。 现代化核心指标，女性的受教育提高。 女性个体意识觉醒，导致离婚率上升，生育率下降。 这个对女性是好事，但是长时间轴看，老龄化趋势加重。 如何振兴东北？寻找一个契机，是东北区域形成新的良性循环。 e.g. 图们江出海口打通，每年多几个月北冰洋到西欧的海上通道。 再做一个新城大城，提供特殊政策，吸引东北年轻人，甚至中国世界的年轻人。 2020-10-28 地方政府亟需从抢资本转向抢人：得年轻人者，得天下！中国地方政府操盘过程中从强资本转向抢年轻人的逻辑。 资本不再稀缺，人才稀缺人才是21世纪最重要的发展要素。 这个转变的原因： 全球需求的萎缩，产能过剩 别国更富的人变老了，在去杠杆 中国人也在变老，产能过剩 欧日长期处于负利率时代 所以目前是“资产荒”，有钱人不愿消费，年轻人没有钱。 蛋糕难以做大。 存量博弈时代，需求侧更加重要，才要抢人。 从别的地方抢年轻人，以邻为壑。 抢人是划算的。 极少部分人才是重要的，其他人只是为了保持基因多样性。 10000人中最优秀的人被吸引走了，那这10000人价值其实很少了。 人才的现金流十分可观。 年轻人的养老教育等都是将来时。 但是年轻人带来的效益是立竿见影的。 （深圳直呼内行。） 鼓励地方政府恶性竞争？这个从来不是新的东西。 以前一直都是抢资本的恶性竞争。 但是抢人会改变竞争的方式和重点。 把补贴外国资本的资金拿来补贴本国年轻人。 以人为中心，比倒贴资本来说，格局更有合理性。 先有人还是先有产业？蛋生鸡还是鸡生蛋？ 有人就有市场。 不一定是要人才，有需求的年轻人都是需要争取的对象。 BBC这种无良媒体，是媒体经济学家。 他们不懂底层逻辑，只拿数据对比。 人口流出地区，借很少的债都是在作孽。 人口流入地区，多借债是没问题的。 比如武汉区域位置很好，大学教育先进，长期来看，武汉的大学生一半左右留在武汉。 此处来看基础设施建设很必要。 如何抢人？ 要形成良性竞争。 年轻人在你这里生活很方便。 基础设施，医疗教育跟上。 抢人再着急也不能抢外国人。 中西部不适合抢人，抢人适合大城市。 政治经济学原理： 财富的源头是人而不是物。 发展的本质是人的能力的提升而不是物的堆积。 技术进步导致人的消费比劳动更重要。 所以年轻人比老人更重要。 自由主义经济学主张，善待企业家。 但是这个很容易变成，善待有钱人，厚待有钱人。 民本主义政治经济学要善待本地的年轻人。","link":"/economy-finance/didongsheng/zjqd/"}],"tags":[{"name":"cs-seminar","slug":"cs-seminar","link":"/tags/cs-seminar/"},{"name":"感悟总结","slug":"感悟总结","link":"/tags/%E6%84%9F%E6%82%9F%E6%80%BB%E7%BB%93/"},{"name":"machine-learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"reinforcement-learning","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"imitation-learning","slug":"imitation-learning","link":"/tags/imitation-learning/"},{"name":"active-learning","slug":"active-learning","link":"/tags/active-learning/"},{"name":"survey","slug":"survey","link":"/tags/survey/"},{"name":"Cuda","slug":"Cuda","link":"/tags/Cuda/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Conda","slug":"Conda","link":"/tags/Conda/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Pytorch","slug":"Pytorch","link":"/tags/Pytorch/"},{"name":"读书","slug":"读书","link":"/tags/%E8%AF%BB%E4%B9%A6/"},{"name":"Research","slug":"Research","link":"/tags/Research/"},{"name":"翟东升","slug":"翟东升","link":"/tags/%E7%BF%9F%E4%B8%9C%E5%8D%87/"},{"name":"Icarus","slug":"Icarus","link":"/tags/Icarus/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Master Ma","slug":"Master-Ma","link":"/tags/Master-Ma/"},{"name":"MacOS","slug":"MacOS","link":"/tags/MacOS/"},{"name":"Hackintosh","slug":"Hackintosh","link":"/tags/Hackintosh/"}],"categories":[{"name":"Computer Science and Engineering","slug":"computer-science-engineering","link":"/categories/computer-science-engineering/"},{"name":"Knowledge from Growth","slug":"knowledge-from-growth","link":"/categories/knowledge-from-growth/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Paper Reading","slug":"paper-reading","link":"/categories/paper-reading/"},{"name":"Programming","slug":"programming","link":"/categories/programming/"},{"name":"Reading Note","slug":"reading-note","link":"/categories/reading-note/"},{"name":"Research","slug":"research","link":"/categories/research/"},{"name":"Economy &amp; Finance","slug":"economy-finance","link":"/categories/economy-finance/"},{"name":"Software Tools","slug":"software-tools","link":"/categories/software-tools/"}]}