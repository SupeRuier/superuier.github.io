{"pages":[],"posts":[{"title":"对抗搜索问题和 Minimax 算法","text":"人工智能中的对抗搜索学习笔记。 对抗搜索问题对抗搜索其实可以看作游戏场景，存在一个我们不能控制的对手。 相比于搜索，我们不是要找一个值或一系列序列，而是要找一个策略（strategy/policy）来对对手的行为进行反馈。 而游戏是人工智能中一个比较难的方向，它可以分为以下几类。 正式化 Formalization将对抗搜索的问题抽象化，正式化，我们会得到以下要素： $s$: state, $s \\in S$. $p$: $p \\in P(s)$, defines which player has the move in state $s$. Usually taking turns. $a$: action, $a \\in A(s)$, returns the set of legal moves in $s$. $T(s,a)$: $S \\times A \\rightarrow S$, transition function, defines the result of a move. $u(s,p)$: utility function or objective function for a game that ends in terminal state s for player p. It describes the earning at the end of the game. $\\pi(s): S \\rightarrow A$, strategy, output an action given a state. Minimax 算法这里很多内容参考了 Yu Zhang 的笔记。 问题背景这里不再是广义的游戏背景，一般来说做出了一些限制。 Minimax 算法常用于「有限状态，零和，完全信息，两个玩家（Max/Min）」博弈问题，比如棋类。 算法概要存在两个玩家，Max（希望最大化自己的收益），Min（希望最小化 Max 的收益）。 从初始状态开始，通过两个玩家选用不同动作交替操作，可以形成一个树，树中的每一个节点都代表一个状态$s$。 我们的目标是找到一个给 Max 的最优策略，使在游戏结束时的状态拥有最大的效用函数，这是一个在树中搜索的问题。 在 minimax 算法中，我们认为每一个状态 $s$ 都拥有一个 minimax 值（递归得到）。 这个值表示从当前状态 s 开始，双方均采取最优策略直至游戏结束时玩家 Max 的效用值 具体到计算过程中，如果状态为终止状态，那么它就是当下的效用值；如果是中间状态，则是子状态 minimax 值中的（最大值，Max 操作，目的最有利于自己；最小值，Min 操作，目的最不利于 Max）： \\text{minimax}(s)= \\left\\{ \\begin{aligned} &u(s, \\text{Max}) & & \\text{if GameOver}(s) \\\\ &\\max_{a \\in A(s)}\\text{minimax}(T(s,a)) & & \\text{if Max's turn}\\\\ &\\min_{a \\in A(s)}\\text{minimax}(T(s,a)) & & \\text{if Min's turn} \\end{aligned} \\right.对于算法的解释 对于游戏树的 DFS 搜索，从终点回推过程节点的 minimax 值。 最优/最终的节点可以出现于任何深度。 在 Max 使用最优策略的情况下： 如果对手 Min 使用最优策略，那么一个节点只要计算出了 minimax 值，就已经看到了游戏的结局（效用值）。 如果对手 Min 不使用最优策略，那么 Max 的最终效用只会更高。 一个简单的例子，自下而上的递归推导。 Alpha-Beta 剪枝假设 game tree 的深度为 $m$，每个节点有 $b$ 种走法，则该算法的时间复杂度为$O(b^m)$，在实际情况中是不现实的。 相应的，其空间复杂度为$O(bm)$。 哪怕是在最简单的一字棋（tic-tac-toe）游戏中，这个数量级也相对较大： 存在 $9!=362880$ 个终止节点。 所以我们必须要进行一定的手段来对搜索空间进行限制，来保证搜索的实际可行性。 具体的思路是，我们其实不必要遍历所有的点，因为很多点是必然不会经历/不需要考虑的。 例如下图，虚线框的值不必考虑，因为 C 节点的 minimax 值必然小于 2（Min 只会选取最小节点），所以在 A 节点上选取是必然不会考虑 C 节点。 所以，在此算法上： 对于每一个节点，我们维护一组上下限 $[\\alpha, \\beta]$，表示在当前节点 Minimax 值的范围，初始设置为 $[-\\infty, \\infty]$。[Max 至少能实现的收益, Max 最多能实现的收益] 对于一个 Max 节点 $s_{max}$，我们向上更新他的 $\\alpha$ 值，将其子节点 $m$ 的 $\\beta(m)$ 值与当前的 $\\alpha$ 做比较： 如果 $\\beta(m) \\leq \\alpha$，则后续分支可以剪掉（直接 return）。 若 $m$ 的一个子节点 $n$ 上，$\\beta(n) \\leq \\alpha$，则 $\\beta(m)$ 必然小于 $\\alpha$。 对于 $\\beta(m) &gt; \\alpha$，则需要更新 $s_{min}$ 上的 $\\alpha$ 值。 对于一个 Min 节点 $s_{min}$，我们向下更新他的 $\\beta$ 值，将其子节点 $m$ 的 $\\alpha(m)$ 值与当前的 $\\beta$ 做比较 如果 $\\alpha(m) \\geq \\beta$，则后续分支可以剪掉（直接 return）。 若 $m$ 的一个子节点 $n$ 上，$\\alpha(n) \\geq \\beta$，则 $\\alpha(m)$ 必然不小于 $\\beta$。 对于 $\\alpha(m) \\lt \\beta$，则需要更新 $s_{min}$ 上的 $\\beta$ 值。 对于叶节点 $k$， $\\alpha(k) = \\beta(k) = \\text{utility}(k, p)$ TIP 即使使用了 alpha-beta 剪枝，在实际中也基本不可能搜索到游戏结束，这就需要使用启发式评估函数 (heuristic evaluation function) 来代替游戏结束时的效用函数，来对一些较深层的状态进行评估，这里不再展开。","link":"/artificial-intelligence/ai-adversarial-search/"},{"title":"人工智能的基本概念","text":"人工智能基础知识和一些概念。 AI 是什么？其实很多地方都给出了不同角度的定义，此处仅记录自己的看法。 AI 是承载于机器或程序，人为设计或训练，并可以解决特定或一些列特定问题的能力。 其涵盖多个计算机领域：机器学习/优化/搜索/etc. 智能体AI 承载于智能体（Agent），即我的定义中的机器或程序。 Agent = Architecture + Program 在解决问题的过程中，智能体通过传感器（Sensors）接受外界环境（Environment）的信号，并自我处理信息，之后作出相应行动（Action）。 于是在对一个智能体进行定义时，通常要考虑 PEAS： Performance Environment Actuators Senors 环境环境有很多重要的类别： Fully observable (vs. partially observable) Deterministic (vs. stochastic) Episodic (vs. sequential) Static (vs. dynamic) Discrete (vs. continuous) Single agent (vs. multi-agent) Known (vs. Unknown) 下面是一些具体的例子。 Environment Observable Agents Deterministic Static Discrete 8-puzzle Fully Single Deterministic Static Discrete Chess Fully Multi Deterministic (Semi)Static Discrete Poker Partially Multi Stochastic Static Discrete Backgammon Fully Multi Stochastic Static Discrete Car Partially Multi Stochastic Dynamic Continuous Cleaner partially Single Stochastic Dynamic Continuous","link":"/artificial-intelligence/ai-basics/"},{"title":"约束满足问题（CSPs）","text":"人工智能中的约束满足问题（Constraint Satisfaction Problems 问题）学习笔记。 约束满足问题约束满足问题的目标是在一定的约束下，寻找符合条件的状态。 这种问题在生活中比较常见，以大学排课为例，已知教授的授课可以授课的时间，寻求满足所有教授时间的排课课程表。 常见的一些约束满足问题有： 八皇后问题 图着色问题 填字游戏 数独 问题定义CSPs 包含以下三个要素： A set of variables（变量）, $X={X_{1},\\ldots ,X_{n}}$ A set of domains （值域） for each variable: $D={D_{1},\\ldots ,D_{n}}$ A set of constraints $C={C_{1},\\ldots ,C_{m}}$ （限制条件） that specify allowable combinations of values. Every constraint $C_{j}\\in C$ is in turn a pair $\\langle t_{j},R_{j}\\rangle$ , where $t_{j}\\subset X$ is a subset of k variables and $R_{j}$ is a k-ary relation (among k variables) on the corresponding subset of domains $D_{j}$. 问题的状态由对部分或者全部变量的定值（assignment）来确定： 如果定值不违反任何的限制条件，我们说他是无矛盾的（consistent） 如果定值包含了所有的变数，我们说他是完备的（complete） 如果定值是无矛盾的且完备的，我们说这个定值是一个解（solution），这样的定值就是 CSP 的解。 一个图着色问题的例子： 一个八皇后问题的例子： 一般来说 CSPs 使用的都是绝对约束，如果是非绝对的约束（偏好性），这样的问题称为约束优化问题（COP），在此不做讨论。 求解 CSPsCSPs 形式化的优点 快速消减庞大的搜索空间 发现某部分不是解迅速丢弃，直观看到哪一部分变量赋值违反约束。 CSP 具有可交换性（commutative） CSPs 中的局部相容性（consistency）在 CSPs 中，算法可以搜索，也可以做一种称作约束传播的推理。 推理的目的是用约束减小一个变量的合法取值范围。 可以把推理作为搜索前的预处理步骤。 核心思想是增强局部相容性，使不相容的结点取值被删除。 节点相容（Node-consistency）：来自节点本身的一元约束 弧相容（Arc-consistency）：某变量所有取值满足该变量所有的二元约束。 最流行的算法是 AC-3，维护一个弧相容队列。 从队列弹出一条弧，使其一个节点弧相容，如果其值域无变化，则处理下一条弧。 如果其值域发生变化，那么每个指向这个节点的弧必须重新插入队列准备检验。 路径相容（Path-consistency）：通过观察变量得到隐式约束并以此来加强二元约束。 比如说有三个连接节点，却只有两种颜色。 所有的 n-ary 约束都可以转换为 binary 约束。 具体解法：回溯搜索仍然使用搜索来求解，一般来说使用 Backtracking Search（BTS）回溯搜索。 用于深度优先之中，每次为一个变量选择一个赋值，当没有合法的值时就回溯。 由于可交换性，我们只用搜索组合而不是排列，所以叶节点个数至多为$d^n$个（不回溯的情况）。 在回溯搜索中，也需要考虑如下问题来对搜索进行改进： 下一步给哪个变量赋值？对于所选变量，选用怎样的赋值顺序？ 每步搜索应该进行怎样的推理？是否能预见失败？ 当我们搜索到某赋值违反约束时，搜索本身能避免重复这样的失败吗？ 1. 下一步给哪个变量赋值？对于所选变量，选用怎样的赋值顺序？选取变量： 最少剩余值启发式 Minimum Remaining Value (MRV)，选择合法取值最少的变量赋值。这样通过早期有效剪枝有助于最小化节点数。 对于第一个节点而言，最小剩余值相同，应该选用度启发式，选择与其他未赋值变量约束最多的变量来试图降低未来的分支因子。 选取赋值： 最小约束值 Least Constraining Value (LCV)，试图为剩余变量赋值留下最大的空间。这里只需要找到一个解，所以优先考虑最可能的值。 2. 每步搜索应该进行怎样的推理？是否能预见失败？搜索和推理应当交替进行。 推理的目的是减小值域，减小搜索空间。 当我们决定给某个变量某个值时，都有机会推理其邻接变量的值域空间。 最简单的形式是向前检验 Forward Checking（FC）。跟踪维护所有为选取变量的可能取值，当任何一个变量没有合法取值时结束。联合使用 MRV 和前向检验，很多问题的搜索将更有效。但是前向检验只使当前变量弧相容，却不向前看使其他变量弧相容。 MAC 维护弧相容，递归传播约束。对一个变量赋值后，使用 AC-3，从临接的未赋值变量开始进行约束传播，如果值域为空，则调用失败立即回溯。 3. 当我们搜索到某赋值违反约束时，搜索本身能避免重复这样的失败吗？ 简单的时序回溯。退回到上一个变量 冲突指导的回溯。退回到可能解决当前问题的变量（因为上一个变量可能无力于解决当前冲突）。构建一个冲突集，回溯到冲突集中时间最近的赋值。","link":"/artificial-intelligence/ai-csp/"},{"title":"人工智能中的逻辑","text":"将逻辑作为基于知识的 Agent 的一类通用表示，这样的 Agent 通过对信息的组合和再组合以适应各种用途。 本文一部分参考了这位同学的笔记。 Logics: formal languages for representing knowledge to extract conclusions 基于知识的 Agent 知识库（KB）：基于知识的 Agent 的核心部件。是一个“语句”集合。 语句（sentence）：用知识表示语言表达，表示了关于世界的某些断言。 公理：当某语句是直接给定而不是推导得到的时候，我们将其称为公理。 每次调用 Agent 程序，他做三件事： Agent TELL 知识库他所感知到的内容。 Agent ASK 知识库应该执行什么行动。 在此过程中可能会对于世界的当前状态，可能行动队列进行大量推理。 Agent TELL 知识库他采取的行动，并执行该行动。 逻辑知识与概念逻辑：一种形式语言，可以表示能得出结论的信息 语法（Syntax）：定义了语言中的句子 语义（Semantics）：定义了句子的意思，即语义定义了每个语句关于每个可能世界的真值 蕴涵（Entailment） 蕴涵意为一个语句逻辑上跟随另一个语句而出现：$KB |= \\alpha$ 知识库 KB 蕴涵句子 $\\alpha$ 当且仅当在 KB 为真的每个世界中，$\\alpha$也为真 蕴涵是句子间的关系，其基于语义。 例如，x=0 蕴含 xy=0。 逻辑推理（inference）： 如果推理算法 i 可以根据 KB 导出 $\\alpha$，我们表示为：$KB |-_{i} \\alpha$，读为“i 从 KB 导出 $\\alpha$” KB的所有推论集合是一个干草堆，α是针，蕴含=干草堆里的针，推理=找到它 对于推理算法 i： 可靠性 Sound：不会虚构事实，只导出语义蕴涵句。 完备性 Completeness：可以生成任一蕴涵句。 命题逻辑Syntax 语法： 原子语句：命题符号P1，P2等是句子，代表一个或为真或为假的命题 复合句： 如果S是一个句子，则┐S也是一个句子（negation 非，否定式） 如果S1和S2是句子，则S1∧S2是句子（conjunction 与，合取式） 如果S1和S2是句子，则S1∨S2是句子（disjunction 或，析取式） 如果S1和S2是句子，则S1=&gt;S2是句子（implication 蕴涵，蕴涵式） 如果S1和S2是句子，则S1&lt;=&gt;S2是句子（biconditional 当且仅当，双向蕴涵式） 语义： 定义了判定特定模型中语句真值的规则。 可以用真值表总结 其中=&gt;的真值表比较令人困惑。 不要用如果 P 那么 Q的思路来理解。 命题逻辑不要求 P/Q 间的相关性或因果关系。 以“如果 P 为真，那我主张 Q 为真，否则无可奉告”来理解。 前提为假的任意蕴含都为真。 该语句为假的唯一条件是 P 为真而 Q 为假。 算符具有一定的运算性质（逻辑等价）： Implication 的几形式变换，真值可能会发生变换： 推导和证明下列表示意为，给定任何形式的上方语句，就可以推导出下方语句： Modus Ponens: 假言推理原则 $\\frac{\\alpha \\Rightarrow \\beta, \\quad \\alpha}{\\beta}$ Modus Tollens: 假言推理原则 $\\frac{\\alpha \\Rightarrow \\beta, \\quad \\neg \\beta}{\\neg \\alpha}$ Addition: $\\frac{\\alpha}{\\alpha \\vee \\beta}$ Simplification/And-Elimination: $\\frac{\\alpha \\wedge \\beta}{\\beta}$ Disjunctive-syllogism: $\\frac{\\alpha \\vee \\beta, \\quad \\neg \\alpha}{\\beta}$ Hypothetical-syllogism: $\\frac{\\alpha \\Rightarrow \\beta, \\quad \\beta \\Rightarrow \\gamma}{\\alpha \\Rightarrow \\gamma}$ Search for proofs is a more efficient way than enumerating models: Truth tables have an exponential number of models. The idea of inference is to repeat applying inference rules to the KB. Inference is sound, but how about completeness? 如何来保证完备性？ Proof by resolution Forward or Backward chaining 归结（Resolution）类似于 Disjunctive-syllogism。 如果两个中必存在一个，而又不是第一个，则是第二个。 单元归结（Unit resolution） \\frac{\\ell_{1} \\vee \\cdots \\vee \\ell_{k} \\quad m}{\\ell_{1} \\vee \\cdots \\vee \\ell_{i-1} \\vee \\ell_{i+1} \\vee \\cdots \\vee \\ell_{k}}全归结（Full resolution） \\frac{\\ell_{1} \\vee \\cdots \\vee \\ell_{k} \\quad m_{1} \\vee \\cdots \\vee m_{n}}{\\ell_{1} \\vee \\cdots \\vee \\ell_{i-1} \\vee \\ell_{i+1} \\vee \\cdots \\vee \\ell_{k} \\vee m_{1} \\vee \\cdots \\vee m_{j-1} \\vee m_{j+1} \\vee \\cdots \\vee m_{n}}其中 $\\ell_{i}$ 和 $m_{j}$ 是互补文字。 合取范式（CNF）以子句的合取式表达的语句被称为合取范式或者 CNF， 合取式不易阅读，但其将成为归结过程的输入： 消去等价词 消去蕴含次 否定词只出现在文字前边（而不是括号前面） 归结算法为了证明 $KB |= \\alpha$，需要证明 $(KB \\wedge \\neg\\alpha)$是不可满足的，通过推倒矛盾来证明。 先将 $(KB \\wedge \\neg\\alpha)$ 转化为 CNF。 对子句运用归结规则，产生新子句，如果其尚未出现过，则将其加入子句集，直到： 没有可以添加的新语句。 两个子句归结出空子句，等价于 False。（函数返回 True） 基本归结定理：如果子句集是不可满足的，那么这些子句的归结闭包包含空子句。 Forward or Backward chaining前向链接 Forward Chaining： 判断单个命题词是否被限定子句的知识库所蕴含。 这个算法运行的时间是线性的。 data-driven 反向链接 Backward Chaining： 从查询开始进行推理，如果查询为真则停止，否则，从知识库寻找以 q 为结论的蕴含式，如果前提都为真，则为真。 这个算法运行的时间也是线性的。 goal-driven DPLL Algorithm Check the satisfiablity of a sentence in propositional logic. 类似 backtracking 但是运用了很多启发式的技术，例如早停、纯符号启发式、单元子句启发式等。 一阶逻辑 First Order Logic（FOL）可以作为 Propositional Logic 的替代。 （命题逻辑表达能力很弱。） 总结 Summary","link":"/artificial-intelligence/ai-logic/"},{"title":"人工智能的中的搜索","text":"人工智能中关于搜索的一些最基础的知识。 AI 中的搜索对于一个 goal-based agent，搜索是使其找到一个动作或者一系列动作来达到目标。 有很多例子，比如走迷宫，寻路问题，8-queen 问题等。 一般来说包含树搜索和图搜索等。 对于考虑路径的搜索而言评估策略需考虑以下四个维度： Completeness: Does it always find a solution if it exists? Time complexity: # nodes generated/expanded. Space complexity: maximum # nodes in memory. Optimality: Does it always find the least-cost solution? 存在两类搜索策略： Uniformed Breadth-first search (BFS): Expand shallowest node Depth-first search (DFS): Expand deepest node Depth-limited search (DLS): Depth first with depth limit Iterative-deepening search (IDS): DLS with increasing limit Uniform-cost search (UCS): Expand least cost node (the cost could be the length between nodes) Informed Greedy best-first search: Expand the node that appears to be closest to goal A* search: Minimize the total estimated solution cost (to middle node + node to goal；f=g+h). BFS mode. IDA: IDS + A. DLS mode. The cost of space is lower than A*. 对于 A* 中启发式策略而言（h）： A good heuristic must be admissible. An admissible heuristic never overestimates the cost to reach the goal, that is it is optimistic For admissible $h_1$ and $h_2$, if $h_1$(s) ≥ $h_2$(s) for ∀𝑠 ⇒ $h_1$ dominates $h_2$ and is more efficient for search. UCS vs Greedy Best First vs A*: UCS：f(n) = g(n) Greedy Best First: f(n) = h(n) A*: f(n)=g(n)+h(n) 对于不考虑路径的搜索Local search: the path doesn’t matter Hill climbing Genetic algorithms Simulated Annealing: Given a chance to jump out the local minimum","link":"/artificial-intelligence/ai-search/"},{"title":"关于“我”","text":"一些关于“我”的事情。 或许都是一些个人喜好以及一些浅层的标签。 上世纪九十年代中期人。 处女座（上升狮子）+ ENTP-A（同时也和 INTP 反复横跳）。 爱好🏀📸️。 拖延症（在改）。 有些自我（在改，学习换位思考）。 有一只猫。","link":"/blog-general-info/about-me/"},{"title":"友情链接","text":"Yu Zhang 偷窥俺的师兄 Metaron 偷窥俺的师弟","link":"/blog-general-info/friends/"},{"title":"有幸拍到的照片","text":"一些我觉得十分庆幸能拍到的照片，或有趣，或好看。 2022 年 2021 年 2020 年 2019 年 2018 年 2017 年 2016 年","link":"/blog-general-info/gallery/"},{"title":"一个简单的留言板","text":"有什么想对超级Rui说的话就写下来吧！","link":"/blog-general-info/message-board/"},{"title":"讲座及研讨班记录","text":"本篇Blog用于记录参加的讲座，不定期更新。 搜索引擎的技术趋势和精准度提高【常毅】 主讲人：常毅教授，吉林大学人工智能学院院长 日期：2020/11/23 1. Introduction搜索引擎架构： 网页爬虫 倒排索引（word &gt; document 拿空间换时间的一个过程） 网页检索&amp;网页排序（0.2s内） PageRank 算法： Google最先提出的一种网页排名算法。 大众一般误认为google精准是仅仅是因为PageRank这一项技术，事实上是因为其很多黑科技的结合。 搜索引擎进化史： 1994-1998 Syntactic matching关键字匹配 1998-2006 PageRank，外加利用指向信息，点击信息等 2006-2014 垂直搜索，知识图谱 2014～ mobile search，私人助手，聊天机器人等 搜索引擎后发劣势（追赶者很难超越）：用户数据积累较少导致效果较差。 深度学习在搜索引擎里的优势远远小于我们所期待的（至少在2016年）。 2. Web search ranking review排序问题的变换： pointwise：退化为回归问题，用gradient boosting类回归 pairwise：排错对数越来越少 listwise 3. Yahoo web search ranking practice简要介绍了2015年KDD best paper的工作。 这是一个系统性的工作，并不仅仅单一的提出了一个算法。 Practical challenges： avoid ugly result on the top Semantic gap between query and document Search queues follows a long tail distributions 3.1. 排序学习的算法2010 Yahoo learning rank challenge中前十名都是使用tree-based算法。 2010年得出的结果中lambdaMart &gt; logisticRank。 2015年KDD best paper中logisticRank则好于lambdaMart和2010年结果相悖。 根据2015年的结果总结得出造成这一现象的原因是以前的community都或多或少忽略了”over 99% query-url pairs are bad”这一现象。 3.2. 查询改写的方法机器翻译需要平行语料库，需要以用户反馈来学习查询语料对训练翻译模型。 举个例子： Tesla Price =&gt; How much is a Tesla 使用用户查询关键字与点击情况进行匹配。 3.3. Click similarity feature当前存在很多feature types，如图所示： Click similarity feature是一种特征提取的方法。 通过二分图对Query进行特征构建 比deep learning效果好。 在特征提取的问题上DL不会dominant，为什么？ Web search ranking is an ‘easy’ and well studies task -李航 DL更适用数据初识表示和解决问题的合适表示相距甚远时 -周志华 DL适用于信息complete的时候，WSR中这一条件并不总是成立。","link":"/computer-science-engineering/seminar-talk/"},{"title":"排序算法","text":"最近看了一篇比较有趣的文章 《比冒泡算法还简单的排序算法：看起来满是bug的程序，居然是对的》，看了半天才看懂。 发现自己其实已经不太记得各种排序方法了，此处做一个复习总结。 （给自己看的笔记也不会很详细，毕竟大部分也都实现过，这里主要记录intuition。） 主要参考了这篇知乎的博文，《八大经典排序算法详解》。 鸽巢排序 基本思想：空间换时间，建立一个足够长的数组，按照数值的大小放到相应的位置，最后遍历删除空位置。 时间复杂度为 O(n)。但是如果需要排序的数中有特别大的，则数组需要建立的特别大，不方便使用。 桶排序上述鸽巢排序可以看作一种极端的（桶数很多的）桶排序： 基本思想：空间换时间，将待排序的序列分到若干个桶中（桶具有顺序），每个桶内的元素再进行基于比较的排序。 时间复杂度为 O(k+n)，k 为最大值。 是稳定的排序方法。 限制比较多：只能int，空间消耗大，只有对于均匀数列效果好 插入排序 基本思想：将待插入样本按其值的大小插入前面已经排序的文件中适当位置上，直到全部插入完为止。 时间复杂度为 O(n^2)。 是稳定的排序方法。 快速排序 基本思想：在待排序的元素任取一个元素作为基准(通常选第一个元素，称为基准元素），将待排序的元素进行分块，比基准元素大的元素移动到基准元素的右侧，比基准元素小的移动到作左侧，从而一趟排序过程，就可以锁定基准元素的最终位置，对左右两个分块重复以上步骤直到所有元素都是有序的（递归过程）。 快速排序平均时间复杂度为 O(nlogn)，最坏情况为 O(n^2)，n越大，速度越快。 不是稳定的排序算法。 选择排序 基本思想：每一次从待排序的数据元素中选出最小的一个元素，存放在序列的起始位置，直到全部待排序的数据元素排完。 时间复杂度 O(n^2)。 选择排序是不稳定的排序方法。（存在不相邻元素的互换） 希尔排序 基本思想：设置一定的步长序列 [a,b,c,1]（最后一个需要为1），然后根据步长来构建分组（以 a 为例，每个分组中都是相隔为a的一组样本：i, i+a, i+2a…），然后对分组里的样本进行排序，之后合并为新序列。a 步长进行完之后，在新的序列上使用下一个步长 b 得到下一个序列，如此往复。最后一个步长需要为1。 时间复杂度 平均时间 O(nlogn) 最差时间O(n^2) 是不稳定的排序方法。 堆排序 基本思想：利用最大堆或最小堆，先将所有值放入堆中储存，再一一取出，则排好序。 堆排序是一种选择排序,其时间复杂度为 O(nlogn)。堆排序是不稳定的 归并排序 基本思想：将待排序的数组分成前后两个部分，再递归的将前半部分数据和后半部分的数据各自归并排序，得到的两部分数据，然后使用merge合并算法将两部分数据合并到一起。 最好、最坏和平均时间复杂度都是 O(nlogn)，空间复杂度是 O(n)。 是稳定的排序算法。 冒泡排序 基本思想：持续比较相邻的元素。如果第一个比第二个大，就交换他们两个。直到没有任何一对数字需要比较。 冒泡排序最好的时间复杂度为 O(n)，冒泡排序的最坏时间复杂度为 O(n^2)，因为循环轮数不确定。因此冒泡排序总的平均时间复杂度为 O(n^2)。 算法适用于少量数据的排序。 是稳定的排序方法。 1234for i = 1 to n dofor j = i + 1 to n doif A[i] &gt; A[j] thenswap A[i] and A[j] Bug 满满的“数组升序排序”这个方法第一次见于《比冒泡算法还简单的排序算法：看起来满是bug的程序，居然是对的》。 伪代码如下。 这个代码相对于冒泡排序看起来有两个地方写错了，一个是判断时的小于号，一个是第二个 for 循环中的开始项。 但是这个算法竟然可以惊讶地得到正确的结果。 1234for i = 1 to n dofor j = 1 to n doif A[i] &lt; A[j] thenswap A[i] and A[j]","link":"/computer-science-engineering/sorting/"},{"title":"有趣的事物","text":"一些有趣的东西。 Computer Science without a computer","link":"/interesting-stuff/interesting-stuff/"},{"title":"摸爬滚打","text":"增强对这个世界各种属性的认识。 Archive 人和人的体质不能一概而论。 eg. 本科的同学们发了很多顶会文章。 重要的个人利益不能放弃，如果可以放弃的话，自由一定会导致强者对弱者的剥削。这就是为什么我们的伦理道德认为一个人是不能处分自己最重要的利益的，因为自由不能以彻底放弃自由为代价。 from 罗翔 eg. 代孕 很多复杂问题是更高维度简单问题的投影，比如说打篮球动作变形、速度慢、配合差是很多时候体力不行；写程序烂、bug 多、时间长是抽象分解问题做的不好 发展不是物的堆积，发展是让人的能力不断提升。 from 翟东升 成功的道路并不像想象得那么拥挤，在人生的马拉松长路上，绝大部分人跑不到一半就主动退下来了。到后来，剩下的少数人不是嫌竞争对手太多，而是发愁怎样找一个同伴陪自己一同跑下去。因此，教育是一辈子的事情，笑到最后的是一辈子接受教育的人。 from 吴军博士，腾讯前副总裁。这里是否为最开始的出处存疑。 虽然听起来是一句鸡汤文，但是还是体现了持续学习的重要性。 有两点感受： 找一个志同道合能一起进步的人十分重要。 持续的学习进步，一点一滴的成长十分关键。请尽量做一些收益可以累计的事情。可以参考《收益值与半衰期》这篇文章。 克制自己的欲望，特别是当它伪装成一种积极的形态出现时；克制自己的傲慢，特别是当它伪装成一种虚幻的正义感出现时。 from 张列弛《冷冬》 可以看作是对自己之前“熬夜是对生活仍有期待”观点的一种驳斥。 让子弹飞一会。 在水落石出前永远不要急于站队，翻转打脸甚至正邪之间的切换可能猝不及防。 阿里女员工强奸事件有感。 让学 yyds。","link":"/knowledge-from-growth/mud-down/"},{"title":"自我提升","text":"一些平时总是注意不到但是应该要注意的东西。 阅读 阅读时要寻找作者的观点（能代表这一篇文章的东西），不要总是无重点的阅读，否则较难形成知识或经验。 表达方式 陈述观点或者描述事物时先简要说结论，再重新以背景、问题、分析、归纳的顺序铺展开来补充。 在打断对方并对自己观点加以补充时需要注意（这种情况一般是发生于你认为对方没有听明白）。 首先你并不清楚是否对方理解了你的观点，可能需要听完对方的表述。 如果对方理解，你加以打断，或是一种自大表现。 如果对方不理解，完全可以等他说完，再补充表述。 如果听完还是不清楚对方是否理解，则可以寻求对方复述一遍。大部分人可能不会介意重复一遍自己的理解，但是介意对方认为自己不理解对方的观点。 思维方式 避免想当然。 一点不要含糊，含糊代表着侥幸、代表着自我欺骗、代表着自我感觉飘然。","link":"/knowledge-from-growth/self-improvement/"},{"title":"糖盐","text":"记录一些内容并未不适合公开的糖盐。 个人觉得还比较有趣，亦不免寓教于乐。 Archive “虚无的充实感” 描述忙活了半天却不知道做出什么进展但却心满意足的样子。 个人认为旨在期望你清楚地认识到自己的进步在哪里，或者说到底有没有进步。 eg. “埋头写了1000行代码，好充实的一天。” “那你/我还不如去买彩票呢” 描述一种期望可能性很小的状态，一般会有上一句作为铺垫。 个人认为旨在期望你认识到此事的荒谬。 eg. “那我要只期望我的学生XXX（此处为动词），那我还不如去买彩票呢。” “如果我没记错的话” 表示我没记错，一般用于展开陈述的开头。 个人认为旨在加强语气，但是留出弹性空间，同时引出需要强调的陈述。 eg. “如果我没记错，现在在校的同学，应该都是申请过提前返校的同学。”","link":"/knowledge-from-growth/sugar-salt/"},{"title":"Attention 和 Transformer","text":"注意力机制和 Transformer 在神经网络中已经取得了良好的表现，此处做一个简要的学习。 因为在自己的工作中并不会用到，所以此处可能更注重一些逻辑上的思路，以加强直观上的理解，具体细节有需要的时候再进行补充。 Attention 机制是什么？Attention 最初面对的是长程梯度消失以及中间隐变量信息含量有限的问题。 其通过将输入中的样本信息指导输出过程，来解决这一问题。 其有着参数少速度快效果好的优点。 Attention 做的事情简单来说就是“加权求和”： 对什么加权？对 feature 或者一系列输入信息加权。 权重是多少？权重为一个函数，而不是固定值。（若为固定值的话则可视为全连接层了。）一般来说这个权重即为 attention 分布，基于输入位置和输出位置的关联性。 求和得到什么？所需的输出。 Attention 定义以下是 “Attention Is All You Need”【4】这篇重磅论文中给出的定义。 这个定义虽说是针对于他们的 scaled dot-product attention，但是整体来说整个 attention 都是一个思路，所以可以拿来作为定义。 \\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V即通过关系矩阵 $Q K^T$ 归一化得到的概率分布 $\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right)$ 对 $V$ 进行重采样。 当然这个 $QKV$ 在不同场景下可能有着不同的具体含义。 Multi-head attention如下图所示。 简单来说就是将多个 scaled dot-product attention 的值拼接，再通过线形结合输出。 具体的， \\operatorname{MultiHead}(Q, K, V)=\\operatorname{Concat}\\left(\\operatorname{head}_1, \\ldots\\right., head \\left._{\\mathrm{h}}\\right) W^O \\\\ where \\quad \\text{head}_i=\\operatorname{Attention}\\left(Q W_i^Q, K W_i^K, V W_i^V\\right)Transformer其模型结构如下图所示。 Multi-head attention 是其主要组成部分。 在 attention 之外，样本的位置（顺序）信息使用 positional encoding 嵌入（因为不像 RNN 有顺序结构）。 Reference 深度学习中Attention与全连接层的区别何在？ - SleepyBag的回答 - 知乎 Attention机制详解（一）——Seq2Seq中的Attention 目前主流的attention方法都有哪些？ - 电光幻影炼金术的回答 - 知乎 Vaswani, Ashish, et al. “Attention is all you need.” Advances in neural information processing systems 30 (2017).","link":"/machine-learning/attention/"},{"title":"Batch Normalization","text":"此处记录对于 Batch Normalization 的学习。 神经网络中存在的 ICS 问题深度学习中存在 Internal Covariate Shift (ICS) 的问题。 类比迁移学习中的 Covariate Shift 指源领域和目标领域数据 marginal distribution 的偏移。 这里 ICS 指神经网络中，由于参数的变化，引起的每一层输出分布的差异变化，换句话说之后下一层的输入在参数变化后，可能基于了另一个分布。 所以下一层在训练过程中就需要不断的去适应这种变化。 于是带来了以下问题： 后面一层的参数需要适应不断变化的分布，训练效率降低。 对于饱和非线性激活函数，例如 sigmoid 和 tanh，容易落入饱和区。饱和区是指由于输入值极度偏离0点，导致梯度计算接近于0，从而难以起到学习效果。 如何解决 ICS 问题此类由于分布变化带来不良影响的问题，最 naive 的解决方式一般来说都是对分布进行限制。 就比如在迁移学习中，会将源域和目标域的样本来做一个统一（减小分布差异）。 在 ICS 问题中，也可以对分布进行限制。 最初，白化（whitening）被提出，一般来说采用 PCA 或者 ZCA 的方法，使所有特征分布均值为0，方差为1（PCA）或相同（ZCA）。 白话的目的是去除数据特征之间相关性（独立），同时使其具有相同均值和方差（同分布）。 这样每一层网络的输入分布被固定，加速网络收敛。 但是白化也存在一些问题： 计算成本高 改变数据表达能力，一些参数信息会被丢失 不可微，难以通过反向传播训练 所以说我们期望有一种计算代价低廉，且能使标准化的数据尽可能保有表达能力的方法。 这就是 Batch Normalization 提出的背景。 什么是 Batch Normalization主要思路： 既然白化计算过程比较复杂，那我们就放松限制一点，尝试只单独对每个特征进行标准化，使其均值为0，方差为1。 既然类白化操作减弱了网络中每一层输入数据表达能力，那再加入线性变换操作，让这些数据再能够尽可能恢复本身的表达能力，使其不因规范化而下降。 通用变换框架如下所示，包含两次平移和伸缩变换，在使用 BN 之后，每层神经元输入的样本的均值仅由 $\\boldsymbol{b}$ 决定，而不像之前由前面一层神经网络复杂的参数决定： h=f\\left(\\boldsymbol{g}\\cdot\\frac{\\boldsymbol{x}-\\boldsymbol{\\mu}}{\\boldsymbol{\\sigma}}+\\boldsymbol{b}\\right)\\\\但是好像这个第二次的仿射变换在理论上是否有用，需不需要用还有争议，日后可以再仔细看一下。（挖坑） 同时 BN 有一些变体，在这里不展开了： 纵向规范化（最基础） 横向规范化 参数规范化 余弦规范化 如何使用 Batch Normalization适用场景： 每个 mini-batch 比较大，数据分布比较接近。 在进行训练之前，要做好充分的 shuffle， 否则效果会差很多。 因此不适用于 动态的网络结构 和 RNN 网络 测试阶段： 保留训练时每一个 batch 的统计量 $\\mu_{batch}$ 和 $\\sigma^2_{batch}$。 使用整个样本的统计量来对Test数据进行归一化，具体来说使用均值与方差的无偏估计。 $\\mu_{test}=\\mathbb{E} (\\mu_{batch})$ $\\sigma^2_{test}=\\frac{m}{m-1}\\mathbb{E}(\\sigma^2_{batch})$ 构建阶段： 置于 Conv 层或全连接层之后 对于饱和非线性激活函数而言，BN 层需要放到 activation 之前。Dropout 则应当置于 activation layer 之后. 对于 ReLU 而言，目前并没有定论，不管是实验还是理论争论都比较多，目前看来 BN 放在 ReLU 之后可能表现更好，但是放在 ReLU 前的可能更多一些（BN 原论文是放在了前面）。（大误） 在 Pytorch 中的 Batch Normalization在 Pytorch 的实现中，BN 也包含两次平移和伸缩变换，其中第二次变换可以通过调整仿射变换参数 affine=True 选择是否打开。 这里可能也体现了这个仿射变换是否有必要的争议。 Reference 详解深度学习中的Normalization，BN/LN/WN - Juliuszh的文章 - 知乎 Batch Normalization原理与实战 - 天雨粟的文章 - 知乎 Batch Normalization 和激活函数的使用顺序是什么，神经元的饱和指的又是什么？ - 无双谱的回答 - 知乎 https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html","link":"/machine-learning/batch-normalization/"},{"title":"深度神经网络模型复杂度","text":"深度学习计算量/复杂度相关知识。 个人在此方面的知识较少，导致上周高性能计算的讲座好多都没有听懂，在此做一个学习。 一些基础概念首先介绍一些概念： FLOPS：floating point operations per second 的缩写，意指每秒浮点运算次数，理解为计算速度，是衡量硬件性能的指标。（常与 FLOPs 混淆） FLOPs：floating point operations 的缩写，意指浮点运算数，理解为计算量，可以用来衡量算法/模型的复杂度。 MACs：multiply–accumulate operations，意为乘加计算数，通常与 FLOPs 存在一个两倍的关系。 关于什么是浮点数可以看这篇文章【2】，个人觉得深入浅出写的比较清晰。 如何计算其实就是计算有多少次运算，对于不同的模型结构计算数目不同。 此处不涉及具体公式，仅介绍 FLOPs 计算思路，MACs（又乘又加）的话通常为 FLOPs 的两倍左右。 卷积层：在卷积层参数数目的基础上乘以输出的 feature map 大小（输出的每一个 feature 都是通过一次卷积层得到的）。 全联接层：即为全联接层参数数目。 激活层：不同激活函数的计算量不同。通常来说不重点计算。 Reference CNN 模型所需的计算力（flops）和参数（parameters）数量是怎么计算的？ - 泓宇的回答 浮点数浮点型到底是什么，能说的简单一点吗，就是用高中生能理解的语言。它们的作用是什么？ - 木木的回答 CNN的参数量、计算量（FLOPs、MACs）与运行速度","link":"/machine-learning/computational-complexity/"},{"title":"Contrastive Learning 学习记录","text":"之前总是会断断续续看到一些自监督学习的工作/想法。 同时也总是看到对比学习这个词，不明所以，所以对此进行一个简单的学习。 本文可以看作对《对比学习（Contrastive Learning）:研究进展精要》这片知乎文章的阅读笔记。 背景自监督学习有着从无标记样本中学习表征的能力。 比较出名的是自然语言模型 Bert 的预训练。 对比学习是一种自监督学习的方式，主要用在图像领域。 目前，对比学习貌似处于无明确定义、有指导原则的状态，它的指导原则是：通过自动构造相似实例和不相似实例，要求习得一个表示学习模型，通过这个模型，使得相似的实例在投影空间中比较接近，而不相似的实例在投影空间中距离比较远。 对比学习的关键点： 如何构造相似实例，以及不相似实例 如何构造能够遵循上述指导原则的表示学习模型结构 以及如何防止模型坍塌(Model Collapse) 目前的对比学习方法分类 基于负例的对比学习方法 基于对比聚类的方法 基于不对称网络结构的方法 基于冗余消除损失函数的方法 几种分类基于负例：SimCLR首先我们需要构建正例和负例。 负例的话，同一个 batch 的其他样本可以当作此样本的负例。 正例的话一般通过图像增强来实现，如下图所示。 模型层面，SimCLR 具有上下两个 Branch，通过学到的表征来计算 Similarity。 需要注意的是，相似度计算需要正则化。 其中的相似度损失使用 infoNCE，其中$τ$为温度参数： L_i=-log (exp(S(z_i,z_i^+ )⁄τ)/∑_{(j=0)}^Kexp(S(z_i,z_j )⁄τ) )温度接近0的时候，该损失基本退化为 Triplet（对很相似样本的区分）。 为什么 SimCLR 投影操作要做两次非线性变换，而不是直接在Encoder后，只经过一次变换？ 在 Moco 中并没有 Projector，在 SimCLR 加入 projector 后效果提升明显。 SimCLR 论文中表示，Encoder后的特征表示，会有更多包含图像增强信息在内的细节特征，而这些细节信息经过Projector后，很多被过滤掉了。 基于负例：Moco V2 其有以下特点： 相比于 Moco 增加了 projector。 相比于 SimCLR，从整个未标记数据集选取负例。 上分枝反向传播，下分枝使用移动平均机制更新参数。 基于对比聚类：SwAV通过上分枝预测下分枝打出的类别伪标记（由聚类得来），同时也要用下分枝预测上分枝。 具体损失函数采用 $z_i$ 和聚类 Prototype 中每个类中心向量的交叉熵表示。 基于不对称结构：BYOL Target 分枝结构类似 Moco V2对应下分枝的 Moving Average 动量更新方式。 Predictor 的存在保证了模型不坍缩（具体为何没有定论）。 基于冗余消除：Barlow Twins Barlow Twins 并没有去除向量的长度因素，它在Batch维度，对 Aug1 和 Aug2 里的正例分别做了类似 BN 的正则。 之后，顺着 Batch 维，对 Aug1 和 Aug2 两个正例表示矩阵做矩阵乘法，求出两者的互相关性矩阵（cross-correlation matrix），其损失函数定义在这个互相关矩阵$C$上。 希望互相关矩阵为对角元素为1的单位矩阵，增强元素间的独立性。 现状与展望现状BYOL，SwAV，DeepCluster-v2 都在 many-shot（ImageNet上学，再迁移到其他数据集） 和 few-shot 上取得了比较好的表现。 其中在 many-shot 上甚至超过有监督。 数据来源于这篇论文，“How Well Do Self-Supervised Models Transfer?”。 问题 数据偏置问题：ImageNet 相对于普通网上能获取的数据还是太干净了。 正例构建问题：除了图像遮挡不变性和颜色不变性，对于其它的常见不变性，比如视角不变性、照明不变性等，对比学习模型的效果要明显弱于监督学习。 对于复杂任务缺乏像素级学习能力：当下都是采用判别模型。 我的看法其实从2019年就已经陆陆续续见过很多自监督学习的工作了，一直也没有静下心来学习一下。 这次通过对这篇介绍性博文的学习，加上之前一些自己的见闻，也算是对这个自监督学习方向有了一个粗浅的理解。 总的来说，自监督学习是希望在完全无标记的样本集中学习一个富有信息的表征。 由于无标记，所以在学习表征的过程中，我们需要创建一些辅助任务来帮助学习表征。 这些辅助任务包括但不限于： 某种填空（类似 bert 的预训练，图像中的补全） 某种相似度量（对比学习） etc. 所以自监督学习问题的关键其实在于如何构建这个辅助任务： 从当下的了解来看，判别式的方法是主流，都是通过生成正负样本来构建辅助任务。 在我的角度看，知乎这篇文章的分类，除了 BYOL 之外，还都是通过正负样本的判别来实现的，只是如何判别略有不同，但还是在一个范式下面。 这种基于判别的辅助任务其实是很简单的（同时在简单的分类问题上起到了很好的表现），但这种基于简单任务学到的表征在复杂任务上的适用性还不清楚（或许不太好，文章中提到的“像素级构建能力”）。 或许设计更加复杂但是可行的辅助任务，能提取到更为合适且富含信息的表征。 与其他领域的关系： 背景上：自监督学习的出现背景还是存在大量未标记数据，其实也是在标记数据不足这个局限性下做的尝试。 目的上：与其他研究领域尝试学模型不同，自监督学习的主要目的是在学表征。 总的来说，我第一次见到这类方法的时候就觉得很直观且很巧妙，经过近年的发展，它的可用性已经得到了验证，个人认为在表征学习的这个道路上，自监督学习还是比较靠谱的。 但是放到具体的下游任务上，面临标记数据的匮乏时，用主动学习选取标注才是合理之选。 或许将来的可以实用的大模型都会包含一个离线的自监督学习表征模块，和一个在线的主动学习模块。 这样既可以缓解主动学习在深度表征上的乏力，也可以高效的在表征确定的时候找到目标模型。","link":"/machine-learning/contrastive-learning/"},{"title":"距离矩阵","text":"在看 k-center 的时候发现自己对其距离矩阵的计算不是很清楚，于是在此记录。 距离矩阵的计算及 Python 实现给定 $m\\times n$ 矩阵 $X$, $X = [x_1, x_2,…,x_n]$， 这里第 $i$ 列向量 $x_i$ 是 $m$ 维向量，任务是计算出一个 $n\\times n$ 矩阵，使得： $D_{ij}=||x_i-x_j||^2$。 具体的，$D_{ij} = (x_i - x_j)^T(x_i-x_j)=x^T_ix_i-2x^T_ix_j+x^T_jx_j$。 用 Gram Matrix 表示，$D_{ij}=G_{ii}-2G_{ij}+G_{jj}$。 放在矩阵的尺度来看，$D = H + K -2G$。 其中，$H_{ij} = G_{ii}, K_{ij} = G_{jj}$。 根据最后一个式子，计算的时候可以避免循环： 12345def compute_dist_matrix(X): m,n = X.shape G = np.dot(X.T, X) H = np.tile(np.diag(G), (n, 1)) # Construct an array by repeating the number of times. return H + H.T - 2*G Reference: 斯坦福CS231N课程笔记（三）-距离矩阵的计算方法","link":"/machine-learning/distance-matrix/"},{"title":"信息论以及机器学习中的熵","text":"这里对机器学习中常用到的信息论知识点做一个总结。 第四次更新本文，每次都能发现之前的理解有疏漏和不清晰的地方。 这次更新本文为两部分，信息论中的熵 &amp; 机器学习中的熵，主要包含相关的定义和何如理解这些概念。 信息论中的熵 — 编码的视角信息编码所谓信息编码，就是在信息传递时，进行的必要转化。 一般情况下，以比特传输信息。 为了使信息高效传播，高概率高频次传递的信息将以更短的编码展现，而较少发生的低概率信息编码则会更长，一个具体的例子是霍夫曼编码。 在这个背景下，某一个事件 $x$ 的编码长度则会与其概率 $p(x)$ 成负相关，一般来说至少需要 $\\log _{2}\\left({p(x)}\\right)$ 个比特来对其进行编码。 信息熵对于一个随机变量 $X$ 进行编码，考虑到每个事件 $x$ 出现的概率 $p(x)$ 和编码长度 $\\log \\left({p(x)}\\right)$，对随机变量的编码可以由对每个事件编码的期望得到，这个期望就是信息熵： H(X)=\\mathbb{E}_{X}[I(x)]= - \\sum_{x \\in X} p(x) \\log {p(x)}在信息编码中，当我们用少于信息熵长度的比特编码时，一定有资讯的损失，所以信息熵被称为理论最优编码长度。 对于随机变量 $X$ 而言，信息熵是其不确定性的度量，也可以看作是信息量的期望： 当分布越平均，事件出现概率都相似，采样不确定性更大，杂乱信息多，信息熵越大。 当分布越不均，事件出现概率大不同，采样不确定性更小，杂乱信息少，信息熵越小。 交叉墒我们已知每一个事件 $x$ 在一个已知的概率分布 $Q$ 下相应的编码长度为 $\\log \\left({q(x)}\\right)$，当我们用这个编码对一个真实的（预先未知的）概率分布 $P$ 进行表示时，可以得到在这个新分布下期望的编码长度，这个期望就是交叉熵： H(p, q)=-\\sum_{x \\in X} p(x) \\log q(x)说人话就是使用基于分布 $Q$ 的编码表来编码服从分布 $P$ 的样本所需的平均编码长度。 这个平均长度一定是不小于使用基于分布 $P$ 本身的编码表的，即$H(p, q) \\geq H(p)$。 相对熵对于分布 $P$ 和 $Q$，已知其交叉熵，可以得到 $P$ 相对于 $Q$ 的相对熵： D_{\\mathrm{KL}}(p \\| q) = H(p, q) - H(p) =-\\sum_{x} p(x) \\log \\frac{q(x)}{p(x)}相对熵又称为 KL 散度，是两个概率分布 $P$ 和 $Q$ 差别的一种非对称性度量。 其表示使用基于分布 $Q$ 的编码表来编码服从分布 $P$ 的样本（相比起用 $P$ 自己的分布来编码）所需的额外的平均比特数。 联合熵Joint entropy，联合分布的熵。 那么对应的 $H(X)$ 和 $H(Y)$ 可被称作边缘熵。 H(X, Y)=\\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x, y) \\log \\left(\\frac{1}{p(x, y)}\\right)条件熵Conditional entropy，以条件概率计算的熵。 H(Y \\mid X)=\\sum_{x \\in \\mathcal{X}} \\sum_{y \\in \\mathcal{Y}} p(x, y) \\log \\left(\\frac{1}{p(y \\mid x)}\\right)由贝叶斯定理 $p(x, y)=p(y \\mid x) p(x)$ 可得条件熵和联合熵关系式： H(X, Y)=H(X)+H(Y \\mid X)=H(Y)+H(X \\mid Y)=H(Y, X)互信息Mutual information 指两个集合之间的相关性，定义是随机变量 $(X,Y)$ 的联合分布与 $X$ 和 $Y$ 的边缘分布的乘积之间的差异，其具有对称性。 I(X ; Y)=D_{\\mathrm{KL}}(p(x, y) \\| p(x) \\otimes p(y))其可以理解为在获得一个随机变量的信息之后，观察另一个随机变量所获得的“信息量”（单位通常为比特）。 也可以理解为不确定性的减少量，即 $Y$ 中包含多少 $X$ 中的信息。 I(X ; Y)=H(X)-H(X \\mid Y)=H(X)+H(Y)-H(X, Y)=H(Y)-H(Y \\mid X)=I(Y ; X) 机器学习中的熵 — 应用与实现作为损失函数使用的交叉墒机器学习中，设计损失函数的思想就是考虑我们预测分布与真实分布的差异。 在这个情况下，相对熵是一个不错的选择。 同时由于相对熵中 $H(p)$ 是一个固定值并不重要，所以我们可以直接使用交叉熵作为神经网络的损失函数。 其形式为， loss = -\\sum_{i=1}^{n} y_i \\log(\\hat{y}_{i})其中 $y_i$ 是第 $i$ 个样本的真实标记（参考 $p$ 来理解）。 $\\hat{y}_{i}$ 是第 $i$ 个样本在不同类别上的概率，神经网络中这个概率常常使用 softmax 函数，或者 sigmoid 函数得到。 Pytorch 中实现在 pytorch 中有两种方式实现交叉熵损失。 可以参考之前的这篇笔记。 可以调用NLLLoss()或CrossEntropyLoss()两个函数。 两函数的不同点主要在于输入不同： 最后一层全连接层的输出可以直接调用CrossEntropyLoss() 对于NLLLoss()，要将最后一层全连接层的输出再通过一次LogSoftmax()计算才能调用。 同时如果类别不平衡也可以在每一个类别上加上相应的权重。 交叉熵的求导因为之前要的工作要手动计算损失函数反向传播时对最后一层参数的梯度，所以需要对交叉熵损失求导。 此处我们只需要考虑对最后一层全连接层的输出求导即可，对参数可以之后再进一步求导。 先考虑一下正向的过程（在 n 个类别下使用 softmax 的情况），对于一个样本在最后一层全连接层的输出 $X = [x_1,…,x_n]$，我们考虑第 $i$ 个类， x_i \\xrightarrow[i_{th} output]{LogSoftmax} - \\ln (\\frac{\\exp (x_{i})}{\\sum_{j=1}^{n} \\exp (x_{j})}) \\xrightarrow[]{Y} - y_i \\ln (\\frac{\\exp (x_{i})}{\\sum_{j=1}^{n} \\exp (x_{j})})当考虑所有类别时，一般是将最右端这一项加权平均或加权求和。 此处我们假设权重相等，对所有类别相加，则可以得到总损失。 loss = - \\sum_{i=1}^{n}y_i \\ln (\\frac{\\exp (x_{i})}{\\sum_{j=1}^{n} \\exp (x_{j})}) = - y_c \\ln (\\frac{\\exp (x_{i})}{\\sum_{j=1}^{n} \\exp (x_{j})})其中$y_c = 1$，指当前样本的真实标记是 $c$。 loss = - \\ln (\\frac{\\exp (x_{c})}{\\sum_{j=1}^{n} \\exp (x_{j})}) = - \\ln(Softmax(x_c)) = - \\ln(S(x_c))此处进行求导， \\frac{\\partial loss}{\\partial x_i} = - \\frac{1}{S(x_c)} \\frac{\\partial S(x_c)}{\\partial x_i}而对于 softmax 函数求导需要分类讨论： 当$i=c$时, $\\frac{\\partial S(x_c)}{\\partial x_i} = S(x_c)(1-S(x_c))$ 当$i\\neq c$时, $\\frac{\\partial S(x_c)}{\\partial x_i} = - S(x_i)S(x_c)$ 所以导数为： 当$i=c$时, $\\frac{\\partial loss}{\\partial x_i} = S(x_i)-1$ 当$i\\neq c$时, $\\frac{\\partial loss}{\\partial x_i} = S(x_i)-0$ Reference Cross entropy - From Wikipedia, the free encyclopedia Kullback–Leibler divergence - From Wikipedia, the free encyclopedia Information theory - From Wikipedia, the free encyclopedia Softmax以及Cross Entropy Loss求导 互信息 Yu Zhang’s Blog","link":"/machine-learning/entropy/"},{"title":"Focal Loss","text":"前几天听到一个关于元学习的报告，里面提到了一下 Focal Loss，意识到总是听人讲起却没有好好看过它到底是什么，于是这里做一个简短的学习。 类别不平衡问题Focal loss 是何恺明大神提出的，面向类别不平衡性能损失问题的一个思路。 通常情况下，面对类别不平衡，如果不加干预，模型则更倾向于对优势类有着更好表现。 通常大家的解决思路是对不同类别的样本损失函数加权，降低优势类损失的权重，增强劣势类损失的权重，比例约为不同类别（正负样本）的数量比。 此处 focal loss 本质上也是一个对于 CEloss 的加权，但是它入手的角度是学习样本的难易程度。 Focal loss 具体形式首先我们给出 focal loss 的表达式： L_{f l} =-\\left(1-p_t\\right)^\\gamma \\log \\left(p_t\\right)\\\\ p_t = \\begin{cases}\\hat{p} & \\text { if } \\mathrm{y}=1 \\\\ 1-\\hat{p} & \\text { otherwise }\\end{cases}同理也有交叉墒损失函数的表达式: L_{ce}=- \\log \\left(p_t\\right)\\\\两者的主要区别在于 $log$ 项之前的权重。 直观上理解，$p$ 反应了分类的难易程度，分类的置信度越高，代表样本越易分；分类的置信度越低，代表样本越难分。 因此focal loss相当于增加了难分样本在损失函数的权重，使得损失函数倾向于难分的样本，有助于提高难分样本的准确度。 而通常情况下，样本较少的类别天然会难分一些。 Reference 何恺明大神的「Focal Loss」，如何更好地理解？ focal loss 通俗讲解","link":"/machine-learning/focal-loss/"},{"title":"Gaussian Process","text":"一份理解高斯分布的笔记。 参考： A Visual Exploration of Gaussian Processes 看得见的高斯过程：这是一份直观的入门解读 如何通俗易懂地介绍 Gaussian Process？ 高斯过程回归：推导，实现和理解 深度学习基础（高斯过程） 模型的理解高斯分布一般用来做回归。 其原理是把回归当成一个采样过程。 在 $k$ 个测试样本上预测的情况，可以想象成在一个 $k$ 维的高斯分布下进行采样。 在每个维度上的采样结果可以当作在这个样本上的预测值。 所以说回归问题转化为如何构建这个 $k$ 维的高斯分布，一旦构建完成则可以用来回归预测。 直接对预测数据 $X^\\ast$ 构建这个先验分布$P(f(X^\\ast))$是困难的，所以说在有训练数据$Y$的情况下，我们期望找到的是一个后验概率分布$P(f(X^\\ast)|Y)$。 多变量高斯分布我们先从多变量高斯分布说起，介绍其形式和性质。 1. 先验分布对于一个先验分布是高斯分布的一组随机变量 $X \\sim \\mathcal{N}(\\mu,\\,\\Sigma)$， $X$ 可以由两部分组成 $X_a$ 和 $X_b$，代表原始随机变量的子集，维度为 $|X_a|+|X_b| = p + q$。 具体来说， X=\\begin{bmatrix}X_a\\\\X_b\\end{bmatrix}_{p+q}\\quad\\mu=\\begin{bmatrix}\\mu_a\\\\\\mu_b\\end{bmatrix}_{p+q}\\quad \\Sigma=\\begin{bmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{bmatrix}_{p + q, p + q}其中 $\\mu$ 和 $\\Sigma$ 是对应的均值和方差。 均值 $\\mu$，一般在数据归一化的情况下，先验均值可以设为 0 函数。 对于协方差矩阵 $\\Sigma$，则可以用核函数来生成。 核函数，一般用来表示一种距离的度量，这里用现有样本的特征 $x$ 建立随机变量间的距离，再用其作为每个维度之间的协方差。 此处的核函数有多种选择方式，同时也可以组合起来使用。 通过不同核函数的选择可以起到添加先验知识的作用。 通过这一个步骤可以建立起联合分布的高斯分布表达式。 2. 通过先验分布得到后验分布此时我们可以通过条件作用 conditioning 从 $P(X_a,X_b)$ 得到 $P(X_b|X_a)$。 这样是从一个维度为$|X_a|+|X_b|$的高斯分布得到一个维度为维度为 $|X_b|$ 的高斯分布。 条件作用之后均值和标准差会发生变化，依据高斯分布的性质可以得到以下条件分布， X_b|X_a\\sim N(\\mu_{b|a},\\Sigma_{b|a})其中， $\\mu_{b|a}=\\Sigma_{ba}\\Sigma^{-1}_{aa}(X_a-\\mu_a)+\\mu_b$， $\\Sigma_{b|a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma^{-1}_{aa}\\Sigma_{ab}$。 直观上讲，训练点是为候选的函数设了一个限定范围，所得到的函数需要通过训练点。 所以在结果中，靠近训练数据点的区域预测不确定性会小，离得越远，不确定性越大。 至此，当我们确定了样本（$X_a$）之后，就可以得到其回归值（$X_b$）的高斯分布，并取 $\\mu_b$ 作为预测得到的回归值。 高斯过程对于观测点 $X$（训练点，注意这里的 $X$ 和上一小节的不同，这里是表示样本的特征）与其对应值 $Y$，所有的非观测点（测试点） $X^\\ast$ 的值定义为 $f(X^\\ast)$。 我们对预测值 $Y$ 构建多维高斯分布。 这里我们把均值向量替换为均值函数。 那么有， \\begin{bmatrix}Y\\\\f(X^\\ast)\\end{bmatrix}\\sim N(\\begin{bmatrix}\\mu(X)\\\\\\mu(X^\\ast)\\end{bmatrix}，\\begin{bmatrix}k(X,X)& k(X,X^\\ast)\\\\k(X^\\ast,X)&k(X^\\ast,X^\\ast)\\end{bmatrix})同样的我们可以得到条件分布， P(f(X^\\ast)|Y,X^\\ast,X) \\sim N(\\mu^\\ast,k^\\ast)其中, $\\mu^\\ast=k(X^\\ast,X)k(X,X)^{-1}(Y-\\mu(X))+\\mu(X^\\ast)$，$k^\\ast=k(X^\\ast,X^\\ast)-k(X^\\ast,X)k(X,X)^{-1}k(X,X^\\ast)$。 一般在数据归一化的情况下，先验均值 $\\mu(X)$ 和 $\\mu(X^\\ast)$ 可以设为 0 函数。 在 sklearn 中，可以通过调节参数 normalize_y=True 实现。","link":"/machine-learning/gaussian-process/"},{"title":"机器学习中的 Lipschitz Continuity","text":"在 discriminator 的学习使用中，经常会见到这个 Lipschitz Condition，在此处做一个学习。 Lipschitz ContinuityLipschitz Continuous 是比可微分更严格的条件，这个性质限制了函数的微分值必须有上下限。 换句话说，目标函数会被一个一次函数上下夹逼，如下图所示。 以公式的形式展开，函数需要满足以下条件： \\|f(x)-f(y)\\| \\leqslant L\\|x-y\\|为什么会在机器学习中使用？本人目前见到过的使用地点多在 discriminator 相关工作中。 Discriminator 在 GAN 相关模型中是一个必要的结构，在域迁移的模型中也多有使用。 其作用是将真实样本与虚假样本分开，之后在将其固定来训练生成器使得判别器无法将虚假样本分开。 当下 GAN 可以生成足以骗过人类的高质量图像，但是其训练过程的不稳定仍然是一个具有挑战性的问题。 因此，一系列的研究工作都着眼于解决不稳定训练的问题。 WGAN 中使用 wasserstein distance 来代替原始 GAN 中的分类损失，效果良好。 \\left(P_r, P_g\\right)=\\inf _{\\gamma \\in \\prod\\left(P_r, P_g\\right)} \\mathbb{E}_{(x, y) \\sim \\gamma}[\\|x-y\\|]但是由于这个形式十分难以求解，所以将该优化问题转换为以下形式。 W\\left(P_r, P_\\theta\\right)=\\sup _{\\|f\\|_L \\leq K} \\mathbb{E}_{x \\sim P_r}[f(x)]-\\mathbb{E}_{x \\sim P_\\theta}[f(x)]这个转化通过对判别器应用正则化或归一化，将判别器形式化定义为一个利普希茨连续的函数（Lipschitz continuous function），其利普希茨常数为 K。 这样，在不大幅度牺牲判别器性能的条件下，判别器的梯度空间会变得更平滑，可以更加稳定的训练。 在此技术上，有谱归一化和梯度归一化等工作。 Reference 舍弃谱归一化，这篇ICCV’21论文用梯度归一化训练GAN，效果极好 深度学习中的Lipschitz约束：泛化与生成模型 What is Lipschitz constraint and why it is enforced on discriminator? Spectral Normalization 谱归一化-原理及实现","link":"/machine-learning/lipschitz-in-ml/"},{"title":"训练集样本对于测试集 loss 的影响","text":"主动学习中通常要评估样本的重要性/信息量，以选择最有价值样本使得模型提升最大。 这一点在样本可解释性中也有体现，即“什么样的样本对于训练来说更加重要？” 本文是几篇论文【1，2】的阅读笔记，从梯度的角度阐释样本对于最终测试集表现的影响。 训练样本如何影响测试表现【2】自然的，去掉一个样本带来的参数变化可以写作 $\\hat{\\theta}_{-z}-\\hat{\\theta}$，其中 \\hat{\\theta}_{-z} \\stackrel{\\text { def }}{=} \\arg \\min _{\\theta \\in \\Theta} \\sum_{z_i \\neq z} L\\left(z_i, \\theta\\right)但是对于每一个样本而言，我们不可能总是去掉它并重新训练一个模型。 于是我们可以从 influence function 的角度来理解这个问题。 即计算样本权重变化一个很小的 $\\epsilon$ 时所带来的参数变化（原始权重为 $\\frac{1}{n}$），在这种情况下 \\hat{\\theta}_{\\epsilon, z} \\stackrel{\\text { def }}{=} \\arg \\min _{\\theta \\in \\Theta} \\frac{1}{n} \\sum_{i=1}^n L\\left(z_i, \\theta\\right)+\\epsilon L(z, \\theta)此时将样本 $z$ 在参数 $\\theta$ 上权重增大 $\\epsilon$ 时所带来的 influence 写作 \\left.\\mathcal{I}_{\\text {up,params }}(z) \\stackrel{\\text { def }}{=} \\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon}\\right|_{\\epsilon=0}=-H_{\\hat{\\theta}}^{-1} \\nabla_\\theta L(z, \\hat{\\theta})其中 $H_{\\hat{\\theta}} \\stackrel{\\text { def }}{=} \\frac{1}{n} \\sum_{i=1}^n \\nabla_\\theta^2 L\\left(z_i, \\hat{\\theta}\\right)$ 是正定的 Hessian 矩阵。 在训练集中去掉 $z$ 点就相当于将 $\\epsilon$ 设为 $-\\frac{1}{n}$，那么参数的变化 $\\hat{\\theta}_{-z}-\\hat{\\theta}$ 则可在不重训练模型的情况下近似为 $ -\\frac{1}{n} \\mathcal{I}_{\\text {up,params }}(z)$. 这样在更改 $z$ 权重的情况下，对于一个测试样本的影响的解析解可以写作： \\begin{aligned} \\mathcal{I}_{\\text {up }, \\text { loss }}\\left(z, z_{\\text {test }}\\right) &\\left.\\stackrel{\\text { def }}{=} \\frac{d L\\left(z_{\\text {test }}, \\hat{\\theta}_{\\epsilon, z}\\right)}{d \\epsilon}\\right|_{\\epsilon=0} \\\\ &=\\left.\\nabla_\\theta L\\left(z_{\\text {test }}, \\hat{\\theta}\\right)^{\\top} \\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon}\\right|_{\\epsilon=0} \\\\ &=-\\nabla_\\theta L\\left(z_{\\text {test }}, \\hat{\\theta}\\right)^{\\top} H_{\\hat{\\theta}}^{-1} \\nabla_\\theta L(z, \\hat{\\theta}) . \\end{aligned}主动学习中对这一结论的运用【1】依赖上一节中的影响力估计，训练集中样本 $x$ 对测试集中样本 $x_j$ 在损失上的影响为 I_{\\text{loss}}\\left(x, x_j\\right)=\\frac{1}{n} \\nabla_\\theta L\\left(f_\\theta\\left(x_j\\right)\\right)^{\\top} H_\\theta^{-1} \\nabla_\\theta L\\left(f_\\theta(x)\\right)那么在整个测试集的总影响可以写作 \\sum_j I_{\\text{loss}}\\left(x, x_j\\right)=\\frac{1}{n} \\sum_j \\nabla_\\theta L\\left(T^{c+1}\\left(x_j\\right)\\right)^{\\top} H_\\theta^{-1} \\nabla_\\theta L\\left(T^{c+1}(x)\\right)其中 $T^{c+1}$ 是在第 $c+1$ 个循环过后的模型。 尽管单一样本可能对某一测试样本有副作用，但是总体来说训练样本对于整个测试集的影响是正面的。 那么，在第 $c+1$ 个循环中移除一个训练样本，得到的测试集损失则变为 \\begin{aligned} L_{\\text {test}}^{\\prime c+1} &=L_{\\text {test }}^{c+1}+\\sum_j I_{\\text {loss }}\\left(x, x_j\\right) \\\\ &=L_{\\text {test }}^{c+1}+\\frac{1}{n} \\sum_j \\nabla_\\theta L\\left(T^{c+1}\\left(x_j\\right)\\right)^{\\top} H_\\theta^{-1} \\nabla_\\theta L\\left(T^{c+1}(x)\\right) \\end{aligned}由于测试集样本未知，即无法直接得到 $\\sum_j \\nabla_\\theta L\\left(T^{c+1}\\left(x_j\\right)\\right)^{\\top} H_\\theta^{-1} \\nabla_\\theta L\\left(T^{c+1}(x)\\right)$。 所以我们同时在等式两边取 Frobenius Norm，得到 \\begin{aligned} L_{\\text {test}}^{\\prime c+1} &=\\left\\|L_{\\text {test }}^{c+1}+\\frac{1}{n} \\sum_j \\nabla_\\theta L\\left(T^{c+1}\\left(x_j\\right)\\right)^{\\top} H_\\theta^{-1} \\nabla_\\theta L\\left(T^{c+1}(x)\\right)\\right\\| \\\\ & \\leq L_{\\text {test }}^{c+1}+\\frac{1}{n}\\left\\|\\sum_j \\nabla_\\theta L\\left(T^{c+1}\\left(x_j\\right)\\right)^{\\top} H_\\theta^{-1} \\nabla_\\theta L\\left(T^{c+1}(x)\\right)\\right\\| \\\\ &=L_{\\text {test }}^{c+1}+\\frac{1}{n}\\left\\|\\nabla_\\theta L\\left(T^{c+1}(x)\\right)^{\\top} \\sum_j H_\\theta^{-1} \\nabla_\\theta L\\left(T^{c+1}\\left(x_j\\right)\\right)\\right\\| \\\\ & \\leq L_{\\text {test }}^{c+1}+\\frac{1}{n}\\left\\|\\nabla_\\theta L\\left(T^{c+1}(x)\\right)\\right\\| \\cdot\\left\\|\\sum_j H_\\theta^{-1} \\nabla_\\theta L\\left(T^{c+1}\\left(x_j\\right)\\right)\\right\\| \\end{aligned}而对于一个固定的测试集来说，$\\left|\\sum_j H_\\theta^{-1} \\nabla_\\theta L\\left(T^{c+1}\\left(x_j\\right)\\right)\\right|$ 可以看做一个不变项。 所以 $L_{\\text {test}}^{\\prime c+1}$ 的上界主要依赖于 $\\left|\\nabla_\\theta L\\left(T^{c+1}(x)\\right)\\right|$。 但是在第 $c$ 个循环，模型 $T^{c+1}$ 不可知，所以使用模型 $T^{c}$ 来做一个近似，即 \\begin{aligned} L_{\\text {test}}^{\\prime c+1} & \\leq L_{\\text {test }}^{c+1}+\\frac{1}{n}\\left\\|\\nabla_\\theta L\\left(T^{c+1}(x)\\right)\\right\\| \\cdot\\left\\|\\sum_j H_\\theta^{-1} \\nabla_\\theta L\\left(T^{c+1}\\left(x_j\\right)\\right)\\right\\| \\\\ & \\lesssim L_{\\text {test }}^{c+1}+\\frac{1}{n}\\left\\|\\nabla_\\theta L\\left(T^c(x)\\right)\\right\\| \\cdot\\left\\|\\sum_j H_\\theta^{-1} \\nabla_\\theta L\\left(T^{c+1}\\left(x_j\\right)\\right)\\right\\| \\end{aligned}那么在主动学习的范畴下，更高的 $\\left|\\nabla_\\theta L\\left(T^{c}(x)\\right)\\right|$ 可以使得 $L_{\\text {test }}^{c+1}$ 的上界更低。 所以应当选择梯度范数更低的样本。 具体梯度的计算需要通过 loss function 使用反向传播。 然而具体的真实标记并未获得，所以需要特地设计损失函数来计算梯度。 文章中提到了两种梯度的计算，第一种是 Expected-Gradnorm Scheme \\begin{equation} L_{exp}\\left(T^c(x)\\right)=\\sum_{i=1}^N P\\left(y_i \\mid x\\right) L_i\\left(T^c(x), y_i\\right) \\end{equation}另一种是 Entropy-Gradnorm Scheme \\begin{equation} L_{e n t}\\left(T^c(x)\\right)=-\\sum_{i=1}^N P\\left(y_i \\mid x\\right) \\log P\\left(y_i \\mid x\\right) \\end{equation}主动学习相关当然也有很多其他的主动学习工作使用到了梯度，但是他们考虑问题的入手角度不同。 本文介绍的这种通过梯度的评估本质上是考虑其对测试误差的提升。 而其他的基于梯度方法通常是来评估样本对模型可能带来多大的改变，比如 EGL 和 BADGE。 尤其是 EGL，在形式上和本文介绍的方法很相似，不同的是其仅计算最后一层的梯度。 一些数学基础：Hessian 矩阵黑塞/海森矩阵是在求多元函数的所有二阶偏导数。 具体来说，假设有一个实值函数 $f\\left(x_1, x_2, \\ldots, x_n\\right)$，其所有的二阶偏导数在定义域内连续，那么函数的黑塞矩阵为 \\begin{equation} \\mathbf{H}=\\left[\\begin{array}{cccc} \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} & \\cdots & \\frac{\\partial^2 f}{\\partial x_n^2} \\end{array}\\right] \\end{equation}用下标记号表示为 $\\mathbf{H}_{i j}=\\frac{\\partial^2 f}{\\partial x_i \\partial x_j}$。 可以在泰勒展开中理解这个矩阵。 在一元函数中，泰勒展开式为 f(x)=f\\left(x_0\\right)+f^{\\prime}\\left(x_0\\right) \\Delta x+\\frac{f^{\\prime \\prime}\\left(x_0\\right)}{2 !} \\Delta x^2+\\cdots相应的，在多元函数中 f(x)=f\\left(x_0\\right)+\\nabla f\\left(x_0\\right)^{\\mathrm{T}} \\Delta x+\\frac{1}{2} \\Delta x^{\\mathrm{T}} \\mathbf{H}\\left(x_0\\right) \\Delta x+\\cdotsReference Wang, Tianyang, et al. “Boosting active learning via improving test performance.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 36. No. 8. 2022. Koh, Pang Wei, and Percy Liang. “Understanding black-box predictions via influence functions.” International conference on machine learning. PMLR, 2017. https://en.wikipedia.org/wiki/Hessian_matrix","link":"/machine-learning/loss-gradient/"},{"title":"LSTM","text":"最近用到的 ASP-MTL 模型中使用 LSTM 作为特征提取器。 自己对于 RNN 的认知很不成体系，在此进行一个梳理。 对 LSTM 来做一个学习，主要针对结构和预测两方面。 RNNRecurrent Neural Networks（RNN）指循环神经网络，用来处理序列数据。 每一个时刻的输出或中间信息会被传递到下一个时刻作为一部分输入，以保留时序信息。 具体模式如下图所示。 其中处理以往和当前信息的结构十分简单，以一个 tanh 来合并。 LSTM经典的 RNN 存在难以解决长距离依赖（long-term dependency）的问题，即时序上距离过远的相关信息难以被学到。 于是 LSTM 模型被提出，其包含了一个记录长效信息的模块。 LSTM 的结构如下图所示。 最主要的核心观点是维护一个 cell state（细胞状态），以使得信息跨时序传输。 当然这里个人认为翻译成“牢房”更为贴切。 不同于简单的 RNN 中只有一个神经网络，LSTM 中含有四个主要的神经网络（门结构）。 遗忘门决定要从细胞状态中舍弃什么信息。 输入门决定保存哪些新信息进入细胞状态。 旧的状态 $C_{t-1}$ 先 forget 再 input 得到新的状态 $C_t$。 输出一个“过滤”后的细胞状态。 模型训练先由正向传播，算出最终的损失 $J$，再反向计算梯度即可。 详细的正向过程可见【2】。 反向过程可见【3】。 值得注意的是，反向过程的梯度更新需要考虑每一个时间步的输出。 Reference Understanding LSTM Networks 人人都能看懂的LSTM介绍及反向传播算法推导 - 陈楠的文章 RNN之随时间反向传播BPTT推导细节，从公式中理解RNN梯度消失与梯度爆炸原因 - 塞巴斯万隆的文章","link":"/machine-learning/lstm/"},{"title":"Prompt 学习记录","text":"在自然语言处理中有一个叫做 prompt 的新范式最近较火，其背景是在少标记的场景下学习。 本文主要内容都是从 Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing 这篇综述中提取。 仅涵盖本人认为最需要被科普的内容。 背景当前在自然语言处理中主要存在以下范式，其中由 a 到 d 基本按照时间顺序出现。 总的来说，目前为止经历了两个 sea changes（重大变化）。 在2017年以前，主要以完全监督学习为主（a和b范式）。 研究的主要内容在于特征提取（传统模型），结构构建（深度模型）。 但是在2017年之后，经历了第一个重大变化，完全监督的旧范式的使用不断在缩小，预训练及微调（c范式）开始流行。 在当前时间节点，2021年，正在经历第二个重大变化。 当前对于下游任务学习并不是通过预训练及微调中的 objective engineering，而是通过 prompt（提示）来实现。 这里举一个 prompt 的例子。 When recognizing the emotion of a social media post, “I missed the bus today.”, we may continue with a prompt “I felt so __”, and ask the LM to ﬁll the blank with an emotion-bearing word. Or if we choose the prompt “English: I missed the bus today. French: __”), an LM may be able to ﬁll in the blank with a French translation. 通过这种选取合适的 prompt 的方法，预训练的 language model（LM）可以用来预测合适的输出，有时甚至不需要 task-specific 的训练。 Prompt 的正式表述基于 prompt 的方法尝试规避无法获得大规模数据的问题，直接对样本 $\\boldsymbol{x}$ 的概率 $P(\\boldsymbol{x};\\theta)$ 进行建模，之后再用这个概率来预测$\\boldsymbol{y}$。 以下是一些基于 prompt 的方法中的术语。 通常来说，prompt 的方法预测高质量的输出 $\\hat{\\boldsymbol{y}}$ 有三步。 Prompt Addition：通过模版，将原始语句转化为 Prompt，含有空白等待填入。 Answer Search：在候选集$\\mathcal{Z}$中，选取 answer prompt $\\hat{\\boldsymbol{z}}=\\operatorname{search}_{\\boldsymbol{z} \\in \\mathcal{Z}} P\\left(f_{\\mathrm{fill}}\\left(\\boldsymbol{x}^{\\prime}, \\boldsymbol{z}\\right) ; \\theta\\right)$。 Answer Mapping：将高分的回答 $\\boldsymbol{z}$ 和高分的输出 $\\hat{\\boldsymbol{y}}$ 对应起来。 设计 Prompt 时需要考虑的具体问题这里一般来说存在以下5个需要具体考虑的问题： 如何选择预训练模型 选择何种 Prompt 来作为 Prompting funtion（模版）。 设计候选集$\\mathcal{Z}$，可能同时还需要考虑与输出的映射。 对简单框架的扩展以提高表现和适用性。 训练参数的策略 此处不一一展开，可以到综述中寻找具体的部分。 Prompt 的应用和挑战这篇综述也详尽的展开了当前 Prompt 的应用和挑战。 这里只简单记录不做展开。 应用方面，几乎涉及了 NLP 的方方面面，集中于以下几大类：Knowledge Probing、Classiﬁcation-based Tasks、Information Extraction、“Reasoning” in NLP、Question Answering、Text Generation、Automatic Evaluation of Text Generation、Multi-modal Learning、Meta-Applications。 挑战方面，主要集中于以下几个大类：Prompt Design、Answer Engineering、Selection of Tuning Strategy、Multiple Prompt Learning、Selection of Pre-trained Models、Theoretical and Empirical Analysis of Prompting、Transferability of Prompts、Combination of Different Paradigms、Calibration of Prompting Methods。 相关资料 Pretrain, Prompt, Predict: A New Paradigm for NLP Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing PromptPapers 对这个范式的看法Prompt 方法归根结底还是面向标记数据缺乏这一老生常谈的问题。 从二十多年前的经典方法开始，大家就在对这一问题展开研究。 从模型角度入手似乎还是比较少见，主要还是在学习范式的角度上进行研究。 从最开始的监督学习，到半监督学习，还有迁移学习，都是致力于使用有限的标记来最大化任务表现。 近几年这种半监督，或者说自（无）监督的方法已经极大程度上融进了目前机器学习的框架中。 有时作为一种特征提取器而存在，有时作为一种正则化而存在。 从这一角度来看，当前的几种范式，都是在探讨如何最大化利用未标记样本，Prompt 也不例外。 对于 Prompt 而言在我来看 Prompt 的定位应该是一种较为成熟的广泛适用于文本的自监督学习方法。 自监督学习关键在于定义一种可以自己知道正确答案的任务，用此任务来训练，以得到对该任务的较好表现，同时也可以获取质量相对较好的特征以适用于下游任务。 当前的预训练模型大多都是由 mask 这类操作来自监督训练。 所以在预训练模型上学习的填空能力是可以很好的用于 Prompt 定义的填空题。 由于这种填词题在训练的时候已经见过，不像很多下游任务还需要知道之前没见过的标签，所以预训练模型在寻找 answered prompt 时可以 zero-shot 或者 few-shot。 总的来说，这种范式设计了一种能“引诱”模型输出之前在大量样本中学到的统计（/逻辑/推断）数据。 个人觉得还是蛮有意思，或许可以用来对模型进行解释。 对于自己主动学习的研究而言主动学习和这些范式其实不太相同，上述半监督自监督学习主要考虑如何利用未标记样本，而主动学习是在探讨如何最大化利用有限的标记成本来选取最有价值的样本。 其已假设更重要的样本能学出来相对随机选取的样本更好的模型。 在当前无监督自监督学习表现如此之好的情况下，单纯使用主动学习得到的标记样本意义似乎不大。 因为单纯选取少量重要的标记样本可能仍旧难以与大量未标记样本上的自监督匹敌。 这就是那两篇主动学习劝退文里面指出的问题。 所以说个人认为主动学习中，尤其是模型的训练部分，为了最大化效用，应该是一定是要使用未标记样本的。 主动学习中存在标记，那么必然是一个下游任务。 那么具体如何结合少量标记样本和大量未标记样本来训练，就是其他那几种范式。 在其他范式上学到的特征提取器上对于下游进行微调，可能才是主动学习最好的实施方法。 具体如何微调，又是一个研究了很久的问题。 所以这种无机的结合可能才是主动学习的实际场景。","link":"/machine-learning/prompt/"},{"title":"再次学习强化学习的笔记","text":"距离上一次学习强化学习已经很久了。 最近由于 learning to optimize 用到了很多强化学习的知识，猛的一看发现又不太懂，于是这里进行对于强化学习的再学习。 上一次对强化学习的学习见这一篇帖子。 总的来说，上次对于强化学习的理解过于浅薄，而且并没有从一个系统性和直观的角度进行理解，导致抄了一堆公式也是白抄。 本文主要以一个直观的角度来对一个重要算法在整个体系中存在的位置做一个定位，不会过多陷入数学计算。 什么是强化学习？（一些基础知识）强化学习目标是学到一个对于目标任务的策略 policy ($\\pi$)，给定一个当前的状态 state ($s$)，策略可以给出当前应该执行的策略 action ($a$) 以获得最大收益。 此外还有以下一些概念： Value 价值：一般单独提到时指的是当前状态的价值。 Q-value 动作价值：指在某一个状态 $s$ 下实施动作 $a$ 所能收获的收益。 Model 模型：指状态转移的模式，即环境对于特定的动作如何反应。 Return 回报：指在整个交互结束之后能收到的总奖励。 最初的方法： Q-Leaning &amp; Policy Gradient我们这里还是更多的介绍一下 model-free 的场景。 具体来说，我们对环境提供的状态转移无知，并且我们不对其进行建模。 在这种情况下，我们想要学习一个策略，$\\pi(s)\\rightarrow a$，可以有两种方式： 学习不同状态下的 Q-value，并对其进行建模。建模成功之后即可选取在当前 state 下 Q-value 最大的 action 实施。 直接学习策略 $\\pi$，则可直接输入 state，得到 action。 那么两者各有什么特点呢？ Q-learning 以值为基础，可以使用时序差分（temporal-difference）的思路，使效用估计朝着理想均衡方向调整，可以进行单步更新。但是由于其离散化的特点，如果在连续空间中进行选择，则会瘫痪。 Policy gradient 可以在毫不费力地在连续空间中进行选择，但是因为其是基于 Monte-Carlo 采样得到的评估，必须等一个回合结束后才可以更新，学习效率低。 结合两者的优点 Actor-CriticPolicy gradient 使用现实中的奖惩来更新 actor。 此时的梯度可以写作： \\nabla \\bar{R}_\\theta=E_{\\tau \\sim p_\\theta(\\tau)}\\left[R(\\tau) \\nabla \\log p_\\theta(\\tau)\\right]这个 reward 奖惩信息 $R$ 可以被学出来，作为一个 critic 来来指导 actor 训练。 Critic 可以看到当前所处状态动作的潜在奖励，所以可以使得 actor 每一步都在更新，而不是到回合结束才能更新。 AC 中的梯度可以写作： \\nabla \\bar{R}_\\theta=\\frac{1}{N} \\sum_{n-1}^N \\sum_{t=1}^{T_n} Q^{\\pi_\\theta}\\left(s_t^n, a_t^n\\right) \\nabla \\log p_\\theta\\left(a_t^n \\mid s_t^n\\right)A2C 和 A3CAdvantage Actor-Critic (A2C)AC 中使用 Q-value，方差大，梯度差异较大训练不稳定。 PG 中同样有这种问题，它的解决方式是引入一个 baseline，用累计奖励 Gain 减去 baseline，可以使梯度减小，训练平缓。 \\nabla \\bar{R}_\\theta=\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n}\\left(Gain_t - baseline(s_t)\\right) \\nabla \\log p_\\theta\\left(a_t^n \\mid s_t^n\\right)A2C 中，认为对于 Q-value $Q^{\\pi_\\theta}\\left(s_t^n, a_t^n\\right) $ 一个自然的 baseline 选择是 $V^{\\pi_\\theta}\\left(s_t^n\\right)$。 于是可以构造优势函数 advantage function $A(s_t,a_t) = Q(s_t,a_t)-V(s_t)$。 其可以通过近似计算得到 $A(s_t,a_t) = r_t^n+V^\\pi\\left(s_{t+1}^n\\right)-V^\\pi\\left(s_t^n\\right)$。 所以可以仅用一个网络来估计 $V$ 值而不用评估 Q-value。 可以说 A2C 解决的是梯度方差大，训练不稳定的问题。 A2C 中的梯度可以写作： \\nabla \\bar{R}_\\theta=\\frac{1}{N} \\sum_{n=1}^N \\sum_{t=1}^{T_n} A^{\\pi_\\theta}\\left(s_t^n, a_t^n\\right) \\nabla \\log p_\\theta\\left(a_t^n \\mid s_t^n\\right)除了方差大，AC 还存在的问题：两个网络交替训练，critic 和 action 强相关，有着很强的时序关联，只能探索到有限的状态和动作空间。 为了打破经验耦合，需要采用 experience replay，使得 agent 在后续训练可以访问到以前知识（例如 DQN 和 DDPG（DQN+AC）这类基于值的方法）。 但是策略类的方法，经验都是以 episode 形式获得，用完即弃。 在此情况下使用并行架构，不同的 worker 与环境进行交互，得到独立的采样经验。 即每轮训练中，Global network 都会等待每个 worker 各自完成当前的 episode，然后把这些 worker 上传的梯度进行汇总并求平均，得到一个统一的梯度并用其更新主网络的参数，最后用这个参数同时更新所有的 worker。 可以说 A2C 也解决了经验耦合的问题。 Asynchronous Advantage Actor-Critic (A3C)A3C 相对于 A2C 使用了异步经验更新。 没有等待所有 worker 完成当前 episode 并汇总的过程。 各个 worker 都分别使用着一套不同的策略，独立的跟自己的环境交互。而主网络保持着最新的策略，各 worker 跟主网络同步的时间也是不一样的，只要有一个 worker 完成当前episode，主网络就会根据它的梯度进行更新，并不影响其它仍旧在使用旧策略的 worker。这就是异步并行的核心思想。 但是研究者们逐渐发现A3C主要优势在于采用了“并行训练”的思想，而不一定要“异步地并行训练”。 异步更新并没有使训练效率和性能取得显著提高【3】，A2C 表现更好（A2C 是在 A3C 后面出现的）。 结语本文并没有深入技术细节，而是从强化学习（在这几个方法上）的演变角度，简单的记录了不同方法的 intuition。 比之前的学习笔记或许在问题定义上清晰了一些。 Reference 什么是 Actor-Critic (强化学习) - 莫烦的文章 深入理解强化学习（七）- Actor-Critic - 莫冉的文章 深度强化学习 — 进击的 Actor-Critic（A2C 和A3C） - Quantum cheese的文章 强化学习AC、A2C、A3C算法原理与实现！ - 梁勇的文章 Actor-Critic的变体 - heaven的文章","link":"/machine-learning/re-learn-RL/"},{"title":"强化学习 &amp; 模仿学习基础知识","text":"总是会看到强化学习及模仿学习的内容，每次看完都会忘记，此处把之前的小笔记总结一下。 基本术语 Terminologies以下术语在强化学习和模仿学习中都经常见到。 agent: the intelligent individual environment: The agent is acting in an environment. state: Current condition. The agent can stay in one of many states of the environment action: The agent chooses to take one of many actions under the certain states. reward: Once an action is taken, the environment delivers a reward as feedback. policy: Agents’ behavior.(s =&gt; a) The agent’s policy π provides the guideline on what is the optimal action to take in a certain state with the goal to maximize the total rewards. value: (s =&gt; value) Each state is associated with a value function V(s) predicting the expected amount of future rewards we are able to receive in this state by acting the corresponding policy. state-value of a state s is the expected return if we are in this state at time t. action-value (“Q-value”; Q as “Quality” I believe?) of a state-action pair is expected return if we are in this state at time t and take action a. A-value: The difference between action-value and state-value is the action advantage function (“A-value”): model: Transition and reward. (s,a =&gt; s’ &amp; r) How the environment reacts to certain actions (we may or may not know). 强化学习1. 马尔科夫决策过程In more formal terms, almost all the RL problems can be framed as Markov Decision Processes (MDPs). All states in MDP has “Markov” property, referring to the fact that the future only depends on the current state, not the history. The goal is to react on each state to maximize the total reward. 如果所有 MDP 成分都已知，我们便可以较容易的训练出来一个 agent。 但是现实情况是很多时候，我们的 agent 对 transition function $P$ 和 reward function $R$ 一无所知，所有的信息都来自于同环境的交互。 1.1. 强化学习方法分类 以是否对环境建模分类: Doesn’t model the environment: Model-free RL: Doesn’t need to know the transition function (“model”), neither the real function nor a learned function. Model the environment: Model-based RL: Need to know the transition function (“model”), either the real function or a learned function. Inverse reinforcement learning: Need to learn a value function for a state. (Imitation learning) 以行动策略和评估策略是否相同分类： On-policy: Use the deterministic outcomes or samples from the target policy to train the algorithm. 行动策略和评估策略相同 Off-policy: Training on a distribution of transitions or episodes produced by a different behavior policy rather than that produced by the target policy. 行动策略和评估策略不同 1.2. 对价值函数进行评估和分解强化学习的目标是可以使最终价值最大化，所以需要对其进行评估。 Bellman equations refer to a set of equations that decompose the value function into the immediate reward plus the discounted future values. \\begin{aligned} V_{\\pi}(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s) Q_{\\pi}(s, a) \\\\ Q_{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_{\\pi} (s') \\\\ V_{\\pi}(s) &= \\sum_{a \\in \\mathcal{A}} \\pi(a \\vert s) \\big( R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a V_{\\pi} (s') \\big) \\\\ Q_{\\pi}(s, a) &= R(s, a) + \\gamma \\sum_{s' \\in \\mathcal{S}} P_{ss'}^a \\sum_{a' \\in \\mathcal{A}} \\pi(a' \\vert s') Q_{\\pi} (s', a') \\end{aligned}2. Common Approaches 2.1. Dynamic ProgrammingWhen the model is fully known, following Bellman equations, we can use Dynamic Programming (DP) to iteratively evaluate value functions and improve policy. The policy would greedy based on the Q-value. Iteratively update the state value and the Q-value. Generalized Policy Iteration (GPI) adaptive dynamic process \\pi_0 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_0} \\xrightarrow[]{\\text{improve}} \\pi_1 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_1} \\xrightarrow[]{\\text{improve}} \\pi_2 \\xrightarrow[]{\\text{evaluation}} \\dots \\xrightarrow[]{\\text{improve}} \\pi_* \\xrightarrow[]{\\text{evaluation}} V_*2.2. Monte-Carlo MethodsModel-free method. It learns from episodes of raw experience without modeling the environmental dynamics and computes the observed mean return as an approximation of the expected return 2.3. Temporal-Difference LearningTD Learning is model-free and learns from episodes of experience. However, TD learning can learn from incomplete episodes and hence we don’t need to track the episode up to termination. 主要思想是将效用估计朝着理想均衡方向调整: TD调整一个状态与已观察到的后继状态相一致 ADP调整一个状态与可能出现的的后继状态相一致 TD可视为对ADP的一个粗略有效的一阶近似 \\begin{aligned} V(S_t) &\\leftarrow (1- \\alpha) V(S_t) + \\alpha G_t \\\\ V(S_t) &\\leftarrow V(S_t) + \\alpha (G_t - V(S_t)) \\\\ V(S_t) &\\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)) \\\\ Q(S_t, A_t) &\\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) \\end{aligned}To learn optimal policy: SARSA: On-Policy TD control “SARSA” refers to the procedure for updating the Q-value. Same routine of GPI. In each step of SARSA, we need to choose the next action according to the current policy. Q-Learning: Off-policy TD control The key difference from SARSA is that Q-learning does not follow the current policy to pick the second action (off-policy). Q-learning may suffer from instability and divergence when combined with an nonlinear Q-value function approximation Deep Q-Network It quickly becomes computationally infeasible to memorize Q-table when the state and action space are large. Use functions (i.e. a machine learning model) to approximate Q values and this is called function approximation. greatly improve and stabilize the training procedure of Q-learning by two innovative mechanisms: Experience Replay: improves data efficiency, removes correlations in the observation sequences, and smooths over changes in the data distribution. Periodically Updated Target: only periodically updated, overcomes the short-term oscillations 2.4. Combining TD and MC LearningIn TD learning, we only trace one step further down the action chain when calculating the TD target. One can easily extend it to take multiple steps to estimate the return. 2.5. Policy GradientAll the methods we have introduced above aim to learn the state/action value function and then to select actions accordingly. Policy Gradient methods instead learn the policy directly with a parameterized function. Measure the quality of a policy with the policy score function. Use policy gradient ascent to find the best parameter that improves the policy. 2.6. Asynchronous Advantage Actor-Critic (A3C) Asynchronous: Several agents are trained in it’s own copy of the environment and the model form these agent’s are gathered in a master agent. The reason behind this idea, is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse. Advantage: Similarly to PG where the update rule used the dicounted returns from a set of experiences in order to tell the agent which actions were “good” or “bad”. Actor-critic: combines the benefits of both approaches from policy-iteration method as PG and value-iteration method as Q-learning (See below). The network will estimate both a value function V(s) (how good a certain state is to be in) and a policy π(s). Agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods. 3. Known Problems Exploration-Exploitation Dilemma Deadly Triad Issue: off-policy, nonlinear function approximation, and bootstrapping are combined in one RL algorithm, the training could be unstable and hard to converge. 模仿学习1. 背景Background: Given: demonstrations or demonstrator Goal: train a policy to mimic demonstrations Intuition: 人们并不总是知道执行某项任务所获得的报酬 但是人们可能会知道“做什么是正确的事情（最佳策略） Rollout: sequentially execute $\\pi(s_0)$ on an initial state Produce trajectory $\\mathcal{T}=(s_0,a_0,s_1,a_1,…)$ 2. 模仿学习分类 Behavior cloning Direct policy learning (multiple step BC) Inverse reinforcement learning (assume learning R is statistically easier) 2.1. Behavioral Cloning (simplest Imitation Learning setting)Treat experts’ states-actions pairs i.i.d and as training example use supervised learning (from state to action). Distribution provided exogenously. When to use BC? 2.2. Direct Policy LearningLearning reduction: Reduce “harder” learning problem to “easier” one Idea: Construct a sequence of distributions or sequence of supervised learning problems. Query interactive oracle about the state and construct a loss function according to our action and expert action on this state. 2.3. Inverse reinforcement learningInverse RL指我们需要对环境的reward进行建模。 RL与IRL的对比如下图所示： In a traditional RL setting, the goal is to learn a decision process to produce behavior that maximizes some predefined reward function. Inverse reinforcement learning (IRL), flips the problem and instead attempts to extract the reward function from the observed behavior of an agent. IRL seeks the reward functions that ‘explains’ the demonstrations. 此时同样存在是否依赖 transition function 的情况 References https://en.wikipedia.org/wiki/Reinforcement_learning https://medium.com/@SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html 人工智能：一种现代的方法 https://sites.google.com/view/icml2018-imitation-learning/","link":"/machine-learning/reinforcement-learning/"},{"title":"分布间距离度量","text":"对常用的分布间距离度量进行一个学习和复习。 本文仅为初稿，日后随着理解的加深应该会进行修改和填补。 Kullback–Leibler Divergence (KL-Divergence) Jensen-Shannon Divergence (JSD) Maximum Mean Discrepancy (MMD) Wasserstein Distance 一维分布下的计算 Bhattacharyya Distance Mahalanobis Distance 正式开始之前，我想先总结一下为什么我们需要分布距离的度量。 首先这里的距离并不是指我们常见的欧式距离，而是某种分布间的差异或者相似度。 在我朴素的视角看来，它能为我们解答的最直观的问题是： 这两个分布是不是同一个分布？ 已知一个分布，另一个分布需要多少额外的信息？ 这些度量一般需要满足一些性质：正定（大于等于零，且可以取到0），对称，满足三角不等式。 以下就是一些常见的度量。 Kullback–Leibler Divergence (KL-Divergence)对于分布 $P$ 和 $Q$，已知其交叉熵，可以得到 $P$ 相对于 $Q$ 的相对熵： D_{\\mathrm{KL}}(p \\| q) = H(p, q) - H(p) =-\\sum_{x} p(x) \\log \\frac{q(x)}{p(x)}相对熵又称为 KL 散度，是两个概率分布 $P$ 和 $Q$ 差别的一种非对称性度量，严格意义上不能理解为距离。 其表示使用基于分布 $Q$ 的编码表来编码服从分布 $P$ 的样本（相比起用 $P$ 自己的分布来编码）所需的额外的平均比特数。 在 KL 散度的基础上可以定义交叉墒，参见我们的Post not found: machine-learning/cross-entropy 另一条博文。 KL 散度的一些特征： $p(x)$ 概率更大的匹配区域更加重要（一个更大的权重）。 不对称性。 不满足三角不等式。 Jensen-Shannon Divergence (JSD)JSD 解决了 KL-Divergence 非对称的问题。 其形式为两个 KL-Divergence 之和。 D_{\\mathrm{JS}}(p \\| q) = \\frac{1}{2}D_{\\mathrm{KL}}(p \\| \\frac{p+q}{2}) + \\frac{1}{2}D_{\\mathrm{KL}}(q \\| \\frac{p+q}{2})其同样可以推广到多分布情况，在每个分布的权重为 $\\pi_{i}$ 的情况下。 D_{\\mathrm{JS}}(P_1, P_2,...,P_n) = \\sum_{i} \\pi_{i} D_{\\mathrm{KL}}(P_i \\|M) = H(M) - \\sum_{i} \\pi_{i} H(P_i)\\\\ where, M = \\sum_{i} \\pi_{i} P_iJS 距离的一些特征： 在以2为底的情况下，JSD的值域为 $[0, 1]$。 Maximum Mean Discrepancy (MMD)迁移学习中常用的损失函数，用来描述两个分布差别。 首先通过一个连续函数 $f$ 将随机变量映射到高阶，再求其期望之差的上界，定义如下： MMD(P, Q) = \\lVert E_{X \\sim P}[ f(X) ] - E_{Y \\sim Q}[ f(Y) ]\\rVert _\\mathcal{H}又由于均值是期望的无偏估计，我们用均值替代: MMD(P, Q) = \\lVert \\frac{1}{n} \\sum_{i=1}^{n}f(x_i) -\\frac{1}{m} \\sum_{j=1}^{m}f(y_j) \\rVert _\\mathcal{H}当映射函数仅仅为 $f(x)=x$ 时，MMD 表示两个分布均值点的距离。 而当 $f(x)$ 为无穷维时，我们无法直接计算，此时可以使用核技巧，等号两边同时平方，定义核函数来计算（常用高斯核）。 Wasserstein Distance又被称为推土机距离 Earth Mover’s Distance，来表述把一堆土堆成另一堆土的形状所需要的最小代价。 在连续条件下可以写成如下形式： W_{p}(\\mu ,\\nu):=\\left(\\inf _{\\gamma \\in \\Gamma (\\mu ,\\nu )}\\int _{M\\times M}d(x,y)^{p}\\,\\mathrm {d} \\gamma (x,y)\\right)^{1/p}其中 $\\gamma$ 是一个边缘分布分别为 $\\mu$ 和 $\\nu$ 的联合概率分布，称作 coupling。 Wasserstein Distance 具有的优点： 可以衡量离散和连续分布间的距离 即使两个分布没有重叠，也可以反映他们的远近 考虑到概率分布的几何特性 可以体现如何从一个分布转换为另一个分布 这个距离是平滑的，可以提供梯度信息 一维分布下的计算对于两个分布 $\\mu _{1},\\mu _{2}\\in P_{p}(\\mathbb {R} )$ 和他们的 CDF $F_1(X), F_2(X)$，和对应的 inverse-CDF $F_1^{-1}(X), F_2^{-1}(X)$，有： W_{p}(\\mu _{1},\\mu _{2})=\\int _{0}^{1}\\left|F_{1}^{-1}(q)-F_{2}^{-1}(q)\\right|^{p}\\,\\mathrm {d} q这可以看作分布间差异的的横向求和，可以理解为在 x 维度上对沙土的搬运。 此外，当 $p=1$ 时，可以看作分布间差异的纵向求和： W_{1}(\\mu _{1},\\mu _{2})=\\int _{\\mathbb {R} }\\left|F_{1}(x)-F_{2}(x)\\right|\\,\\mathrm {d} x在运用 Wasserstein 距离的 WGAN中，在对 W 距离进行优化的时候，可以转化为对以下 Loss 进行优化，其中 $P_r$ 和 $P_g$ 分别是真实分布和生成分布： L = \\mathbb{E}_{x \\sim P_r} [f_w(x)] - \\mathbb{E}_{x \\sim P_g} [f_w(x)]在这个简化的数学形式下，和 MMD 十分相似。 Bhattacharyya Distance巴式距离，很好理解，看公式一目了然。 需要注意的是其并不满足三角不等式。 BC(p,q)=\\int {\\sqrt {p(x)q(x)}}\\,dxMahalanobis Distance这个其实不是分布间距离的度量，而是与欧式距离曼哈顿距离同一个层级的概念。 主要是每天听 dzy 说这个距离，就顺便看一眼。 首先要提到欧式距离的缺点： 不同维度等同对待，哪怕单位不同 不考虑维度的相关性（非独立同分布） 举例：身高体重，量纲不同，两者相关 马氏距离的主要思想在于使用主成分分析中的主成分来进行标准化。 由主成分分析可知，由于主成分就是特征向量方向，每个方向的方差就是对应的特征值，所以只需要按照特征向量的方向旋转，然后缩放特征值倍即可。 当个维度独立同分布，则变为欧式距离。 D_{M}(x, y)=\\sqrt{(x-y)^{T} \\Sigma^{-1}(x-y)}其中 $\\Sigma$ 是多维随机变量的协方差矩阵。 参考： Statistical distance Jensen–Shannon divergence From GAN to WGAN 令人拍案叫绝的Wasserstein GAN Wasserstein metric MMD Maximum Mean Discrepancy 最大均值差异 马氏距离(Mahalanobis Distance)","link":"/math/distribution-distance/"},{"title":"深度主动学习批判文章阅读","text":"目前出现一些对深度主动学习批判的文章，结合自己的实践，深以为然，此处将其整理一下。 同时这些文章也收录进了本人 awesome-active-learning 的仓库，详见此链接。 在本文末尾，本人也提出了一些自己的看法。 文章列表： Parting with Illusions about Deep Active Learning [2019, Arxiv] Towards Robust and Reproducible Active Learning Using Neural Networks [2020, Arxiv] Effective Evaluation of Deep Active Learning on Image Classification Tasks [2021, Open Review] 文献概览1. Parting with Illusions about Deep Active Learning [2019, Arxiv]这篇文章指出当前主动学习没有考虑到半监督学习及数据增强等平行设定，于是他们在图像分类和语义分割上做了一个对比实验。 实验中对比到的内容包括：多种 SOTA 主动学习策略、加入数据增强模块的主动学习策略、加入半监督学习方法的主动学习策略、半监督学习策略。 分类任务的结果: 主动学习加入数据增强可以带来提升，但是这会模糊不同主动学习策略间的区别，他们表现都大致相同。 主动学习与半监督学习结合可以带来比单独使用主动学习和半监督学习都要好的表现。 不同主动学习策略的优劣排序在不同数据集上不同。 在样本数量极少时，主动选取的样本表现差于随机选取，包括在半监督学习上的随机选取。 语义分割的结果： 半监督学习加随机选取表现最好。 总结: 当前用于评估主动学习的模式并不好，会带来错误的结论。 半监督学习的加入会大幅提升主动学习表现。 很多 SOTA 的主动学习方法会比随机选取要差，尤其是当标记样本数很小时。 2. Towards Robust and Reproducible Active Learning Using Neural Networks [2020, Arxiv]这篇文章指出在不同的主动学习文章中，作为基准的随机选取策略表现飘忽不定。 为了提高主动学习工作的可复现度和鲁棒性，这里对当前方法做一个公平的对比。 同时本文指出，当前主动学习方法大多忽视了正则化对减小泛化误差的作用。 所以作者也将不同的正则化项 (parameter norm penalty, random augmentation (RA), stochastic weighted averaging (SWA), and shake-shake (SS)) 加入了对比。 图像分类任务的结果: 相比各作者在原文中提到的结果，这里随机选取的表现都要比他们宣称的更好。同时没有策略可以明显超过随机选取。 使用不同的主动学习批选取数目，得到的表现不一致。 主动学习方法没有超越随机选取，且在类别不平衡设定上表现不鲁棒。 加入了 RA 和 SWA 的正则化，主动学习表现明显提升，同时不同策略间表现差异减小。 把选取的样本迁移到不同的模型结构时，不同策略表现不同，但是随机选取表现依然占优。 讨论： 基于这种相同 baseline 表现不一致的情况，强调主动学习的对比应当在一系列限定条件下施行。 不同于 Parting with Illusions about Deep Active Learning 这篇文章，作者认为 主动学习的表现提升仅在有限的的实验条件下才会出现 这种基于随机选取的提升不具有统计学意义 在加入正则化之后，这些对于随机选取策略的提升消失。 正则化在小样本情况下带来的提升可观 对于一些在模型中使用到未标记样本训练的主动学习策略，例如 VAAL 更应该使用半监督学习来作为基线方法。 相信当前主动学习的工作都是基于未正则化的模型，其表现提升不能说是由于样本选取的好。 提出了一个评估的标准： 策略要在不同优化器、模型结构、批选取大小等上保证鲁棒。 模型训练时应当加入 RA 或 SWA 这类的正则化。 迁移测试必须进行，用来保证选取的样本的确是高质量的。 实验应该使用统一的评估平台 实验过程的快照应该留存发布 训练、测试、标记、未标记、选取样本的编码应该被分享。 3. Effective Evaluation of Deep Active Learning on Image Classification Tasks [2021, Open Review]个人认为这篇文章十分全面且实验到位。 本文指出了当前 AL 工作的一些问题： 不同 AL 策略在不同的工作中往往存在矛盾的表现 非故意的排除一些用于深度学习的重要一般化步骤，例如数据增强和 SGD 优化。 对于标记效率等评估方式缺乏认知 对 AL 在什么情形下能超过随机选取没有清晰的认知 作者展示了一个统一的对于 SOTA 主动学习策略在图像分类下的复现，且在几个不同方面做出了分析。 他们指出了一些重要但是目前不清楚的细节： 模型在每个轮循环之后需要从头训练还是可以 fine-tuning？ 使用精心选取的初始集对 AL 有何影响。 有没有一个完美的 AL batch size？或者这个 AL batch size 重要吗？ 什么情形下 AL 可以超越随机选取？什么因素对其产生影响？ AL 的 scalability 如何？具体的，模型的训练和 AL 的选取各需要多少时间？ 以下是他们的重要结论和实用的经验： 数据增强对测试集表现以及标记效率能带来提升。 SGD 表现要比 Adam 表现好。 在使用数据增强和 SGD 优化器时，Diversity 选取策略难以比简单的 Uncertainty 选取表现好。 在加入人工制造的数据冗余时，BADGE 开始表现好于 Uncertainty 选取。 每个类别中未标记样本的数量对于表现有很大影响：每个类别中样本数越少，对于 AL 的提升空间越小。 初始标记数据的选取在几轮循环后对 AL 表现将几乎没有影响。 AL batch size 同样对表现几乎没有影响。 在每轮循环中，重训练模型或者 fine-tune 模型，只影响最开始几轮选取的表现。 最费时的步骤是每轮循环中模型的重训练。 总结、讨论与个人看法总结这三篇文章都是对当前深度主动学习的批判与调研文，个人很钦佩这种刨根问底的态度。 首先这三篇文章有一些共性的结论： 数据增强可以带来性能提升 [1,3] 半监督学习，正则化可以带来性能提升 [1,2]（个人看法，某些类的半监督学习方法可以看作正则化项，于是这里放在一起） 在加入适用于深度学习的提升方法后，不同 AL 策略间的优劣程度变得模糊 [1,2,3] [1,2] 认为应当将半监督学习作为基线方法。 当然也有一些互相排斥的结论： [2] 认为加入正则化之后，主动学习相对于随机选取的提升消失，不具有统计意义，但是 [1,3] 中数据增强后仍相对随机选取有提升。 个人看法就这三篇文章而言，背景是当前一大部分的深度主动学习仅考虑了主动学习的范畴而没有更多考虑深度学习的问题。 仅从主动学习范畴而言，这二十多年来，Pool-Based AL 的架构，评估方式都没有发生过太大变化： 全监督式学习 循环架构并每次从头训练 通过学习曲线评估不同策略 以随机选取样本的监督式学习模型作为最基础的基线 如果从传统模型来考虑，这个主动学习的范畴其实没有太大问题。 但是近十年来，深度学习兴起，其实为主动学习带来了新的实现环境。 之前的有关深度主动学习的调研（见这篇笔记），其实已经指出了很多在深度模型中使用主动学习面临的挑战。 其中第一条就是主动学习期望选取少量重要标记样本，而深度学习期望大量训练样本，两者之间有着矛盾。 所以在深度学习的框架下，不能仅仅考虑主动学习的范畴，也要结合深度学习的情况。 深度学习在这十年的发展中，其实也苦于标记样本数目稀少，但是在深度学习的社区中，采用了一些其他的方式来解决此问题，并取得了十分不错的效果： 数据增强 利用未标记的样本 半监督学习 自监督学习 预训练 所以目前来看，主动学习的范畴致力于用少量的标记样本达到与随机选取相同的表现，深度学习的这些方法致力于用已有的标记数据达到更好的表现。 这两种角度其实是一件事情，放在主动学习评估的学习曲线中，就是横向比较和纵向比较。 其实在评估时，绝大部分的主动学习工作，都是宣称前者但是采纳后者。 换言之就是更侧重于纵向的比较（当然横向好就会带来纵向好毋庸置疑）。 在这种情况下，许多深度主动学习仍然使用传统主动学习范畴就很不公平了。 根本原因在于比较的基线方法是一个明明能获取大量未标记样本却只在少量标记样本上训练的模型。 换句话说，当前的很多深度主动学习方法还是在虐菜，虐那些没有使用先进的深度学习方法且仅在少量标记样本上训练的神经网络的菜。 再打个比方说，主动学习可以看作一种样本维度的提升方式，当前的很大一部分深度主动学习是在对像刀剑一样的武器上进行磨刀抛光提升，表明我磨的比你好。 殊不知现在神经网络经过多年社区的贡献已经变成了诸葛神弩或者是火枪。 所以说现在仍基于对刀剑提升的作为主动学习的评估已经过时了，磨好的刀是比不过未经提升的火枪的。 只有在火枪上进行提升，才可能是当前深度主动学习需要考虑的方向，才是能带来实用价值的方向。 正式的说，就是如何在当前深度学习这些已经验证好用的通用方法下，进一步使用主动学习减少标记成本。 目前来看，在策略设计之外，我们需要对主动学习的框架来进行调整，这些调整和主动学习策略的设计可以是正交的，并不一定互相影响： 首先比较的基线需要是随机选取样本训练的，纳入很多先进成熟深度学习方法的神经网络，包括但不限于：半监督，自监督，正则化，数据增强等方法。 模型的训练可能需要重训练和增强训练交替进行，而不是一味的重训练（考虑到相较于经典方法来说，神经网络训练时间较长。） 评估的方式不能单纯以学习曲线论，labeled efficiency 也是一个更直观的角度。","link":"/paper-reading/deep-AL-criticism/"},{"title":"A Survey of Deep Active Learning","text":"记录一篇深度主动学习的调研文章。 2020年，这篇 survey 被挂在了 arxiv 上， 见此链接。 此篇 survey 全30页，引用189篇参考文献。 IntroductionBackground: A rich variety of related work has been published, DeepAL still lacks a unified classification framework. Challenges of DeepAL: Insufficient data for label samples The labeled training samples provided by the classic AL are insufficient to support the training of traditional DL. The non-batch sample query method commonly used in AL is not applicable in the DL context. Model uncertainty The softmax response of the final output is unreliable as a measure of confidence, and the performance of this method will thus be even worse than that of random sampling. DL model can be too confident about the output results. Processing pipeline inconsistency AL: fixed features + learned classifiers DL: learned features + learned classifiers Only fine-tuning the DL models in the AL framework, or treating them as two separate problems, may thus cause divergent issues. ApproachesIn general they summarized the corresponding approaches for the challenges. Insufficient data for label samples data augmentation assigning pseudo-labels combine supervised and semi-supervised training batch sample Model uncertainty Bayesian deep learning Processing pipeline inconsistency Modify the combined framework of AL and DL to make the proposed DeepAL model as general as possible Query strategiesBatch Mode DAL should be satisfied. The reference indexes are copied. Uncertainty-based and hybrid query strategies directly use uncertainty sampling (non-batch) [9, 59, 114, 123] query batch sample set representative to the distribution [177:Exploration-P, 183:DBAL, 99:WI-DL] the richer the category content of the dataset, the larger the batch size, and the better the effect of diversity-based methods; query batch sample set representative to the gradient embedded space [10:BADGE] query batch sample set representative by adversarial method [146:WWAL, 150:VAAL, 81:TA-VAAL] Deep Bayesian Active Learning obtain the posterior distribution of network prediction [47:DBAL, 118:DEBAL, 28:DPEs, 114:ActiveLink] obtain the mutual information between the batch samples and the model parameters [84: BatchBALD] reconstructed the batch structure to optimize the sparse subset approximation [117:ACS-FW] other [54, 105, 126, 147, 175, 179] Density-based Methods core-set approach [49:FF-Active, 138:core-set] discriminative approach [35:Active Palmprint Recognition, 51:DAL] Other RL approach [37] Adversarial examples[36:DFAL] ensemble [14] flexible acquisition function [57:RAL, 100:DRAL] with NAS [50:Active-iNAS] Model trainingNormally accompanied with insufficient data (under the view of DAL). assigning pseudo-labels [166:CEAL] combine unsupervised feature learning [99:WI-DL] data augmentation [187:GAAL, 162:BGADL, 107:ARAL] add adversarial task [150:VAAL, 81:TA-VAAL] DAL on the generalization of the model Use the final layer output to make query ALso use the middle layer output to make query [59:AL-MV, 178:LLAL, 182]","link":"/paper-reading/deep-AL-survey/"},{"title":"代理模型辅助的演化计算","text":"本文是对 Surrogate-assisted evolutionary computation: Recent advances and future challenges 这篇综述的阅读笔记。 这是一篇发表于2011年的关于在演化计算中如何应用辅助模型的综述。 前言绝大部分的演化计算都假设存在一个可以为每一个个体提供 fitness value 的手段，或是模拟，或是实验，或是一个显式的 fitness function。 但是有的时候，这个 fitness 的评估是非平凡的，例如当模拟或实验需要消耗大量成本。 此时通过使用代理模型的演化计算方法来减少昂贵问题上使用演化计算时间成本。 代理模型往往和真实的 fitness function 一起使用，以防演化计算被误引入代理模型提供的错误最优。 当问题越高维度，代理模型的构建难度就越大。 代理模型的策略 No analytical fitness function exists for accurately evaluating the fitness of a candidate solution. Instead, there are only more accurate and less accurate fitness estimation methods, which often trade off accuracy with computational costs. 使用代理模型也需要权衡模型的效率和保真度。 最初，一部分工作完全依赖代理模型进行演化搜索，但是代理模型引入的可能并不存在的最优会带来严重的问题。 代理模型几乎可以用在所有的演化计算步骤中来剔除差的结果，并且减少随机性：population initialization, cross-over, mutation, local search and fitness evaluations。 根据代理模型的使用对象进行不同，可以将代理模型方法分类可以分为以下三类： Generalization based: surrogates are used for fitness evaluations in some of the generations, while in the rest of the generations, the real fitness function is used Individual based: the real-fitness function is used for fitness evaluations for some of the individuals in a generation Population based: more than one subpopulation co-evolves, each using its own surrogate for fitness evaluations. Migration of individuals from one sub-population to another is allowed. 单代理模型我们假设与真实 fitness 函数交互费时，希望尽可能减少与真实函数交互。 那么关键的问题就在于如何确定哪个个体是应该被重新评估的。 我们需要考虑到以下三个方面。 1. 选取重评估样本的标准不得不说这些选取的方式和主动学习极度相似。 选取具有以下特征的样本评估: potentially have a good fitness value representative large degree of uncertainty in approximation fitness landscape around these solutions has not been well explored improve the approximation accuracy of the surrogate, 如何描述或者估计这种不确定性或者错误呢？ uncertainty is roughly set to be inversely proportional to the average distance to the closest data samples estimating the variance of the individual estimates given by an ensemble of surrogates 2. 如何评估代理模型的好坏首先代理模型并不是需要严格和 fitness function 相同才可以发挥作用，在存在较大的预测错误时同样可以起到作用，如下图所示。 一些常用的度量： mean squared error between the individual’s real fitness value and the predicted fitness the number of individuals that have been selected correctly using the surrogate the rank of the selected individuals, calculated based on the real fitness function. rank correlation: measure for the monotonic relation between the ranks of two variables continuous correlation between the surrogate and the original fitness function. 3. 提升预测的准确性 神经网络的正则化 随着搜索更新模型 建立一个地位的搜索空间 评估时利用生成的中间数据 多代理模型模型类别可能不同，模型保真度（fidelity）可能不同。 此处根据对 fidelity 的掌控把多代理模型分为两类，同质（homogeneous）和异质（heterogeneous）多代理模型。 By homogeneous multiple surrogates, the fidelity of the surrogates are not explicitly controlled, even if different types of surrogates are used. On the contrary, heterogeneous surrogates vary in their fidelity due to an explicit control in model complexity or training data. 同质（homogeneous）多代理模型 多个模型可以提升预测质量，也可以帮助识别较大的预测错误。 其经验有效性在多篇工作中得到证实。 异质（heterogeneous）多代理模型 其实是组合了一些列不同粒度的代理模型。 这类工作提出的背景也是训练不同粒度的代理模型会有不同的成本，粒度越低成本越低。 在昂贵问题之外更多的考虑 Surrogates in interactive evolutionary computation Surrogated-assisted evolution for solving dynamic optimization Surrogates for robust optimization Surrogates for constrained optimization 实际应用代理模型的方法更多的是应用驱动。 One intensively researched area is surrogate-assisted design optimization, such as turbine blades, airfoils, forging, vehicle crash tests, multi-processor systems-on-chip design and injection systems. Other applications include drug design, protein design, hydroinformatics and evolutionary robotics. 未来挑战 Theoretic work Multi-level, multi-fidelity heterogeneous surrogates Surrogate-assisted combinatorial optimization Surrogate-assisted dynamic optimization Rigorous benchmarking and test problems","link":"/paper-reading/surrogate/"},{"title":"Cuda 使用记录","text":"在服务器上使用cuda遇到一些问题，在此记录。 在 torch.cuda.is_available() 返回 False最主要的问题是在 python 中输入torch.cuda.is_available()时，返回 False。 猜测应该是某种版本不匹配造成的问题。 于是先查询版本。 通过nvidia-smi 查询到的版本为 CUDA Version: 11.0， 和我安装的pytorch对应的cuda10.0不兼容。 于是下载对应版本即可解决，注意与 python 发行版的冲突。(下载贼慢) 1conda install pytorch torchvision cudatoolkit=11.0 -c pytorch 查询版本查询版本有三种方法。 1nvidia-smi 这个命令既可以查cuda的驱动API版本，也可以查看GPU运行状态。 查询到的版本为： NVIDIA-SMI 450.57 Driver Version: 450.57 CUDA Version: 11.0。最终需要匹配的版本以此命令为准。 1cat /usr/local/cuda/version.txt CUDA Version 10.0.130 1nvcc --version Cuda compilation tools, release 10.0, V10.0.130 有时会显示command not found，解决方法见此。 nvcc —version command not found首先查看 1ls /usr/local/cuda/bin 存在nvcc命令，此时配置环境变量即可。 .bashrc12export PATH=/usr/local/cuda/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH 然后更新配置文件。 1source ~/.bashrc","link":"/programming/cuda/"},{"title":"Docker","text":"Docker 使用过程中的一点记录。大部分内容来源于此教程。 简单介绍Docker 包括三个基本概念: 镜像（Image）：Docker 镜像（Image），就相当于是一个 root 文件系统。比如官方镜像 ubuntu:16.04 就包含了完整的一套 Ubuntu16.04 最小系统的 root 文件系统。 容器（Container）：镜像（Image）和容器（Container）的关系，就像是面向对象程序设计中的类和实例一样，镜像是静态的定义，容器是镜像运行时的实体。容器可以被创建、启动、停止、删除、暂停等。 仓库（Repository）：仓库可看成一个代码控制中心，用来保存镜像。 Docker 容器通过 Docker 镜像来创建。 容器与镜像的关系类似于面向对象编程中的对象与类。 基础命令Docker run 可以在容器内运行一个应用程序。 1$ docker run ubuntu /bin/echo &quot;Hello world&quot; 以上命令完整的意思可以解释为：Docker 以 ubuntu15.10 镜像创建一个新容器，然后在容器里执行 bin/echo “Hello world”，然后输出结果。 12$ docker images #检查有哪些镜像$ docker ps –a #检查有哪些容器 12345678910$ docker run -it ubuntu /bin/bash #运行交互式容器$ docker run -itd ubuntu /bin/bash #后台启动容器$ exit (ctrl+D) #容器内退出容器$ docker stop [id/name] #停止容器（在容器外）$ docker start [id/name] #后台启用停止的容器$ docker restart [id/name] #重启容器$ docker exec -it [id/name] #进入容器，退出终端容器不停止$ docker attach [id/name] #进入容器，退出终端导致容器停止$ docker rm [id/name] #删除容器$ docker rmi [id/name] #删除镜像 创建镜像更新镜像当我们从 docker 镜像仓库中下载的镜像不能满足我们的需求时，需要对镜像进行更改。 一般有两种方法： 从已经创建的容器中更新镜像，并且提交这个镜像 使用 Dockerfile 指令来创建一个新的镜像 此处我们只考虑更新镜像。 123456$ docker run -it ubuntu /bin/bash # Open a container$ /# # Do modification$ exit # exit container# Create image$ docker commit -m=&quot;has update&quot; -a=&quot;user&quot; container_id image_name 镜像备份1docker save -o image_name.tar image_name 执行后，运行 ls 命令即可看到打成的 tar 包 镜像恢复与迁移首先我们先删除掉 image_name 镜像，然后执行以下命令进行恢复。 1docker load -i image_name.tar 执行后再次查看镜像，可以看到镜像已经恢复。 使用 GPU1docker run --gpus all nvidia/cuda:11.1-base-ubuntu18.04 nvidia-smi","link":"/programming/docker/"},{"title":"早停与验证集损失","text":"一些有关神经网络训练早停及验证集损失的记录。 实验中发现验证集损失与其准确度不严格单调负相关，所以搜索一下答案。 早停训练神经网络时，随着训练迭代次数的增加，训练损失会下降。 而验证集损失一般会经历一个先下降后上升的过程，上升时则为过拟合出现。 在此情况下，一般选用早停来结束当前模型的训练。 对于某种评估指标，假设其越低越好（/越高越好），在训练过程中，如果在一定数量的 epoch 的容忍度之内，其没有比之前的最好评估要低（/高），则停止训练，并重载之前表现最好时的模型参数作为最终的模型输出。 一般的，我们使用在验证集上的损失来作为早停的依据，如果验证集损失不进一步下降，那么则停止训练。（这也是目前来看大多数人用的依据。） 但是早停的依据到底是使用验证集上的 loss 还是 accuracy（或者其他相应的 metric）并没有看到一个统一的说法，互联网上两派的支持者有之。 对于这种情况，George 在其博文中指出了早停的一些问题。 其中最重要的一点是，验证集准确度并不随着验证集损失的减小而严格单调递增，如下图所示。 所以之后 George 实现了一种双重标准的早停，只有当损失和准确率都变糟糕时才早停。 验证集损失与准确率一般来说，验证集损失与准确率呈负相关关系。 但是有些时候，存在验证集损失上升，但是验证集准确性仍进一步提高的情况。 这出现于模型对于预测过于自信和极端的情况。 换句话说，模型倾向于输出较为极端的预测值，这样使得在同样数量分错的情况下，少数预测错的样本主导了 loss，但是对整体准确率影响不大。 以下是一些可能出现的原因： 训练集验证集数据分布不一致 训练集过小未包含验证集所有情况 Github 上也有一个 Repo 来讨论此现象。 对于解决方法来说似乎没有一个明确的结论。 作者在尝试了较小模型和批正则化后仍然会出现此问题。 Reference Early Stopping and its Faults 验证集loss上升，准确率却上升该如何理解？ - 刘国洋的回答 - 知乎 https://stats.stackexchange.com/questions/282160/how-is-it-possible-that-validation-loss-is-increasing-while-validation-accuracy","link":"/programming/early-stopping/"},{"title":"Latex 图片表格排版记录","text":"此处记录一些 Latex 的用法。 图片排版子图排版涉及子图的图片排版可以参考此链接。 在此就不再赘述。 将图片放置于表格中12345678910111213141516\\begin{table} \\centering \\caption{The table of figures} \\hspace*{-0.15\\linewidth} \\begin{tabular}{cM{0.25\\linewidth}M{0.25\\linewidth}M{0.25\\linewidth}M{0.25\\linewidth}} \\toprule &amp; A &amp; B &amp; C &amp; D \\\\ \\midrule K &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} \\\\ L &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} \\\\ M &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} \\\\ N &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} &amp; \\includegraphics[width=\\linewidth]{figure.png} \\\\ \\bottomrule \\end{tabular} \\label{table:table-of-figures}\\end{table} 子图超出行宽强制不换行这个需求是因为在写文章时有时模版会在子图间加距离，导致按比例设置大小的子图超出行距换行。 另一种情况就是有时我们需要稍外超出一些行距来使图片更大一些。 一般来说这里需要使用到 \\makebox 命令。 1234567891011121314151617181920212223242526272829\\begin{figure} \\centering \\makebox[\\textwidth][c]{ \\subfigure[Caption A]{ \\begin{minipage}[b]{0.3\\linewidth} \\includegraphics[width=1\\textwidth]{figure/digit_best.png} \\end{minipage} } \\hspace{-0.05\\linewidth} \\subfigure[Caption B]{ \\begin{minipage}[b]{0.3\\linewidth} \\includegraphics[width=1\\textwidth]{figure/amazon_best.png} \\end{minipage} } \\hspace{-0.05\\linewidth} \\subfigure[Caption C]{ \\begin{minipage}[b]{0.3\\linewidth} \\includegraphics[width=1\\textwidth]{figure/office_best.png} \\end{minipage} } \\hspace{-0.05\\linewidth} \\subfigure[Caption D]{ \\begin{minipage}[b]{0.3\\linewidth} \\includegraphics[width=1\\textwidth]{figure/imageCLEF_best.png} \\end{minipage} }} \\caption{A box of figures.} \\label{fig:box-figures}\\end{figure} 子图引用如果是单张子图，可以直接引用 Fig.~\\ref{子图}。 如果是多张子图，可以使用 Fig.~\\ref{主图}\\subref{子图1}\\subref{子图2} 来进行引用。（但是这个方法无法放到同一个括号里。） 表格排版竖排表格中的文本1\\rotatebox{90}{some rotated text} 表格中文本加粗通常情况下加粗后会使文本变宽，导致表格无法对齐。 前人也遇到过这个问题，并尝试了了不同的加粗命令，发现使用\\pmb可以避免文本变宽情况出现。","link":"/programming/latex/"},{"title":"Linux 常用命令","text":"常用到的 linux 命令语句 进程相关使用 ps 使得命令查询任务。 12# user's running processesps r -ef | grep username 终止进程 1kill PID 高级终止 1234# 将用户colin115下的所有进程名以av_开头的进程终止:ps -u colin115 | awk '/av_/ {print &quot;kill -9 &quot; $1}' | sh# 将用户colin115下所有进程名中包含HOST的进程终止ps -fe| grep colin115 | grep HOST |awk '{print $2}' | xargs kill -9; 后台进程 12345678# 置于后台ctrl+Z# 重新拉到前台fg# 查询后台进程jobs# 删除进程，使用 job 查询到的任务号kill %1 后台运行使用 nohup 使得命令后台运行，同时断网和关闭终端都不会终止任务，适合用来作为任务提交方式。 1nohup /root/start.sh &amp; 此外，python 中自带的 subprocess.popen() 方法同样可以起到后台运行命令且关闭终端不退出的作用，所以不必要使用 nohup。 文件处理解压缩1234# 对于 zip 文件unzip compressed.zip# 对于 tar.gz 文件tar -xvzf compressed.tar.gz","link":"/programming/linux-commands/"},{"title":"Matplotlib 学习笔记","text":"这里是个人的 Matplotlib 学习笔记。 记录一些作图的过程。 用法减小空白边界由于使用 Latex 排版时需要严格控制图片位置、大小、排布方式，所以任何冗余的空间都是需要极力避免的。 在做图的时候可以直接将图片空白边界抹去。 1plt.savefig('test.png', bbox_inches='tight') 但是这个方法有一个问题就是多个图片的留白可能不一致，导致如果多子图放置于 latex 文档中会出现排版无法对齐的情况。 此时需要手动对边界进行设置。 1plt.subplots_adjust(left=0.12, right=0.97, top=0.98, bottom=0.1)","link":"/programming/matplotlib/"},{"title":"Miniconda","text":"Miniconda 与 Anaconda 为 conda 的发行版，主要用于包管理，其中 miniconda 更轻量级。 由于日常使用总是会忘记，所以此处记录一些常用的命令。 安装首先从清华源下载安装包并安装。 123wget -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda2-4.5.11-Linux-x86_64.shbash Miniconda2-4.5.11-Linux-x86_64.shsource ~/.bashrc Conda 默认的软件源在国外,速度非常的慢,我们可以将其更换为清华源。 可以直接在.condarc中添加 .condarc1234- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/- https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/- https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ 之后需要使用 conda clean -i 清除索引缓存，保证用的是镜像站提供的索引。] 注意有的时候会显示 url 错误， 此时换成 https 可能会解决问题。 配置对环境更改创建所需要环境，一般有三种情况。 123456# 创建新环境conda create -n py3.8 python=3.8# 克隆环境conda create --name myclone --clone myenv# 直接从 yaml 文件生成新环境conda env create -f environment.yml 激活环境 1conda activate py3.8 然后下载所需软件包 1conda install XXX 退出环境 1conda deactivate py3.8 删除环境 1conda remove --name myenv --all 批量导入导出组件导出导入 yml 文件。 1234conda env export --no-builds &gt; environment.ymlconda env create -f environment.ymlconda env create -f environment.yml -n new_env_name # 新名称conda env update --file environment.yml # 用于对当前环境更新 导出导入 txt 文件： 123conda list -e &gt; requirements.txtconda create --name py3.8 --file requirements.txt # 顺便创建环境conda install --yes --file requirements.txt # 不创建新环境","link":"/programming/miniconda/"},{"title":"神经网络损失函数","text":"由于总是忘记一些 loss 的常用场景和区别，此处记录一些常用的神经网络 loss。 Pytorch 官方文档中有十余种 loss 函数，其中常用的主要是CrossEntropyLoss、NLLLoss、MSELoss等。 这里仅先对这些常用 loss 展开。 NLLLoss负对数损失，常用于分类任务。 值得注意的是，这里的输入 X 需要已经包含对于相应类别的 log-probability。 使用这个 loss 的时候需要在模型最后加入 LogSoftmax 层。 l(X,y) = L = \\{l_1,...,l_N\\}^\\top, \\\\ l_n = -w_{y_n}X_{n,y_n},其中 X 是输入，y 是目标，w 是类别的权重，N 是 batch size。 具体的计算例子可以看这篇文章。 CrossEntropyLoss交叉墒损失，同样常用于分类任务。 这个标准结合了 LogSoftmax 和 NLLLoss 两个部分。 首相介绍一下 LogSoftmax，其包含 log 函数和 softmax 函数。 \\operatorname{LogSoftmax}\\left(x_{i}\\right)=\\log \\left(\\frac{\\exp \\left(x_{i}\\right)}{\\sum_{j} \\exp \\left(x_{j}\\right)}\\right)对于交叉墒损失，值得注意的是，这里的输入 X 需要已经包含对每一个类，未经处理的 unformalized score。 换句话说，使用这个 loss 的时候不需要在模型最后加入 Softmax 层。 \\operatorname{loss}(x, \\text { class })=-\\log \\left(\\frac{\\exp (x[\\text { class }])}{\\sum_{j} \\exp (x[j])}\\right)=-x[\\text { class }]+\\log \\left(\\sum_{j} \\exp (x[j])\\right)对于加权的情况， \\operatorname{loss}(x, \\text { class })=\\text { weight }[\\text { class }]\\left(-x[\\text { class }]+\\log \\left(\\sum_{j} \\exp (x[j])\\right)\\right)最终的 loss 需要加权平均得到， \\operatorname{loss}(x, \\text { class })=\\text { weight }[\\text { class }]\\left(-x[\\text { class }]+\\log \\left(\\sum_{j} \\exp (x[j])\\right)\\right)其中 x 是输入，class 是目标类，weight 是类别的权重，N 是 batch size。","link":"/programming/neural-network-loss/"},{"title":"Numpy 相关","text":"一些在使用 Numpy 时需要注意的东西 随机数生成为保证实验结果可复现，一般我们使用 np.random.seed(number) 来固定随机种子，之后可保证调用随机数生成器产生的结果相同。 但是在项目规模较大，且需要导入其他包时，这种固定随机种子的办法可能会出现一定问题。 原因在于其他的包中，可能同样会设定其他的全局随机种子 np.random.seed(other_number)。 导致之后生成的样本不是按照自己设定的随机种子来生成。 “The preferred best practice for getting reproducible pseudorandom numbers is to instantiate a generator object with a seed and pass it around” — Robert Kern, NEP19. 解决方法是定义一个随机数生成器，并将其传送到需要使用随机数的地方。 1234import numpy as np&gt;&gt;&gt; rng = np.random.default_rng(2021)&gt;&gt;&gt; rng.random(4)array([0.75694783, 0.94138187, 0.59246304, 0.31884171]) Reference: Stop using numpy.random.seed() NEP 19 — Random Number Generator Policy","link":"/programming/numpy/"},{"title":"Python","text":"这里是个人的 Python 学习笔记。 记录一些常用到但是总忘记的的 Python 知识/语句。 也写一下自己有且被解决的疑问。 用法星号变量单个星号代表这个位置接收任意多个非关键字参数，在函数的*args位置上将其转化成元组，而双星号代表这个位置接收任意多个关键字参数，在**args位置上将其转化成字典。 反射反射是 Python 中很实用的一个功能，他可以通过字符串来导入模块和方法。 123package = importlib.import_module(f'{name_of_package}')class_name = getattr(package, f'{name_of_class}')class_instance = class_name() # Remember the instantiation. 参考这两篇博文： Python 使用反射机制实例化对象 Python 动态导入对象, importlib.import_module() 使用 功能格式化字符串12345678910111213141516171819202122232425## Basics ## use &quot;·&quot; to visualize whitespace&quot;{} {}&quot;.format(1, 2) ## &quot;1·2&quot;f&quot;{1} {2}&quot; ## &quot;1·2&quot;## Padding and alignmenta = &quot;test&quot;f&quot;{a:10}&quot; ## &quot;test······&quot;f&quot;{a:&lt;10}&quot; ## &quot;test······&quot;f&quot;{a:&gt;10}&quot; ## &quot;······test&quot;f&quot;{a:^10}&quot; ## &quot;···test···&quot;f&quot;{a:_&lt;10}&quot; ## &quot;test______&quot;f&quot;{a!s}&quot; ## equals to f&quot;{str(a)}&quot;f&quot;{a!r:10}&quot; ## f&quot;{repr(a):10}&quot;## Floatsb = 0.5f&quot;{b:5}&quot; ## &quot;··0.5&quot; !!!f&quot;{b:&lt;5}&quot; ## &quot;0.5··&quot;f&quot;{b!s:5}&quot; ## &quot;0.5··&quot;f&quot;{b:05}&quot; ## &quot;000.5&quot;f&quot;{b:.3f}&quot; ## &quot;0.500&quot;f&quot;{b:.3e}&quot; ## &quot;5.000e-01&quot;f&quot;{b:.2%}&quot; ## &quot;50.00%&quot; Copy from Yu’s blog. 将 Traceback 信息保存到 log 文件中有时候要跑一个运行时间很长的文件，中间会创建 log 文件。 如果跑到一半报错终止，文件中却没有相应记录则很难追踪错误。 所以需要加入报错信息。 1234567import tracebacktry: main() # The code need to be executed.except Exception as e: logger.error(f&quot;Main program error: {e}&quot;) logger.error(traceback.format_exc()) ~ 的使用与补码使用 ~ 进行按位取反，包含符号位。 举例，5 的二进制为 0101： 各位取反，1010 变为负数，转化为其补码形式（符号位保持不变，各位取反，再加1） 符号位不变，各位取反 1（1101） 再加1（1110），也即 -6 12&gt;&gt; ~5&gt;&gt; -6 一些疑问及回答同一个可迭代对象的多个迭代器是否独立？独立。 这问题其实是在问生成的两个迭代器是不是指向同一个内存。 这个显然是不同的。 所以说用两个迭代器同时遍历一个列表，都会是从头开始。 需要注意的是在 pytorch 中，遍历 dataloader 需要使用迭代器。 此时如果用两个迭代器，在第一次迭代生成的样本可能不同。 这个其实是因为你在生成 dataloader 的时候可能传入 shuffle = True，所以在生成迭代器的时候会采用不同的顺序读区。 使用中的一些坑Terminal 传入 Boolean 类型的参数由于传入的参数以 string 解析，所以还需要对 string 进行判断。 sys.argv provides you the string representation of cmd line params. ‘True’ and ‘False’ are both strings, and if a string has any content in it at all, the boolean representation is True. You should simply change your condition to if test.lower() == ‘true’ 所以在习惯上还是尽量使用 json 文件传参数。 但是如果使用命令行进行调试时，还是需要输入。 所以目前的解决方案是在 argparse 中默认设置为 False，传入 True 则覆盖。 一些常见的报错有一些经常遇到的报错，在此记录解决方案。 ModuleNotFoundError: No module named ‘xxx’这个报错经常在服务器上调用子文件夹中的结果分析软件时得到，通常的原因是因为自建的module包所在路径不在 PYTHONPATH 下，此时把根目录路径加入即可。 参考此博客链接。 1234import sysimport os# 把当前文件所在文件夹的父文件夹路径加入到 PYTHONPATHsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))","link":"/programming/python/"},{"title":"Pytorch 踩坑","text":"使用 Pytorch 时学到的一些知识 1. 用法 1.1. 随机种子 1.2. zero_grad optimizer or net？ 1.3. 初始化网络 1.4. nn.module 中 __call__ vs forward 1.5. NLLLoss &amp; CrossEntropyLoss 1.6. tensor 非 contiguous 导致无法使用 view() 1.7. pytorch 中 hook 的使用 1.8. 查看某一层梯度 1.9. 计算某一层梯度 1.10. 计算梯度的时间 1.11. 增加与删减维度 1.12. 允许 batch 中的样本不等长 1.12. 对 BatchNorm 的参数进行固定 1.13. 对使用线程数进行固定 2. 设置 2.1. Dataloader 中的 num_workers 造成训练循环缓慢 3. 报错 3.1. RuntimeError: CUDA error: device-side assert triggered 3.2. RuntimeError: CUDA out of memory 3.3. RuntimeError: Function ‘MulBackward0’ returned nan values in its 0th output. 1. 用法1.1. 随机种子 在导入文件之前，先导入与随机种子相关的包，这样导入的文件随机数也被确定。 在文件的开头添加以下代码： 1234567891011def seed_torch(seed=1029): random.seed(seed) os.environ['PYTHONHASHSEED'] = str(seed) # 为了禁止hash随机化，使得实验可复现 np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) # if you are using multi-GPU. torch.backends.cudnn.benchmark = False torch.backends.cudnn.deterministic = Trueseed_torch() 1.2. zero_grad optimizer or net？ model.zero_grad() and optimizer.zero_grad() are the same IF all your model parameters are in that optimizer. It is safer to call model.zero_grad() to make sure all grads are zero. e.g. if you have two or more optimizers for one model. 1.3. 初始化网络网络参数初始化会对模型表现产生影响，一般通过一些随机的方式初始化参数。具体的影响可以见这篇博文。 具体如何实现网络权重初始化，可以通过对模型每一层遍历赋值实现，参见如下代码。 12params = list(net.parameters())torch.nn.init.xavier_uniform(layer) for layer in params 1.4. nn.module 中 __call__ vs forward call 方法中调用了 forward 函数，区别主要在于如果使用 forward 函数来进行前向传播，则无法使用 pytorch 提供的 hook 功能。 1.5. NLLLoss &amp; CrossEntropyLoss从文档中： This CrossEntropyLoss criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class. 可以简单理解为： CrossEntropyLoss == LogSoftmax + NLLLoss 那我们为什么要用 LogSoftmax 呢？ 因为在实现上，算log值更加便捷，如果直接计算指数值，可能会出现极大或者极其接近0的情况。 所以使用 LogSoftmax 的话数值稳定性可能会更好。 参考此链接。 1.6. tensor 非 contiguous 导致无法使用 view()当使用 tensor 操作时，新建了一份 tensor 元信息，并重新制定 stride，导致其不连续，无法使用 view()。 最简单的解决方法是使用tensor.contiguous(), 此时会重新开辟一块内存储存底层数据。 若不介意底层数据是否使用了新的内存，用reshape()则更方便。 这篇文章提供了一个非常完善的解释。 1.7. pytorch 中 hook 的使用Pytorch 中的 hook 为我们提供了一个较为方便的方式来访问网络某一层的输入与输出（前向的话返回 feature，反向的话返回梯度。） 具体的使用方法，首先要在相应的层上打开前向或者反向的 hook： 1234567891011121314151617181920212223242526272829# forward# 定义 forward hook functiondef hook_fn_forward(module, input, output): print(module) # 用于区分模块 print('input', input) # 首先打印出来 print('output', output) total_feat_out.append(output) # 然后分别存入全局 list 中 total_feat_in.append(input)# Add hook on the layer you wantmodules = model.named_children() # for name, module in modules: module.register_forward_hook(hook_fn_forward)################################################ backwarddef hook_fn_backward(module, grad_input, grad_output): print(module) # 为了区分模块 # 为了符合反向传播的顺序，我们先打印 grad_output print('grad_output', grad_output) # 再打印 grad_input print('grad_input', grad_input) # 保存到全局变量 total_grad_in.append(grad_input) total_grad_out.append(grad_output)modules = model.named_children()for name, module in modules: module.register_backward_hook(hook_fn_backward) 注意 register 函数接受的是一个函数，会为传入的函数传递三个参数 module， grad_input， grad_output。 这里的 input 和 output 都是以前向网络的方向来进行标记的。 反向传播中对于线性模块：o=W*x+b ，它的输入端包括了W、x 和 b 三部分，因此 grad_input 就是一个包含三个元素的 tuple。 而在 forward hook 中，input 是 x，而不包括 W 和 b。 详见这篇非常好的讲解。 1.8. 查看某一层梯度hook 是一种提取梯度的方法，同样的，还有其他方法可以提取梯度。 12# 一个全链接层举例list(model.modules())[5].weight.grad 1.9. 计算某一层梯度其实如果使用 loss.backward() 然后再利用 hook来提取梯度会有一些耗费时间，因为反向传播是要从尾到头的，如果你只需要倒数几层的梯度的话，其实可以直接计算。 torch.autograd.grad 方法提供了一个计算梯度的方式，可以看以下例子，此方法返回的对象是一个仅有一个元素的元组。 123# 计算梯度# 如果需要多次计算的话记得保留计算图grad= torch.autograd.grad(outputs=loss, inputs=W, retain_graph=True, only_inputs=True)[0] 一般来说当我们计算每一个样本所引起的梯度时，可以将 batch_size 设为1，然后分别求梯度。 但是这样是比较费时的，所以可以使用 autograd.grad 中的 is_grads_batched 选项 (Pytorch 1.11 版本)。 在源代码中对其这样描述： 1234'''is_grads_batched (bool, optional): If ``True``, the first dimension of each tensor in ``grad_outputs`` will be interpreted as the batch dimension.'''grad= torch.autograd.grad(outputs=loss, inputs=W, retain_graph=True, only_inputs=True, is_grads_batched=True)[0] 1.10. 计算梯度的时间在我的实验终有一个计算每一样本对梯度贡献的需求，有两种方法计算： 将 batch_size 设为1，然后使用 torch.autograd.grad 计算梯度。 用非1的 batch_size，计算 loss 时，不 reduce，这样得出来的 loss 是一个向量。遍历这个向量，对向量中每一个tensor使用 torch.autograd.grad 计算梯度。 但是发现一个问题。 在 batch 下，平均每个样本的前向时间是要远小于不使用 batch。 但是平均每个样本的后向时间是要远大于不使用 batch。 （这里远小远大是指数量级）。 推测原因为如果遍历向量的话的话，获得的 tensor 中 grad_fn 是 UnbindBackward 而不是 nlllossbackward。 所以尝试在计算 loss 之前就对样本进行遍历，但是其实时间上和遍历 loss 是一样的。 因为是用 loss 计算梯度是要使用之前的计算图，遍历网络输出会使遍历的每一个输出的 grad_fn 变化。 用这种方式虽然看起来 loss 的 grad_fn 还是 nlllossbackward，但是在梯度的计算过程中还是会遇到 UnbindBackward。 所以这个问题没有想到具体的解决方法，就选取了耗费时间相对较短的方法。 1.11. 增加与删减维度有时在对批数据进行乘法等矩阵操作时，时常需要对数据进行升维降维。 此处记录一下操作流程。 例如我们相对两个矩阵进行乘法，第一个矩阵唯度为[N, M]，第二个矩阵维度为[N, K]，其中 N 为该 batch 中样本数目。 我们期望通过得到一个维度为[N, M, K]的三维矩阵。 但是直接的矩阵相乘并不能起到升维的效果，所以在相乘之前要进行升维。 将两个矩阵分别升维到[N, M, 1]和[N, K, 1]。 此处用到两个方法： torch.squeeze(n)：若第 n 维维度为1，则将此维度删除。 torch.unsqueeze(n)：将第 n 为维度增加维度为1。 有时在增加删减维度之后，需要对原始维度进行重新排序，此时可以用到torch.permute()方法。 1.12. 允许 batch 中的样本不等长一般情况下 pytorch 中，每个 batch 的每一个样本都是等长的，如果不等长的话会报错。 1RuntimeError: each element in list of batch should be of equal size 一般来说，这是因为 Dataloader 中的参数 collate_fn 的默认值为 torch 自定义的 default_collate，collate_fn 的作用就是对每个batch进行处理，而默认的 default_collate处理出错。 自定义的 default_collate 的作用是将列表中的元素变成 tensor 的形式，详见这篇文章，同时源码见这里。 所以这个时候的处理方式是自定义一个 collate_fn，并在其中使用 padding，将每个样本扩充至等长，使得变为 tensor 这一过程不出错。 参考了这篇文章。 1.12. 对 BatchNorm 的参数进行固定采用预训练模型的时候，其中经常包含 BatchNorm 层，在对模型进行改造的时候，有的时候会出现问题，这个时候就需要对 BatchNorm 层进行一个固定。 123for m in net.modules(): if isinstance(m, nn.BatchNorm2d): m.eval() 1.13. 对使用线程数进行固定自己在使用 Pytorch 的时候发现，有时候一个文件会占用很多的 CPU 核心。 当提交多个任务时，其会将所有 CPU 快速占满，且难以高效运用，导致影响所有的实验任务。 于是对单个任务的 CPU 使用加以限制成为了一个需求。 此时可以使用以下命令来限制 torch 使用的线程数。 1torch.set_num_threads(1) 2. 设置2.1. Dataloader 中的 num_workers 造成训练循环缓慢在本地跑实验，一个简单的网络的训练，发现 Dataloader 中 num_workers 设置的数目越大，在 batch 中训练越耗时，表示莫名其妙。在我的情形下将其设为8要比将其设为0慢了百倍以上。 仔细看了一下 mini-batch 的训练过程并且记录了一下时间，发现主要的时间开销发生于 for 循环遍历 loader 之后退出循环时。 所还还是将其设为了0。 造成这个的主要原因可能是 IO 耗时和模型前/后传耗时之间的 GAP 太大，导致进程间造成了阻塞 PyTorch DataLoader 初探 Pytorch训练加速技巧 3. 报错3.1. RuntimeError: CUDA error: device-side assert triggered参考此篇文章。 一般来说这个报错存在于在 GPU 运行时，不易清晰定位到错误源，所以网络上大家给出的建议是去 CPU 上跑一下。 这个错误出现的原因是数据中的类标记label和网络中的类标记label不匹配。包括但不限于以下几种问题。 pytorch识别的类别 数据中的类别 [0,1,2,3] [1,2,3,4] [0,1] [0,1,2,3] 解决方法只要找到矛盾发生的地方，对数据中类别的标签进行改动即可。当然有的时候也可能是网络格式写错。 3.2. RuntimeError: CUDA out of memory起因在于丢了49000张 mnist 数据进去没有分 batch，本来以为数据的大小只占了450m内存应该不会有问题，但是发现跑了一个前向就加了七八个g的显存，甚至一个模型直接把24g的显卡显存跑炸了。 分析原因应该是因为 batch size 较大的时候，前向输入模型，在某一层计算时申请了很大的 tensor 导致消耗了成倍与数据大小的显存。 这个在小 batch 的情况下应该并不会有太大影响，所以说还是需要使用 batch。 当然还是可以在需要的时候释放缓存，治标不治本。 1torch.cuda.empty_cache() 这篇文章简要介绍了 pytorch 的缓存机制。 3.3. RuntimeError: Function ‘MulBackward0’ returned nan values in its 0th output.其实很多类似的这种报错，只是 Function 不一样而已。 出现这种情况的原因是出现了梯度爆炸（gradient-explode），导致出现 NaN 值。 解决的方法是先定位，再处理。 定位可以使用 detect_anomaly，找到报错位置。 12with autograd.detect_anomaly(): loss.backward() 处理的话方法不定，一般来说是需要让梯度不要过大。 有一些人提到可以减小学习率，但是这个治标不治本（到底多少才算小呢？）。 所以比较好的方法还是在容易出现梯度爆炸的地方进行处理。 一般来说容易出现梯度爆炸的地方是 log 函数中，在对概率求熵时出现。 所以可以给概率值加上一个很小的数值，如下所示： 123def calculate_entropy(y_softmax): entropy = - torch.sum(y_softmax * torch.log(y_softmax + 1e-6), dim = 1) # Avoid gradient explode. return entropy","link":"/programming/pytorch/"},{"title":"Vim 常用功能记录","text":"Vim 是一款文本编辑器。 说来惭愧，自己在日常使用中并不熟练，于是此处记录一些有需求的功能，便于查阅。 四种模式 正常模式：&lt;ESC&gt; 命令模式：: or / 插入模式：i or a or o（new line） 可视模式：v, V, &lt;Ctrl&gt;+v 正常模式下常用命令基本： 保存退出：:wq or ZZ 不保存退出：:q! or :qa! 光标位置和文件信息：&lt;Ctrl&gt;+g 显示及取消显示行号：:set nu and :set nonu 定位到 n 行：:n 翻页：ctrl+f（下一页） ctrl+b（上一页） 搜索： 搜索：/+&lt;content&gt; 继续搜索：n（向下） or N（向上） 跳转至从哪里来：&lt;Ctrl&gt;+o（向后） or &lt;Ctrl&gt;+i（向前） 对应符号的跳转（例如括号）：% 正常模式下编辑插入或更改： 插入模式：i or a or o（新一行） 替换字符：r+&lt;char&gt; or R+&lt;char&gt;（多个字符） 删除： 删除当前字符：x 删除当前单词：dw 删除2个单词：d2w 删除当前行：dd 删除2行：2dd 删除到当前行尾：d$ 撤销删除 撤销：u（单次） or U（整行） 整行重做：&lt;Ctrl&gt;+R 复制粘贴： 复制：y（在可视模式下选中） 粘贴内容：p（） 替换文本： To substitute new for the first old in a line type :s/old/new To substitute new for all ‘old’s on a line type :s/old/new/g To substitute phrases between two line #’s type :#,#s/old/new/g To substitute all occurrences in the file type :%s/old/new/g To ask for confirmation each time add ‘c’ :%s/old/new/gc 移动光标行内移动光标： 向前移动两个单词至词首：2w 向前移动两个单词至词尾：2e 移动到行首：0 移动到行尾：$ 跨行移动图标（显示什么内容）： 光标定位到第 n 行的行首 nG 光标定位到第一行的行首 gg 光标定位到最后一行的行首 G 光标定位到当前屏幕的第一行行首 H 光标移动到当前屏幕的中间 M 光标移动到当前屏幕的尾部 L 把当前行移动到当前屏幕的最上方，也就是第一行 zt 把当前行移动到当前屏幕的中间 zz 把当前行移动到当前屏幕的尾部 zb 其他功能大小写转换 将光标下的字母改变大小写 ~ 将光标位置开始的3个字母改变其大小写 3~ 改变当前行字母的大小写 g~~ 将当前行的字母改成大写 gUU 将当前行的字母全改成小写 guu 将从光标开始到下面3行字母改成大写 3gUU 将光标下的单词改成大写。 gUw 将光标下的单词改成小写 guw","link":"/programming/vim/"},{"title":"好文阅读转载","text":"此处记录一些之前阅读，感触颇深或者觉得有趣的文章，并附连接。 人生成长 收益值与半衰期 - 采铜：从个人提升的角度，评估一件事是否该做。 25岁的年轻人，要想清两件事 - 北冥乘海生：定义人生的目标函数，弄清人生的约束条件。 胸怀伟大理想 践行爱国情怀 | 学术人生：朱松纯在北大元培学院对2021级新生的演讲，介绍了人的“中心极限定律”。 宗教信仰 怎么样回答外国朋友“这个世界人人有寄托，为什么中国人没有信仰”这个问题？ - 赵浪：简单宗教探讨中，如何进行辩经。 哲学 我们穷极一生，究竟追寻的是什么？ - 曹哲 写作与表达 你以为我在剑桥读经典，其实我不过是学会说话：\b准确使用语言。","link":"/reading-note/article-sharing/"},{"title":"《世界简史》读书笔记","text":"通俗历史的高度概括，从空间时间的起源到世界政治与社会的重建。 作者H•G·韦尔斯（1866-1946），英国著名作家。早年在一家布店当过学徒，后毕业于英国皇家学院。 曾尝试以教书为生，却以新闻和文学创作闻名于世。 其所著《时间机器》、《隐身人》为现代科幻小说开山之作。 一生涉猎甚广，虽不是历史学家，却以《世界史纲》跻身于史学大家之列。 （摘自豆瓣） 内容简介《世界史纲》乃鸿篇巨制，煌煌近百万字，非一般读者所能吸纳。 韦尔斯于1923年出版的这本包含着“崭新立意和写法”的《韦尔斯世界简史》，作为前者的普及版。 作者以无比开阔的视界、轻快简洁的笔调将自生物起源以来的生物及人类历史，有条不紊地展现在读者面前，作者的目的不拘泥于考据，而在于给读者提供一个宏伟、宽广的大视野。 （摘自豆瓣） 个人看法极简评价惊讶于二十世纪初作者对世界历史的了解。 史实之外，作者还加入了很多对事件发展变迁的认识。 帝国的此起彼伏、潮起潮落，在作者的笔下以平实的语言叙述出来，并且能直接地指出变迁的关键事件（原因），对于帮助认识历史有着极大的好处。 本书跨度极大，从宇宙地球讲起，直到一战前的世界格局，从原始的部落社会，到后来的资本主义和社会主义，可以说是一本极好的科普书籍。 感想看法 这本书讲了很多生命起源，原始人什么的。对我而言很无趣且冗长。 讲了很多公元前的埃及巴比伦那边的人种，包含大量的地名人名，没有配合图片难以阅读。 发现对熟悉领域的扩展阅读似乎总是比不熟悉的领域要看得舒服一些。大量的欧洲史个人就觉得有点晦涩难懂。 有点感叹，在写成这本书的无互联网的年代，搜集到如此多的历史信息并成文是一件很不容易的事情。 一个国家的灭亡根本的原因会是什么呢？不得人心吗？还是经济无力支撑？ 教皇原来曾经有这么大的影响力。十字军东征原来是教会扫除异教徒的行为。 蒙古人的西进并非人多势众野蛮暴烈，他们的行军很有战略规划和情报刺探。而基督教诸国却对自己的敌人一无所知。这对自己原来对蒙古领土横跨亚欧大陆的认识相差甚远，以前只是觉得可能蒙古人太骁勇了。 元朝看起来只是蒙古统治的一个分支。 教会压制了人们对自由科学的追求，教会发达的时候自然科学完完全全被压制。就让人想到中国的这些儒家文化道家文化，从来都是三纲五常，可能有着对人探索精神的压制，或许这也是中国自然科学没发展起来的原因之一。 原来是蒙古人把枪炮和火药带到的西方。 工业革命开始的这段科技发展的时间在历史长河中实在是过于的短暂，或许可以说这个是需求推动的科技发展？ 其实对中华文明的概念越来越模糊，元朝算吗？清朝算吗？如果算的话，如果日本占领了全中国建立了满洲国，那满洲国算吗？感觉值得思考。 交通真的对稳定统治起到了极其重要的作用。 历史上撕毁协议的事情真的是屡见不鲜。 内容摘录本就是简史，此处不会详细记录，仅仅记录自己认为有趣的描述和事实。 这本被后世基督徒称为《旧约》的，正是希伯来《圣经》，他大约成书于公元前400年或公元前500年。 《奥德赛》，这是一部历险记，主要内容说的是，希腊人英明的首领奥德赛从特洛伊返回自己的国家时，在漫长旅途中的冒险故事。 公元前六世纪，在人类历史上具有非同凡响的意义。当时，在希腊，哲学家们开始探讨人类在宇宙中的地位，在印度，释迦摩尼开始布道解惑，在中国，孔子和老子的学说广为流传。 一种盛行于雅典的高尚死法是苏格拉底的最终选择：在众目睽睽之下，喝下用有毒草类炮制而成的毒酒。 柏拉图是第一个向我们描绘“乌托邦”的人，那是一种不同于现有社会、比任何时代的社会更加美好的社会蓝图。《理想国》这本柏拉图早起的著作，讲述了一个共和主义贵族的梦想。 罗马和汉朝这两个处于同一时代的庞然大国，却对彼此一无所知。 罗马帝国之所以能崛起，主要是依仗着早期民权的意识团结了人民。罗马帝国之所以灭亡就在于他失去了意志。 公元751年中国人曾袭击撒马尔罕的阿拉伯人，然后被击败被俘获的中国人中有一批熟练的造纸行家，于是造纸的技术就这样传过来。 蒙古人对欧洲征服活动，极大的刺激了欧洲人的地理想象力。 等欧洲步入16世纪以后历史的兴趣，已经从王朝的更迭，转到了政治和社会组织实验的广泛性与多样性上。 枪炮和火药打破了住在城堡中的贵族和有城墙防护的城市的安全感，也彻彻底底地埋葬了封建制度。 一种对抗力量出现了，他避免了联邦的分裂，并在世界各地发挥着重要作用。这就是蒸汽轮船的发明以及铁路和电报的相继出现。他们拯救了美国，把分散的居民结合起来，使美国成为第一个现代化的国家。 从本质上来说，世界上任何政治协议都是暂时性的。 到了，19世纪的后半期德国科学领域的进展已经遥遥领先，每一位科研人员要是不想落伍，就一定得掌握德语，以便去进行学习。 到了19世纪以后，那些有头脑的人越来越清楚地意识到比起劳役苦工，一般平民更加可贵，必须要让他们接受教育，因为他们起码要知道自己在干什么，这样才能确保工作效率。 现在我们所说的社会主义实际上等同于一种集体主义，他允许私人占有相当数量的财产，但是教育运输矿山土地所有权重要物质的生产，则主张控制在具有高度组织性的国家手中。 为促进世界范围的繁荣十分有必要在全世界进行通畅的自由贸易，个人主义者对国家的敌意实际上就是对关税和国界设定的不满，就是对法定关税和国界限制了国际自由行为和运动的不满。 个人主义和社会主义这两个理论虽然立场完全相反，但他们共同探索着同一个问题，即为人们如何才能共同劳动这一问题找出建立在更广泛的社会和政治基础上的解决方案。","link":"/reading-note/brief-world-history/"},{"title":"合作的进化","text":"《合作的进化》 罗伯特·阿克塞尔罗德 极简简介罗伯特·阿克塞尔罗德是美国科学院院士，著名行为分析和博弈论专家。 主要由他在博弈论和复杂性理论上基础性突破而广为人知。 他是把计算机模型运用到社会科学问题领域的权威学者。 这本书是一本乐观的书。 本书从一个重复囚徒困境的竞赛结果出发，分析了表现优异策略的特点，并分析推广到社会生活中。 阐述了合作如何诞生与发展，与合作存在的环境。 通过分析“一报还一报”，将其特点作为重要元素，指导我们真实的合作进程。 个人看法极简评价这是一本蛮有趣的社会学书籍，其研究方法较为严谨符合逻辑。 本书主张的观点，其实是可以为每一个正直人所用，的确有一些受益。 最主要的观点在于合作中成功策略应该有四个特性，从个人角度看来，是可以在日常生活中运用的。 这样在善良的前提下，可以保护自身利益。 此外作者对于合作产生的背景的研究也蛮有趣。 我觉得这也是一个比较重要的部分。 其重要性在于作为管理者，如何使被管理的部门产生且维持高效的合作。 这样其实是会增大整体的效应。 本书主要观点其实不多。 翟老师之前的介绍中其实已经基本涵盖。 书中主要是多了很多实验细节和阐述，其实不需要刻意来读。 缺点（喜爱挑毛病）这种基于强假设的逻辑推演局限性明显。 因为真实环境，假设并不一定成立。 此外计算机的模拟其实并不能很好的作为“提炼道理”的工具。 这就有些像日常生活中看到什么事，悟到了什么道理，这个事未必全面，这个道理未必准确。 这也是很多 heuristic 方法的通病。 个人认为这本书其实是提出来一个假说。 但是假说要通过演绎才能使其更加完备。 按节讨论/摘录第一章：合作的问题在什么条件下才能从没有集权的利己主义者中产生合作？ 国与国之间人与人之间都会面临这个问题。 本书重点研究追求各自利益的个体行为，并分析在社会系统中有哪些因素影响个体行为。 目标是建立一个合作理论以帮助我们理解合作出现的必要条件。 基本假设是个体追求自身利益。 假设下一步对局的收益是上一步对局收益的w倍（w是折扣系数，小于1）。 那么如果折扣系数足够大，则不存在独立于对方所采用策略的最优策略。 举例，参议院中，你最好喝将来会回报合作的人合作，而不是与将来行为不太受现在影响的人合作。 囚徒困境的框架： 对策者的收益不可比较。 收益不必对称。 对策者收益是相对的而不是绝对的。 决定是否合作不必顾及他人的看法。 不必假设对策者是理性的，不必假设她们总是企图争取最大利润。 对策者的行为不必都是有意识地选择 第二章：“一报还一报”在计算机竞赛中的胜利“一报还一报”策略在重复囚徒困境中胜出。 同时在演化计算的方式下，“一报还一报”在代际间保留，且比例增加。 在演化后期，一些前期成功的不善良程序转折变差。 一个成功决策应有的四个特性： 对方合作你就合作以避免不必要的冲突。 对方的背叛你是可激怒的。 在给挑衅以反击后你是宽容的。 行为要简单清晰。 第三章：合作的建立探索结果的适用范围。 简单来说要求个体有足够大的机会再次相遇，形成未来打交道的利害关系。 在进化中数学表达就是当折扣系数足够大时，一报还一报是“集体稳定的”，不容易被侵入。 相似的，一个被认为在下次选举落选的国会议员很难在原有的信任和声誉上和同僚们做立法交易。 “总是背叛”的策略总是集体稳定的，可以阻止个体的侵入。 但是新来者是一个小群体的话，就有机会建立合作。（相遇匹配不随机，新来者之间有机会建立合作。） 善良的策略不能被单个个体侵入，那么也不能被这类个体的小群体侵入。 这样合作的进化可以分成三个阶段： 起始阶段：合作可以在无条件背叛的世界里产生。有交往的可能，合作便会出现。 中间阶段：基于回报的策略在许多不同类型策略中成长起来。 最后阶段：基于回报的策略一旦建立，能防止其他不太合作的策略的侵入。因此社会进化的齿轮是不可逆转的。 第四章：一战中“自己活也让别人活”的系统四五章具体说明这个结果的适用范围。 第四章论述“自己活也让别人活”系统。 讲述了一战英德前线对峙的例子。 在都没有前进的企图时，双方愿意一起克制而不愿交替采取敌对行动。 合作的开始，有同时进餐的因素，也有糟糕天气的因素。 同时在合作中双方也会证明自己有必要是会报复的。 当然也有很多影响合作的问题： 部队的换防 炮兵更不容易受到报复（在克制和报复中起到重要作用 最终由司令部的突然袭击破坏合作） 第五章：生物系统中的合作进化战壕里的士兵清楚理解和认识到回报在维持合作中的作用。 但是参与者的这种理解并不是合作出现和稳定必须的。 合作可以在没有预见的情况下产生。 这一章主要是在生物学的角度论述。 在没有明显亲缘关系的情况下，自然界中形成了许多共生关系。 这一章并没有太大兴趣，可以之后再仔细阅读。 第六章：如何有效的选择预见对于合作的进化不是必要的，但是的确很有帮助。 在持续的囚徒困境中如何表现，作者向个体选择提供四个方面的建议： 不要妒忌对方的成功 要求自己比对方做的更好不是一个很好的标准，否则可能导致危险的冲突 一报还一报总是表现不错，但是他从来没有比别人得更多的分。 不要首先背叛 要对合作和背叛都做出回报 不耍小聪明 不要用一些复杂的方法来推断对方，这些推断通常是错误的。 自己的策略复杂到对方不可理解是非常危险的。 在非零和博弈中，诀窍在于鼓励合作，表明你愿意回报。 第七章：如何促进合作改革者通过改变相互作用的条件来促进合作： 增大未来的影响，增加接触频率 改变收益值，使利害关系可以被看到 教育人们相互关心 教育人们要回报 改进辨别能力 第八章：合作的社会结构合作理论应用的领域。 不同类型社会结构如何影响合作发展方式。 四个能引起有趣的社会结构形式的因素： 标记：划分种群。 信誉：体现在其他人对他采用策略的信心上。 管理：政府处理个人间公司间纠纷。 领地：策略在领地上传播。 第九章：回报的鲁棒性进化的方法基于一个简单的原则：成功的东西可能在将来经常出现。 这本书研究从在没有集权的情况下，合作如何从自私者中产生，扩展到当人们却是关心会发生什么和当有集权时又会发生什么。 其中友谊不是合作的进化必要的，预见性也不是必要的。 从小群体开始合作，在善良、可激怒和某种程度的宽容规则中逐步成长，一旦成为一个群体，采用这种有识别力的策略的个体就能保护自己不受侵入，总体的合作水平是在上升而不是在下降。 换句话说，合作的进化是不可逆转的。 最有价值的发现是：具有预见能力的参与者了解合作理论的真谛后，可以加快合作的进化。","link":"/reading-note/evolution-of-cooperation/"},{"title":"亲密关系","text":"《亲密关系》 克里斯多福·孟 极简简介这是一本介绍亲密关系的书，两性关系为主（relationship）。 其重点讲的是从作者角度出发亲密关系中不愉快的成因及少数他认为的解决办法。 个人看法间断地浏览了一遍这本书，或许遗漏了重点，此处仅以自己印象来进行评价。 可能有些地方记错记漏，但是无所谓，不喜勿喷。 以下所有仅从个人角度出发。 极简评价并不是一本可以学到知识的书，但是如果比较善于推广反思，倒是可能会有一些收获。 简单理性人寻求知识不建议阅读。 如果对自己的感情状况毫无头绪无从下手可以适当阅读。 缺点（喜爱挑毛病） 自己造定义，但是讲述又不严谨。 所以这本书不能以工具书的角度来看，但也不能以闲书的角度来看，我就因为总忘了定义导致可能理解有出入。 几乎把所有的问题归因到了原生家庭和小时候的痛苦。 不失有一定道理，但是看了整本书都是这么一套 cliche ，便觉得有点点肤浅了。 很多问题都是由他自己的经验出发举例说明。或者说讲了很多有的没的的故事。 举例论述其实并不很有说服力，且不具有覆盖性。 给人一种看起来这个人怎么这么善于反思这么厉害的样子。（成功学大师的影子） 很多说得很绝对的话，虽然可能有一定道理，但是明明是 opinion 却以一种 fact 的样子讲出来，让人不舒服。 举例：“我们所看到的每件事，其实都是我们内心的投射，我们怎么评论别人就是我们怎么看待自己。” 其实没有提到有效的解决方案。 只是很乐观的提沟通，提去爱，神神叨叨的讲什么灵魂真理。 他其实提倡对伴侣没有期望。 Ridiculous 感想/看法 这个作者每次讲观点总是说的不是很一致（说的不清楚，可能他自己也没搞清楚），而且最后也总是引到奇奇怪怪的结论。所以我此处给他的观点总结一下，这也可能是这个作者唠唠叨叨写了一本书想讲的东西。反正我总结完觉得还是有点道理。 其实很多亲密关系中面临的问题都只是导火索，究其根本，悲伤失落或者生气的原因都不是导火索看起来的那样子。【背景】 这件事其实很多人意识不到，以为只是导火索这件事的问题（或者说很多过去的没点燃的导火索的问题）。【背景】 稍微深一点的原因在于不同的人生活上需求可能不同（eg. 不同的生活习惯）。【过度】 更深层的原因在于心理上的需求，这一点不同人可能是相同的（eg. 都希望被尊重）。【原因】 对于心理上的需求可沟通解决。【解决】 其实面对很多问题的时候，跳出事件本身，客观的想一想自己要什么可能会好一些。 我不认为要无期望或者低期望。 我觉得人与人之间的相处总是以期望为前提的，无期望这件事太理想化了。 但是对人的期望最好同时也是对自己的期望。比如你期望伴侣让你快乐，为什么不可以自己快乐呢？当自己内心强大的时候，对别人期望的落空其实并没有那么难以接受。 句段摘录可能是一些我觉得可能有点道理的话。 最悲哀的是，在得到满足前，我们不愿意去爱自己的伴侣，紧抱着需求不放手，又不让自己去爱。 我们真正需要的，没有人能给。 如果对别人取悦我们的能力抱有太大的期望，那么失望便会是必然的结果。 不满意时，问问自己此时此刻希望从伴侣身上得到什么。 愿不愿意放弃这项期望，是由自己满足还是由伴侣满足。 我们宁愿争吵也不愿面对伤口，是因为生气比承受心碎简单的多。 过去的创伤并不会随着时间逝去。每个自我局限的信念，都来自于过去的创伤。 找出这些信念在我们心中驻足的所在，将会很有帮助。 亲密关系让我们有机会面对并治好旧伤，从而改变衍生自伤痛的错误想法。 当被卷进权力斗争的漩涡时，你一定要切记，我生气的原因不是我自己想的那回事。 对伴侣发怒的原因 麻痹自身心中的痛 是对方有罪恶感，有效控制对方的行为 如果无力掌控大局又不想感觉能力不足，或没安全感，最快的解决方法就是证明自己是对的。 如果你相信对亲密关系，你只需要付一半的责任，那么，即使你能付出百分之百，实际付出的，却只有50%。既然你所看到的一切都是你内心的投射。你就会发现伴侣，也只是付出50%。这样一来，你们两人都会坚持自己已经做了自己该做的那一份，却指责对方不肯尽全力。 只有当你愿意为发生在自己身上的事情完全负责的时候，你才能得到选择的力量。 如果你为了伴侣牺牲，那么你就会把对方看成是利用你的人，因为他们没有尽他们应尽的力量。 虽然弥补伴侣的放纵不是你的责任，但对方的放纵行为的确是你的责任。这句话的意思是你有能力对伴侣的行为作出响应，而不是采取牺牲的方式。 我们所看到的每件事，其实都是我们内心的投射，我们怎么评论别人就是我们怎么看待自己。","link":"/reading-note/intimate-relationship/"},{"title":"富可敌国","text":"一本介绍对冲基金发展的书。 作者塞巴斯蒂安-马拉比， 保罗•沃尔克时期对外关系委员会国际经济高级研究员，《华盛顿邮报》的专栏作家。 他在《经济学人》杂志工作了13年，在《华盛顿邮报》编委会工作了8年，主要专注于经济全球化和政治经济领域。 极简简介一本权威的对冲基金发展史。本书作者对该行业进行了包括300个小时访谈和无数内部文件在内的深入调查，并在此基础上，讲述了关于对冲基金鲜为人知的故事。 个人看法极简评价首先作为一本纪实介绍类的书，人名太多了，对于不明所以的人来说有点难以阅读。但是讲述较为生动全面，个人认为还是比较有启发。当然全书的立意还是针对于对冲基金，说实话离大部分人都很遥远，当成故事书来说更为合适。作为一本故事书，其中的很多成功和失败案例能为自己的投资提到警示作用。 感想看法 人名儿太多了，难以阅读。 量子基金做空英镑的故事的确有趣，不像是那些鸡汤文或者财经文里面介绍的那么浅显，本书将前因后果和细节交代的十分到位。给我的感想就是，了解宏观经济十分重要，了解政治也十分重要。这并不只是一场经济游戏，里面也暗含了很多政客的小心思。各种基金一起做空英镑的场景让我联想到了翟老师讲的华尔街做空雷亚尔的故事，书中也写到“亚洲和拉丁美洲的新兴经济体卡在了盯住汇率政策上，这在未来十年给对冲基金创造了巨大的机会”。 很多无风险套利的机会是要通过大规模资金才能实现。比如影子银行，借入短期资金，贷出长期资金，例如最开始的支付宝。（原来支付宝之前玩的都是美国90年代玩剩下的）。 一方面，对冲基金可能会加快金融市场的传导机制（例如长短期利率），另一方面，对冲基金可能也被政府纳入了政策制定的考虑当中。 虽然都叫对冲基金，很多是很不一样的。当然也有很多名称不一样，但做的事情本质是一样的。 原来索罗斯在做空泰铢的8亿美元收益在做多印尼盾中几乎赔了进去，interesting。 机构投资者的高仓位可能暗示着强烈的抛压，而不是暗示着还将继续上涨。记住，继续买入才会引发上涨。 其实自己在买基金和股票的时候要考虑一下流动性风险，不是自己的流动性，而是机构的流动性，他们有没有对手盘来操作。 再次重复，人名儿太多了，难以阅读，有一种看外国小说的感觉。 你可以对冲价格风险，但是你无法对冲流动性风险。 其实不认为小到可以随便倒闭就是对冲基金适合去吸收风险的理由。因为可能多个基金加总，体量也会变大，这样总风险也会变大。 ”这就意味着资本价格对于脚踏实地的公司来说过高，而对于华而不实的公司来说过低。成长的概念被滥用“不敢更赞同。 过早的正确就等于错误，择时！ 流动性真的太重要了，参见近期的房地产，和一些远离成交价的期权。 内容摘录 有时CAPM模型不能解释市场异象，就是单纯考虑市场风险因素是有欠缺的。于是三因素，四因素，多因素模型提出，异象不复存在。 一方面有人认为有效市场的均衡状态永远不可能出现。另一方面，有人认为有效市场是一直存在的，只是没有找到正确的衡量风险的方法。 短期来看，市场中存在着大量的无效情况，这些无效情况可以提供无风险的套利机会。 货币会波动，利率会波动，越来越多的公司会发发行股票和债券。债务会被证券化。弄清楚什么样的金融机构能最吸收这些类型的风险将变得至关重要。这本书提供的答案是对冲基金会最好的完成这个任务。因为他们优越的激励机制，因为它们小到可以随意倒闭。因为他们可以比任何对手都更负责任的管理风险。 对冲基金并没有精确的定义，也不是所有的故事都和套期保值和杠杆效应有关。 天才并不是时刻都知道自己在干什么。 那些看上去不怎么样的价值型股票，相对于吹得天花乱坠的成长型股票来说，价值被低估。这就意味着资本价格对于脚踏实地的公司来说过高，而对于华而不实的公司来说过低。成长的概念被滥用。 他们时而被当作尽力是无效价格回归正常的稳定器而加以赞扬，时而又因其自身的不稳定性被视为威胁全球经济的因素而加以批判。 问题在于杠杆交易。杠杆效应使大额交易成为可能，因此使价格更有效，更稳定。但是杠杆效应也使得对冲基金在面对冲击时更加脆弱。 到底谁能比较好的管理风险，是那些我破产，要么靠政府埋单的商业银行和投资银行。还是一位兜售货币市场产品迫使政府支持的共同基金公司。是风险集中，使得纳税人也深陷其中的大银行，还是风险分散到从不指望政府救援的小规模对冲基金。 如果投资者是风险厌恶型，它就应该买最好的股票，但只用它继续的一部分。如果投资者是风险偏爱型，他应该购买完全一样的股票，并借钱了，卖更多。 所有新的市场首先都是低效的。这种效率，对最早顺应市场的人来说，则意味着利润。 当一种商品价格突破其一贯的价格区间，压错赌注的投资者就会出现大幅亏损，因为恐慌，他们将匆匆平仓使价格更加远离之前的价格区间。 索罗斯认识到了均衡理论无法解释实际中的货币波动，当热钱流入美国美元升值，而美元的升值又吸引了更多的投机者，使汇率更加远离均衡点。如果投机者是决定汇率水平的真正力量，那就意味着货币永远会出现先繁荣后萧条的过程。 索罗斯的投资决策往往是险中取胜，事实是，市场至少在一定程度上是有效的，所以大多数信息已经反映在价格中，投机的艺术就是看到其他人忽略的一个点，然后利用那一点小小的优势大量交易。 经验丰富的投资者也不是总能成功地纠正错误定价。 如果大交易商们已经满仓，这种消息几乎不会影响价格，但如果他们在等着进入，市场就会迅速上涨。 下跌的股票将引发投资组合保险人的抛售，而这种抛售将导致股票下跌更多。 市场可以脱离其基本价值，因为投资者缺乏挑战共识的实力，而这种趋势能持续到远远超出一个理性的点，但如果你比其他人更具备实力和勇气，你可以突然袭击，在市场毫无准备的情况下将其扰乱，因为你导致了一个新的趋势，你会是最先受益的。 进行基本面分析得知股票或债券的价值被高估了是一回事，知道什么时候市场会回归正常是另一回事，而图表则能暗示答案。 如果你了解市场上的其他玩家，你就可以找出那些非常吸引人的风险回报比的交易。 自从由于恶性通货膨胀帮助了希特勒崛起的那个时候起，德国人就十分注重货币的稳定。 当你确定自己是对的，就没有什么投注太多这样的说法，你会尽可能多的买入。 如果市场主要由追求最大利润的理性投资者控制，那么效率有可能会占上风；但如果市场由其他人驱动，那就没有理由期望有效定价。 当对冲基金通过积极购买较长期的国库券来响应美联储的低短期利率时，短期和长期利率之前的联系会更紧密。 不确定性意味着风险。 在一个券商的追加保证金就可能迫使杠杆式基金火速卖出的情况下，关键是要小心高负债的人集中的市场。 如果投机起到预警的作用而不是打击的作用的话，是让贫穷社会受益的。 当市场出现恐慌时，流动性溢价可能会增加：怯弱的投资者希望自己能迅速卖出债券，他们愿意为此付出代价。 自我纠正的趋势只是一种趋势，一个极端的事件可能会改变价格的惯常走势，这种改变可能会持续很长一段时间，甚至永远。 长期资本管理公司失败的真正教训不在于它衡量风险的方法过于简单，而是想要对风险准确衡量，这不可避免地要失败。 长期资本管理公司的失败表明没有后备资金而赌整个世界的稳定性是不理智的。 市场保持非理性的时间可能比你可持续的时间更长。过早但正确就等于错误，就像投资者多次发现的那样。 当市场价格不再具有指南作用时，你得根据一个资产所能产生的现金流量来决定购买该资产的投入。 对于一个资产以每4-5年增长一倍的基金来说，其内部专业知识的增长速度很难和拥入的资金增长同步。 在普通的具有流动性的市场，价格通常有效，预测他们是困难的。相比之下，在流动性不充分的市场会有大量便宜货，但是因为错误所付出的代价可能也会非常惨重。 对于流动性不充分资产的买家来说，最大的危险是在危机中这些资产崩溃的幅度会最大。 正是因为缺乏保密性，才使得一旦市场对他不利，它就容易成为被攻击的目标。 波动率低是因为世界上充斥着冒险资金，但充分的流动性让人误以为稳定是由金融体系的结构得到改善而带来的。 由于对冲基金习惯了自己做决定，不受监管和评级机构的干扰，他们往往表现较佳。 当交易员盲目降低负债率的时候，资本结构套利的原因就不再重要了。 政府的行动降低了“大而不倒”的机构冒险的代价，其结果就是更加冒险。 政府该如何促进能够很好的管理风险的“小到可以随意倒闭”的公司的发展呢？这是金融业未来的关键所在，而且部分答案是显而易见的：政府必须鼓励对冲基金发展。","link":"/reading-note/more-money-than-god/"},{"title":"乌合之众","text":"《乌合之众》古斯塔夫·庞勒（1841-1931），法国社会心理学家。 极简简介这是一本百年前对群体心理学研究的书籍。主要介绍了三个方面：群体的心理与特征，群体意见信念及形成原因和演变，群体的分类与特点。 作为参考，豆瓣上有一份个人认为比较全面且简略的内容梗概。 个人看法极简评价这是一本能帮我们意识到群体具有其独特性质的书，一定程度上可以让我们对生活中一些事件有一些初步的认知，比如说： 可以以此分析EDG夺冠后群体的不合理发泄的合理性（即群体的一般特征）。 1989的事件，群体的煽动性和不可控性展现的淋漓尽致。 翟老师对于之前美国通胀的表述，人民群众起义抵抗。 长津湖观影时觉得士兵慷慨赴死，体现了群体道德的一部分。 但总体而言，书还是过于老旧，启发式作用远大于知识性作用，读来涨见识不错，实际运用的话还是缺少很多证据支持及实操手段，相信后人也有着更加细致的研究。 缺点（喜爱挑毛病） 虽然结论很精彩，但论述也很薄弱（多用例证法，虽然这已经在当时是比较好的论述方式了）。的确此书指出了很多群体中出现的现象，也分析了一些原因等，但是其并没有一些指导性的建议缓解群体的这些缺点。 如今看来政治十分不正确。本书多次提到了女性和孩童容易走向极端和性格反复感情用事。同时也提到拉丁族裔的狭隘。也将种族作为一个影响群体意见的很大因素。 感想/看法 “乌合之众”，何为“众”，其实是一个值得定义的东西。虽然书中列举了一些群体，但是我觉得还是过于宏观，或许不能一概而论。而且不同的粒度下或许群体性质会有改变。 作为管理者的话，此书或许较为重要，可以帮助理解群体的思维方式，但是具体如何管理，还是需要参考其他的著作。 “为公众留下深刻印象的不是事实，而是事情发生并引起注意的方式。”认为这句话说的很对，例如最近的一系列小作文事件。如何博眼球，如何娱乐化，如何占据道德的制高点，显得比事实更加重要。 罗伯特•墨顿的序“勒庞《乌合之众》的得与失”写得很好，他提到一点，“《乌合之众》的当代意义，在于它发现问题的功能而非解决问题的功能。”（摘自此链接）。 按节讨论/摘录引言 文明更新过程中，仅有的重大变化就是受其影响的观念理念和信念的变化。 人们的行为从来都不是建立在纯粹理性之上的。 第一卷 群体心理第一章 群体的一般特征 集体心理使他们的感受思想和行为方式与其作为独立个体时的感受思想和行为方式有着很大的不同。 不同的原因造成群体的新特征 构成群体的个体在考虑事情是时从数量方面出发，他们会感受到一种无敌的力量，并因此形成一种本能。当他们独自一人时，则必须克制这种本能。群体是无名的，因此也无需承担责任，然后一直控制着个体的那种责任感便完全消失。 传染决定着群体特殊特征的表现。个体的受到传染后，很容易会被集体利益，而牺牲个人利益。这种能力有违个体的本性，而且除非作为群体的一部分，否则一个人很少能做到这一点。 在暗示的影响下，个体将会在不可抵抗的冲动的支配下，完成某些行为。 群体在治理方面总是劣于其独立个体，但是从群体所引发的个体感受和行为的观点来看，根据不同的环境，群体可能会优于或劣于其个体。 群体通常都是犯罪性的，但英雄性群体是同样存在的。 第二章 群体的情感和道德 独立的个体具有支配其反射作用的能力，而群体则不具备这种能力。 群体的多变性使得他们难以管辖。尽管群体的愿望十分狂热，但是这些愿望仍不会长久，因为群体既没有能力，也没有意愿为任何事做长久打算。 群体并不准备承认任何事都或成为其欲望和欲望实现之间的阻碍，由于感觉到人多势众所赋予的不可抵抗的力量，所以群体没有能力理解这种阻碍。 群体会永远徘徊在无意识的边陲地带，迅速屈服于所有的暗示，游泳所有无法上溯至理性影响的生命体所特有的暴力感受，同时失去所有批判能力，而且除了过度轻信意外再无其他能力。 对于一个群体而言，不可能的事是不存在的。 通常来说，如果一件事得到了成千上万的目击者的证实，那么事情的真实情况一定与大家所说的情况存在较大差异。 不管群体展现出来的情感是好是坏，他们都展现出了双重性格，简单与夸张。 在群体中，愚蠢、无知和有嫉妒心的人感受不到他们的毫无意义和无能为力。 群体愈加夸张的去时通常会施加于坏情绪上（作为个体对惩罚的恐惧迫使其竭力控制坏情绪这种本能），因此群里很容易会导致坏的过激行为的发生。 这并不意味着在受到巧妙的影响下不能表现出英雄主义以及成为最高美的标准的证明。 如果一位演说家要感染一个群体，那他必须粗暴地使用激烈的言论，必须通过夸张、断言或依靠与重复，而且永远不要试图通过推理的方式来证明任何事。 群体会夸大对英雄的情绪。 个体成为群体的一部分之后，他的治理标准就会立即大幅度降低。 群体只知道简单和极端两种情绪；他们将所受到的观点想法和性能作为一个整体而接受或拒绝，而且会将他们看作绝对真理或绝对错误。 个体可能会接受矛盾并进行讨论，但是群体将永远不会这么做。 群体会对强势力量“俯首称臣”，而且很少会被善良所打动，因为善良对于他们而言不过是一种软弱的表现。他们从来不会同情随和的人，但却会同情极力压迫他们的暴君。群体所欣赏的那类英雄总是会与凯撒有相似之处，他的徽章吸引着他们，他的权利震慑着他们，他的利剑让他们对其充满恐惧而又敬畏。 群体的光荣、名誉和爱国主义情感尤其可能会对其中的个体产生影响，而且个体通常也会在这种影响的作用下献出生命。 第三章 群体的观念、理性思考能力和想象力 观念只能以群体所假定的简单形式才会被群体所接受，而且通常必须要经过最彻底的转变才能变得流行。 群体理性思考能力的特征就是仅在表面上相互关联的不同事物之间的结合，而且那些特定情况立即就会变的一般化。那些知道如何管理群体的人总是会向群体展示这种论证。 争取切法理性思考能力的人一样，群体的想象力是非常有利、非常积极，且容易受到强烈影响的。这就是为什么能让群体留下深刻印象的事总是那些不可思议和充满传奇色彩的事。 当我们对一种文明进行分析时，我们会看到，真正支撑文明的就是那些不可思议和充满传奇色彩的事。在历史上，表现总比现实发挥着更重要的作用，而不真实总是比真实更加有力。 每一个时代、每一个国家的伟大政治家，包括绝对专制的君主，他们都非常重视大众的想象，而且将此作为权利的基础并在对大众进行统治期间从未尝试反对他们的这些想象。（举了拿破仑天主教徒、伊斯兰教徒、教皇权力主义者的例子） 安东尼并非是利用花言巧语才成功让平民们反抗凯撒并谋杀了他；而是通过向大众宣读凯撒的遗嘱并抬出了他的尸体。 事情必须以整体的方式呈现在群体面前，而且不能告诉他们事情的起源。 为公众留下深刻印象的不是事实，而是事情发生并引起注意的方式。 第四章 群体信仰的宗教形态 群体在受到适当的影响之后会准备为他们所激发出来的理想而奉献自己。 宗教情绪的特点非常简单，比如对想象中的高级生命体的崇拜、对其权力的恐惧、对其命令的盲目尊从、不具备讨论起教条的能力，以及将不接受这些教条的人看作敌人的倾向。 所有宗教或政治信条的的创始人之所以能够成功，是因为他们都通过那些狂热的情绪极大地鼓舞了民众，而这些情绪则使得人们在崇拜中找到了幸福，并且他们愿意且已准备好为他们的理想而献身。 第二卷 群体的观点与信念第一章 群体观点与信念的间接因素 决定群体的观点和信念有两种因素，间接因素和直接因素，间接因素中存在着一些一般属性，而这些属性就是群体所有信念和观念的基础。这些属性就是：种族、传统、时间、制度和教育。 种族的力量就在于在不经历最深刻变革的情况下，任何要素都不能在种族之间进行传播。 如果一个民族允许其风俗习惯变得根深蒂固，那么这些传统将不再发生变化，而且就像中国一样，这个民族就不再拥有改进与提高的能力。在这种情况下，暴力革命是没有任何作用的，因为其结果无非是那些被打碎的束缚再一次拼凑在一起，并且让过去的帝国原封不动地重现，接下来就是残余的历史碎片很快会破碎，然后陷入无尽的混乱。 一些重要因素的形成是依赖于时间的，比如种族，如果没有时间，种族便无法形成。 一个民族无法按照自己的意愿选择题制度，就像他们不能选择自己头发和眼睛的颜色一样，制度和政府都是种族发展的产物。他们并不是时代的造物主而是时代造就了他们。 制度并没有固有美德，就制度本身而言，他们没有好坏之分。那些良好的制度，是指在既定时间内对既定人群无害的制度，但是对于其他民族而言，这种制度可能会是另一个极端。 教育使得一个国家的年轻人可以了解这个国家的未来。 第二章 群体观点的直接因素 词语的力量与他们所唤起的图像有密切的联系，而且不受其真正意义的支配。有时候意义最不明确的词语反而拥有最大的影响力。 语言所包含的词语在时代的发展过程中变化的非常缓慢，但是这些词语所唤起的图像或者它们的意义却在不停的变化。 政治家最基本的职能之一就是，无论如何都要经得住流行用语以及群体无法忍受其原有名称词语的考验。 在一个同样的社会中，同样的词语对于不同的社会阶层有着不同的含义，他们表面是使用同样的词语，实际上他们所说的是不同的语言。 无论是谁，只要能为他们带来幻想，这个人就会轻易成为它们的领袖；无论是谁，只要试图毁灭他们的幻想，这个人就会成为他们的受害人。 第三章 群体的领袖及他们的说服方法 领袖的行动方法：断言、重复和传染 对于一个人而言，如果模仿是十分容易的，那么模仿是必然的。 人们会把从高位跌落的英雄看成与他们平等的人，而且还会对这位不再有任何优越性的人采取报复行动。 从威望遭受质疑的那一刻起，，威望便不再有任何威力。如果要得到群众的仰慕，威望的载体就必须与他们保持距离。 第四章 群体观点与信念可变性的局限性 真正广义信念的数量是非常有限的。他们的出现和消亡都来自于历史上每一个种族历史的巅峰时刻，它们构成了文明的真正框架。 一般来说想要改变一种信念就必须要以暴力革命为代价。事实上，一场革命的开始就是一种信念的结束。 一种伟大信念开始衰败的精确时刻是很容易辨认的，就是当人们开始质疑其价值的时候。 现在一种观点在被大众普遍接受并成为一种普遍观点之后就已经消亡了。 第三卷 不同群体的分类和描述第一章 群体的分类 由于种族的精神力量十分强大，因此群体的劣质特征并不占据重要比重 第二章 犯罪群体 通常群体犯罪动机是一种强烈的暗示，而且参与犯罪的个体在事后坚信他们的所作所为是遵从义务，这与普通的犯罪情况相去甚远。 第三章 刑事陪审团 展示出易受暗示的特征，但是他们却几乎没有能力进行理性思考。容易受到群体领袖的影响，而且还主要收到无意识情绪的引导。 群体的力量是令人敬畏的，但是某些社会等级群体的力量是更加令人恐惧的。群体还有被说服的可能，但是那些社会等级群体则永远没有这种可能。 第四章 选民群体 头等重要的是，候选人必须要想有威望，而这种威望能迫使选民们没有异议地支持自己。 拥有威望还不足以确保候选人能获得成功，选民们特别重视别人对其贪婪和虚荣的恭维。 对于社会问题，由于其数量是未知的，所以人从本质上来讲都是同样无知的。 第五章 议会 在群体中最重要的特征就是他们观点的简单化。到目前为止，这种群体的不变趋势就是通过最简单的抽象原则和适用于所有情况的普通法律来解决最复杂的问题。 通常情况下，议会的投票只代表少数人的观点。 领袖应该掌握一种特殊的雄辩术，包括有力的肯定，无需证据证明，以及令人印象深刻的形象，同时伴随非常简明扼要的论证。 所谓自由的增加，必定紧随着实际自由的减少。","link":"/reading-note/the-crowd/"},{"title":"Principles for Academic Writing","text":"A note about the principles for academic writing. Details Capitalize the first letter of a proper noun For models: True For paradigms: False Do not use “And” at the start of a sentence. Narrative/Statement Avoid and eliminate the fuzziness and subjectivity of language to the greatest extent. Avoid to use “we”, “I think”, etc. Avoid verbose expressions. Make the statement accurate and clear. Make the definitions accurate and clear. Use the right symbols for different concepts. Polish Emphasize the contributions in your introduction. Presentations of Results Present the results in a legible way. Include both tables and figures. The figures should be in proper size and easy to be recognized. FormattingReference Unify the formats of the references in Capitalization Abbreviation Constitution The formal BibTex references of publications in CS literature could be found in DBLP. For the conferences, record the month, address.","link":"/research/academic-paper-writing/"},{"title":"翟东升教授文章摘录总结","text":"作为翟老师的粉丝，翟老师的文章视频几乎每期必看。 此处将翟老师一些微博，头条号中的文章进行观点记录，以便查阅。 翟（dí）东升：中国人民大学国际关系学院副院长、博士、教授。中国人民大学世界经济专业与国际政治经济学博士生导师。中国人民大学国际货币所特聘研究员。 研究方向： 货币与金融的国际政治经济学，中国对外经济关系美国政治经济 人民币汇率、美元指数与全球通胀问题 2021-05-28 首先人民币汇率并不强，只是由于美元下跌看起来涨势明显。 时至今日，美国仅仅是我们的第三大出口市场。要讨论汇率对实体经济的影响，必须看CFET指数，也就是人民币兑一揽子主要贸易伙伴的货币的加权平均值。 人民币正在成为逆周期货币，且对美元应该有长期升值趋势 美元指数本身有17年左右的大周期，我们此刻处于新一轮下行周期的早期，考虑到新冠疫情冲击下美欧等国货币政策的现状和差异，明后两年内美元指数如果暴跌到70，无须惊讶。 关于美联储货币政策是否会尽快转入收缩的问题，我的判断是，他们会口惠而实不至。美联储主席和总统的关系会影响到货币政策。 鲍威尔若想获得连任，2022年之前很难加息缩表 我们此刻也许正在一个通胀率大时代的转折点上，未来三十年，也就是到2050年之前，全球主要经济体的通胀率也许是震荡上行的，或者说具有上涨容易下跌难的特点，各种预料之外的事情，比如疫情、战争、大国刺激生育和产业链调整等等，都会推动这个趋势的展开。 硕鼠的养成与美国的政治极化 2020-05-20 任何一个政治经济体系都存在不完美或者有待弥补的漏洞。识别、利用并扩大这些漏洞，就能给某些聪明人快速收割巨额财富和权势的机会，但却是以国民和政府的损失为代价的。他们的财务成功不是因为创造了财富，不是因为发明了新的技术和工艺，而是利用了政治和经济体系中功能失调的那部分扭曲和漏洞。 尽管饱受批评和嘲讽，科赫兄弟仍然是美国的成功人士，或者说仍然在操纵着美国的政治经济乃至科研和教育，而他们在中国的模仿者如今纷纷落马。中国特色社会主义市场经济与美国资本主义市场经济的根本区别由此显现：事实雄辩地说明，中国走的是以人民为中心的路线，美国走的是以资本为中心的路线；我们是经得起考验的人民共和国，而美国越来越像一个“香蕉共和国”。哪个国家更有前途？我从2009年起就断言，中国更有前途。","link":"/economy-finance/didongsheng/articles/"},{"title":"大师计划系列视频","text":"记录一下翟老师《大师计划》视频的内容。视频时间：2021.06 1. 当今世界格局与力量对比幂律现象：强者恒强弱者恒弱。 美国领先，中国紧随其后，但是远超其他。 GDP规模 财政开支，军费开支 互联网巨头平台 独角兽企业数量 顶级大学 博士学位，高被引学者 专利产出，人工智能 中国仍未到达第二的地方 国际舆论话语权 国际组织影响力 货币国际化指标：美欧英日中 超过或正在超过美国： 出口贸易额 国内消费市场 理工科大学生人数 中等收入人群规模（4.5-5亿，是否具有小汽车） 汽车市场 智能手机销售 互联网接入用户 制造业产出增加值 能源资源消耗 500强数目 5g技术 2. 大胆拥抱崛起的中国至2013-2035年。 目前大学生知识分子较多，能积极参与社会分工。 2012年前出口导向，央行资产负债表扩张（以美元储备作为基础）。 2014年前中国主权信用严重压抑，将来有巨大发展空间。 2013年以来，出口对外依赖度转变。提出并推动一带一路（非美经济体）。 以美元信用为基础发行货币，无论你的质量科技含量再怎么高，都是加强美元的购买力。 所以真正重要的不是跑的多努力，而是往什么方向跑。 3. 中国相对于美国“道”的优势人类文明史上两种策略的竞争和对立（格劳秀斯的海洋文明流派 &amp; 康德的大陆文明流派）： 美国：选取政治、三权分立、私有产权、自由市场 集体价值本位：个体服从整体，自上而下分配管控 前者在市场竞争有优势（利益，贪婪），后者在军事上有优势（荣誉感，恐惧） 国家于市场的关系具有三角对立关系。 国家发展的好取决于公共部门的能力和特质。 底边越长越好。税率越低越好效率越高越高。 学习苏联，把底边建立。（但是苏联的税点特别高） 学习美国及盟友，降低综合税率，所以出现了繁荣的私人部门。 两者不能相互否定，而是要正体反体合体（中庸之道）。 4. 中国相对于美国“治”的优势两个重要任务： 数字经济的转型：机遇和冲击 数据所有权对社会的冲击，国内现在正在规范，之前也进行了对本土公司的保护。 社会政治影响，民众识字率大幅提升，年轻人只从互联网获取和传播再加工信息，大众政治由小众政治所取代。 解决贫富分化。 美国：印钱，加税 中国：反腐，对不合规资本系打击，财富再分配（精准扶贫） 5. 美国衰落的主要原因盎格鲁撒克逊王朝的帝国的生命周期末端的老年病： 精神分裂： 两党意识形态分歧变大，社会极化，攻势瓦解。 种族上也有投影。共和党白人，民主党有色人种。 建制派和民粹分裂。民众认为精英欺负大家。 四个所谓的美国： 极左：民主党民粹派（桑德斯、沃伦，主张现代货币理论，发钱） 偏左：民主党建制派（拜登希拉里，拜登政策在讨好民粹派） 偏右：共和党精英（米奇·麦康奈尔，切尼，布什。正在失去共和党基本盘） 极右：共和党民粹（特朗普。占据共和党基本盘。） 脑瘤： 华尔街的金融资本。70年代之后被一个特殊族群主导。高盛出产的精英在政界影响巨大。 内政上金融压倒实业。 外交上偏向于对中东伊斯兰的镇压征服。 经常制造泡沫。 肝癌： 肝癌导致有机体力量萎缩。 医疗医保医药构成的医疗系统挤占大量的财政空间。 美国是在医疗上制造支出最大的经济体，但是是OECD国家中人均寿命明显偏低的。 三医系统的政治捐赠最多。 6. 在中美的长期竞争中，我们如何赢得胜利 要调整好心态。前倨而后恭。 2035年前鼓起勇气敢于竞争。 2035年后美国可能经历惨烈的去杠杆。我们需要谦虚谨慎。 2035-2050如果美帝国出现崩盘，中俄关系可能也会出现不利于我们的翻转。 赢得竞争不是两个国家的竞争，而是两个体系的竞争。 美国同盟战略。源自罗马同盟大战略。 孟德斯鸠的研究，罗马在击败对手后吸纳盟友。但是盟友间不友好。而且会干涉盟友权力传承。 新的阵营浮现。美国赢得冷战后，应该避免使中俄结盟。 日本永远忠诚于最强大的国家。 印度对外政策，利用别国矛盾向双方索取好处。 我们对抗的是整个美国的同盟体系。 美国处境危险。多空双方对峙。多方中国没有加杠杆，空方拉盟友加杠杆才能均衡。如果中国加杠杆，空方可能爆仓。（如果未能兑现一个盟友的安全经济承诺。） 使他使用成本越来越高，用时间耗死他。 做好国内再分配，和国际再分配。 中央和地方的再分配。中央应该利用良好的信用敢于借钱，形成巨大的国债池。但是地方应该少借债。 地区再分配，东南沿海到中西部。 减少外汇储备，投到一带一路沿线的股权和债权。 积攒金钱是之前贫穷给我们带来的匮乏感的二次伤害。","link":"/economy-finance/didongsheng/dashi/"},{"title":"杠杆率无法衡量经济风险","text":"2019.12. 该视频是翟币的由来视频。 去杠杆概念的源起Ray Dalio，桥水基金创始人。 他的核心概念就是去杠杆。 他认为债务/GDP不能太高而且不能增长太快，否则经济会崩塌。 但是这个可能是在胡扯。 逻辑可能是错的。 债务是存量概念，GDP是流概念。 能不能得出有意义的结论？ 举例，银行放债。 企业的销售额除以债务能不能说明问题。 京东和字节跳动相比，字节跳动销售额虽然可能小，但是利润更大。 杠杆率与经济危机无关日欧债务率都相当高，但是即使负利率，都很债务安全。 杠杆理论的反例，债务率低的崩盘了很多。 那什么具有决定性意义呢？ 债务的定价货币是什么。 翟币用10万翟币债券债务关系买手机。 一年之后还钱只需要再印十万就好。 格林斯潘：“女士们先生们，这些钱我们永远都不用还。” 中国政府债务中国没有借美元债，都是人民币债。 中国债务率大家都觉得高。 中央政府债务率占GDP比例20%都不到。 但是地方政府借了很多债。 即便中央地方债务都算进去，也仅有60%左右。 和德国中央政府差不多。 本币债和外币债有重要区别。 本币债其实本质上是一种隐形的税收。","link":"/economy-finance/didongsheng/dibi/"},{"title":"国际经济形势与中美关系","text":"翟东升 2021-10-28 于海南讲座。 本次讲座涉及四个 Topic。 1. 疫情冲击何时结束 疫情传播似乎更加普及，但是疫苗的使用下，死亡率重症率下降。 11月8日开始世界大国开始放开人员往来。 中国可能在较长一段时间保持是否开门的难题。 由于疫情，东亚供应链可能重新收缩为中国供应链，一部分产能试图回中国，但是目前看来我们也不愿意接受。 2. 美联储是否有能力加息缩表 美国当前通胀率很高。 70年代中后期，婴儿潮，美国通胀率也高。美国政府主要挑战就是国内控制通胀。控制通胀的目的是来打击国内左翼激进工会势力。 长周期十年国债收益率大方向可以测，由人口结构预测。 以前08年也大量印钞票但没有使通胀大幅上升，因为钱没有进老百姓口袋。 这次疫情很多资金直接进入了老百姓口袋，零售急速上升，供给跟不上，直接导致通胀。 美国上升，其他地方也会上升，我们也会，只是暂时还没传导到下游消费。 每天嚷嚷的缩表只是象征性的，不断印钱才是常态。 主流流氓国家国债总量指数上升（尽管美国有扩有缩）。 央行独立性不能迷信。 明年一月份，美联储大概率换人，鲍威尔不连任 未来两个月加息缩表会比较严重 但是明年二月开始政策会继续宽松，原因因为疫情还不是个头 3. 中美关系未来走向 目前是暴跌之后的小反弹，可能四年。 高盛前总裁桑顿去新疆参观。 戴琦说不脱钩，要重新挂钩，务实坦率。 中国应对美国策略，合作进化，tit for tat新型大国关系。 中美安全领域互信重建困难。 近期拜登对华态度软化（背景是拜登上台七个月民意死亡交叉） 过去四年一群成人哄老小孩玩，现在一个老头带大家玩 现在拜登懂政治且固执，手段足够，且拜登中期选举必输。 目前美国政治形式拜登参议院50+1对50，众议院民主党领先8票。 中期选举众议院执政党一定会丢掉一些票，平均丢27.5，众议员管钱，丢掉众议院意味着大规模财政开支基建计划全部完蛋。 大概率参议院众议院全丢，所以真正有效执政只有是前两年。 拜登意识到这一点已经放弃连任幻想，按照自己的想法实施，需要立自己的历史遗产。 三年机会之窗，新型大国关系，平等不冲突 现在中国也硬了，现在有机会窗口 民主党2024可能出大事，共和党重来，中美关系暴跌 4. 海南自贸港建设的前瞻和思考 冻结的人与人交流即将放开，海南可以争取放开的优先权，外国人进入中国的缓冲通道。 争取建立多个领馆。 高端服务业来海南。 推进金融开放。 吃海外人员回流福利。","link":"/economy-finance/seminar-talks/china-us-relationship/"},{"title":"Dancing With The Dragon - China - Friend or Foe?","text":"牛津大学的辩论，关于中国是敌是友。 原视频链接 写在最前其实他们的辩论还是比较逻辑清晰，相信在国外高知中具有一定的代表性，此处总结一些论据，最后加一丢丢个人的看法。 之前在抖音和B站看过一些关于这个辩论的精彩节选，当然这些节选只包含了“能给中国人看的”或者“中国人喜欢看的”部分（全部都是“友”这一方向的辩词）。本着看事情看全面的态度，特意去油管找了全视频，正反方都来看看。 辩论双方 立场 姓名 简介 Friend Ser Vince Cable，文斯·凱布爾爵士 英國自由民主黨籍政治家和經濟學家 Foe Michael Pillsbury，白邦瑞 美國國防政策顧問，前美國聯邦政府官員，中國問題作家 内容摘录Round One：辩论环节Foe 过去大家总认为是美国打开了中国的国门，但是其实现在来看更像是中国打开了美国，并且吸纳了种种的好处。 在联合国知识产权组织的领导者竞选中，一个中国人，党员，脱颖而出而即将赢得竞选。辩手谴责了这一现象，一个著名的知识产权的盗贼国，却即将赢得竞选，这很荒谬。（当然竞选最终被一个新加坡人赢走了） 中国存有的的人权问题，审查制度，一党制，白色恐怖让中国显得像是“西方将中国拉入西方世界”这场游戏的胜者。 中国的野心不是纳粹那样，也不是日本那样的东亚共荣圈。而是重新恢复往日荣光。习需要有第三个任期来完成复兴之路。 Friend 四十年前，仅通过简单的算术就可以判断中国会成为最大的经济体。四倍的美国人口，意味着中国只要达到四分之一的美国人生活水准，将会达到相同的经济总量。中国将贫困人群的生活水准提高，这是我们非常乐意看到的。（当然印度也有潜力） 英国人用了一百年习惯了 slipping down the big table 的这种落差。而美国却还没能适应这种现象（中国将进入整个世界的运转规律，而这之前是由美国主导的）。 中国模式有两个很清晰的原则：稳定和安全（在百年的混乱革命内战等等之后），提高中国的国民待遇（引入市场经济，卓有成效，虽然也存在一些问题，比如欠消费和企业负债等） 如果中国的模式没有成功，那我们不会关注他。但是问题是，如果中国的模式持续成功了，我们该如何看待和相处，对于这种社会主义市场经济和一党制。我们承认接受不同体制的存在。 香港问题，我们对看到香港很多人入狱，甚至报纸被关停十分愤怒。但是中国对香港的红线问题十分清晰。中国本可以排支军队收复香港，就像很多其他国家一样，但是它没有，它看到了一个独立的实体可能带来的好处。 你可以言论自由，但是如果暴乱，秩序被破坏，他们会阻止。那些香港人，以言论自由和民主为名，袭击警察，破坏立法会，尽自己所能杀死了香港的民主。 关于外交关系：习主席将他的施政方针在十年前就展示的很清晰。如果你想和中国保持好的关系，你需要尊重他们的原则：主权和领土完整，不干涉内政。 新疆问题，我确信那里的人权遭到了破坏，但是西方在这一问题上的立场过于强硬。明确的是，很多其他国家，包括很多重要的穆斯林国家，都站着中国那一边。你想和中国相处，要意识到他们有着不同的体系，不要干涉他们的内政。 对于英国来说，和中国合作经济上有益。比如电动车产业，路虎的利润，阿斯利康疫苗公司的利润，英国大学的学费等等。 中国实际上平衡了全球的财政制度，虽然很多人指责中国在破坏这些制度，但是其实特朗普才是破坏全球财政制度的那个人（取消对 WTO 的支持等）。 知识产权方面，日本，韩国，台湾，甚至是美国（对于最初的英国），都是在偷窃知识产权上建立起来的。这就是世界如何运行的。 国家间是需要合作的，从疫情就可以看出。以气候问题为例，我们不能寄希望于在没有中国合作的情况下解决气候问题。中国的确意识到气候问题，他们也做出了努力：碳排放交易，最大的可再生能源，新能源汽车。你不能在一个冷战背景下解决类似这样的大问题。 中国对很多拥核国有着影响力，如朝鲜，伊朗，巴基斯坦等。中国仍在扩充核武，尽管他们承诺不先使用。但是也只有和平交流才能保证安全。 Round Two：主持人提问环节What would you say is the clearly defined role that china ought to have in its region and beyond globally? (Micheal) 用英国和当时的希特勒举例。一些英国人认为希特勒是合法的，甚至他可以做得更极端（这显然是错的，因为生灵涂炭）。 对于中国来说，我们的期望是：停止对维族人的种族灭绝，控制核武器问题。 英国当过老大，美国也当过，但是让中国当是一件危险性极大的事情。 （但是发言中，他对于这个问题并没有一个清晰的回答。） Isn’t part of the problem that people have assessing how to engage with china? Not just we have to engage with china but actually we don’t agree with their vision of the internet for instance we don’t agree with militarization of the South China Sea and these are matters us. (Vince) 首先（对于 Micheal 的回答）我觉得用纳粹来类比中国是一件很 offensive 的事情，这种言语或许会毒害整个辩论并让其看起来不正确。我们讨论的起点是经济关系，但是总讨论新疆，其实解释了我们为什么总是会走到这种冷战的环境中。 （名场面）如果中国说，我们将不和美国贸易，除非你们废除宪法第二修正案（民众持枪），因为我们中国人觉得这带来的杀害侵犯了人权。如果总是以自己的角度定义人权，那将不会开始进行交流，这显然终止了讨论。中国至少是这个世界系统中的一部分，他们必须被接纳。 中国用2%的 GDP 来做军费支出，这和北约的英国一样多，他不像苏联一样致力于军事化建设。从国际扩张角度来看，美国可能有200个国际基地（表示对此没有意见），中国有2个，一个在巴基斯坦，一个在（实在是听不清）。这并不是国际军事扩张。 他们想成为国际经济的一部分，我们需要找一种方式来接纳他们。 Belt and Road brings infrastructures to the place where the west hasn’t provided a sensible response to. Why doesn’t the west have a better answer to those perfect legitimate questions (e.g. we want 5G) which Chinese are answering? (Micheal) 一带一路本身并不邪恶。但是一带一路可能会带来主权问题。 中国在推广他的模式，很多人喜爱中国的模式，这是我们不能接受的。 如果种族灭绝发生，你肯定不能把这个政府当成正常的政府看待。 （总体上仍然是回避了问题） Essentially the price of doing a trade deal with china is essentially that you have to shut up about china if you’re not gonna say something nice. (Vince) 如果我们想和有相同观念的国家在一起，我们会和欧洲在一起（然而我们事实上已脱欧）而不是中国。事实上，我们并没有很多选择，我们只能和这些新兴的快速成长的经济体合作。我们需要更务实（pragmatic）。 我有机会和中国总理谈论人权，你们是一个社会主义国家，但是你们的劳工都没有权利罢工。我们邪恶（wicked）的资本主义国家至少还有工会等等。我们详谈了一个半小时。 我不知道新疆确实发生了什么，我读了文献，那里可能的确有一些不合适的行为，但是我们绝对不应该用“种族灭绝”这样的词汇，这是很不合适的。当蓬佩奥开始使用这样的词汇时，他的律师与他撇清了关系，如果那里真的很邪恶，请我们使用正确的语言来描述它。 观后感对于辩论整体 在不考虑观点论据正确性、真实性的情况下，明显能感觉到凯布尔爵士和白邦瑞两个人的语言逻辑完全不同，甚至可以说，两个人一个是在辩论，另一个更像是在做政治宣传（当然辩论也是传播思想的主要途径之一，在此不做评价）。 对于 Foe 方，在阐述过程中，白邦瑞多次使用没有论据支撑的论点，以假设语气反复强调某一论点，并且时常跑题。在提问环节中，从不正面回答问题，反而呈现的是特朗普政府那一套顾左右而言他的政客话术，个人感觉难以从他的谈吐中学到东西。 对于 Friend 方，凯布尔爵士将朋友（dance with dragon）和合作交换了概念，毕竟合作不一定需要是朋友。他的整个阐述逻辑都是围绕合作展开，个人认为这个逻辑替换还是相对可以接受。在他的小论点下（反驳目前的很多西方指责的点），他摆出了很多事实论据作为支撑，也包含他在与中国实际接触中的例子。总体的陈述方面，他先点明大原则，再在此之下展开讨论，逻辑清晰。在问答方面，个人认为他也有一些跑题，但是总体上还是围绕问题展开，也举了很多有趣的例子，我觉得还是比较不错的。 对于辩论内容当然屁股决定脑袋，我的屁股站中国，我的脑袋也一定是站中国。其实不论他们两个人的立场（不管是个人立场还是辩论立场），我们可以从他们展现或挖掘出来的信息里看到很多东西。 白邦瑞： 首先最明显的一点，这个人在整个辩论过程中都充满了对自己新书的推销。甚至话说一半，剩下的“可以去我的书里找”。这或许能代表很大很大一部分美国退休政客的退休生活。 我的直观印象来看，他和特朗普在表达上有着很大的相似之处，最主要的一点在于演讲性和煽动性。对于这些性质，之前读《乌合之众》时也有一点感想，说服一群人有时候并不需要举证，举出一个观点并调动大家的情绪（威胁论，迫害论）可能就足够了。但是这是在辩论场，而且是在 Oxford Union 的辩论场，我怀疑这样是否有效，同时也觉得这样可能给他败了一波好感。 从他的发言来看，美国人最不爽的其实是“中国打开国门之后变得很好”。他们存在这么多问题但是经济还很好，所以引发了对中国体制和政党的口诛笔伐。同时也能看出来他们对于 C party 的敌意。 当他用纳粹和中国类比的时候我惊呆了，没有想到美国的高层会对中国有这样的认知，不论是否有普遍性，这种观点的产生其实足够让人思考。（当然他们很多是以新疆为论据，这个我未做调研，但是我的屁股坚定的坐在中国这一边） 个人觉得，他们是有一个中国过于强大的预期，但是他们并不满意于中国现在的一些行为方式、体制、施加影响的方式，（在不考虑“不愿意让出老大地位”的这个事实下）假设有朝一日他们愿意把老大让给中国，他们是不希望这些他们不喜欢的行为方式、体制、施加影响的方式作用在他们身上。 凯布尔爵士： 首先我认为，凯布尔爵士作为一位的确与中国有过沟通交流的政治家，经济学家，他的发言相对客观及有理有据，而且透露着一种礼貌谦逊，虽然还是有着“号召者”的那种气场（国家地位不允许），但是总体来说还是让人觉得舒服且信服的。 他说的英国已经习惯了这种从世界中心滑落的落差，而美国没有，我觉得是十分有趣的一点。这让我觉得美国当前的敌对行为是合理的，没有人愿意看到自己的相对衰落，如果真的发生，那么唯一能做的就是阻止和拖延，直到不可挽回躺平。 他能看到事情发展的两面，比如香港问题，他提到了一些对一些事件的惋惜，但是同样的，他也对香港暴徒进行了指责，我认为这是十分客观的。 他的回答很大程度上是从经济角度来看（包含 ESG），在这种情况下，他提到的合作带来的益处显而易见，这应该是无法反驳的。 从核问题角度来看，他的回答的确体现了如何从政治上进行权衡，和平的交流对中方施加善意，中方也会利用自己的影响力向其他反对西方的拥核国施加压力。而白邦瑞的态度则截然不同，他的观点更在于对抗。这就让我想起了发言人华春莹的那句话，“有没有想过把一个大国逼到绝境的后果？” 知识产权方面，的确人间清醒，这种事过去在发生，现在在发生，未来也会发生，这就是事物发展的趋势无法阻挡。我十分认同，但是我也十分支持知识产权的保护，我认为两者并不冲突。 气候问题方面，他的回答展示出很多西方高层的确看到了中国对 ESG 方向的努力，我认为这个一定程度上能体现一部分西方高层对华某些方面由衷的褒奖。 在凯布尔爵士的回答中，两次对白邦瑞的用词进行了批判。一是“纳粹”类比，二是“种族灭绝”。通过他的反驳，我对这个人肃然起敬，我觉得能为他人受到的不公发声是一件很优雅且很有勇气的事情。 对于新疆问题，他直言“我不知道新疆确实发生了什么”。我觉得其实绝大部分人，新疆人，外国人，中国人，包括我自己都不知道那里真的发生了什么，所以其实绝大部分人无证据的谈论都是毫无意义的，只能徒增吵架的材料。 在他的回答中，多次提到了接纳、融入、思考在一个有中国的体系中如何做。这是很务实且很善意的行为，我个人觉得十分欣赏。","link":"/economy-finance/seminar-talks/dance-dragon/"},{"title":"中美关系、人民体系与未来起点收入","text":"翟东升2021年12月中旬于财科院演讲。 1. 中美关系当前态势与未来趋势当前中美关系： 2021年上半年斗争为主 下半年互相让步 美方主动缓和，中方两手准备 2021年11月双方开会会谈，无人知道结果。（谈的好才保密，谈的差必然宣布“英勇斗争”） 拜登政府对华特点： 前倨后恭 受限于国内反华民意，力求平衡 经济困境：债务上限、高通胀 华尔街对白宫影响部分恢复 白宫对各个部门影响力有限 国内支持率低，已经弃疗，重心转向历史遗产 众议院领先、参议院有略微优势 中期选举参议院必丢、众议院也会丢（目前领先5，平均丢27.5席位） 7.31号美国债务上限恢复，于是八月份拜登宣布在阿富汗撤军。 中美关系走向预判： 中期选举之后横盘，否则会有来源于国会的压力。 三年后可能再次下跌。 特朗普在干什么？（下次竞选代表共和党可能性较大）。 2. 如何用“人民体系”替代布雷顿森林体系及其衍生物三个强盗分金沙： 两个人分：先分后挑 前一个人分，下一个人如果认为多了，减沙自取 和平的无政府博弈的关键在于： 权力和责任要匹配：美国获得货币特权，却不愿意承担逆差的结果。 原则理念与现实手段要匹配 怀特和凯恩斯的辩论： 怀特的理论有内在缺陷：例如特里芬难题（一国货币成为国际储备货币） 盖恩斯的理论更为完善 怀特计划形成布雷顿森林体系 GATT 关税贸易总协定 世界银行 金汇兑本位制：美金可以换黄金，其他货币不能换黄金。（戴高乐发债换美元赎黄金再用黄金抵押发债） “人民体系”：以全世界人民为中心而不是以金融资本为中心（资本相对劳动占据优势地位） 原有体系受益者是资本拥有者（印钱推动资产上升） 为什么全世界绝大多数国家会乐意参与这个新体系？ 分配方式：由新的全球央行通过购买各国国债来为各国提供资金 布雷顿森林体系根本困境： 权责不对等导致现行秩序合法性权威性缺失。 特里芬难题，美元作为国内货币主权性和作为国际货币的超主权性的矛盾。 基于英美谈判，难以反映现今国际政治经济权利格局。 自由贸易成为冲突之源。 需求： 一个权责匹配的，正确反映当前国际权力格局和时代挑战的国际经济新秩序 中美欧应当就21世纪货币金融贸易产业等国际规则展开谈判，寻求顶层规划，设计公平合理可持续的“人民体系”，以取代布雷顿森林体系和牙买加体系。 “人民体系”（翟东升，嵇先白）： 基本平衡的贸易 Balanced Trade：而不是自由贸易 超越主权的货币 World Currency：而不是主权货币 这种国际货币需要锚定一揽子商品。 绿色普惠的金融 Responsible Finance：ESG为主，而不是利润最大化导向 如何把这套体系变成现实： 中美谈判难以成功。将欧洲纳入。人民体系一定程度上是欧洲一体化经验教训的全球版。（但是非平衡贸易，商品服务不能自由流通。） 短期难以谈成，但是可以先下手为强，抢占制度变革的话语权。 有可能将世界分为两个平行的国际体制。 3. 如何用“未来起点收入”推进共同富裕和内循环如果美方不接受2中的体系，或者我方没有下定决心进行改革，那么就需要在内部搞共同富裕。 民粹主义两大支柱： 数据信息传播模式发生变化。 贫富分化。 为什么必须搞共同富裕？ 避免民粹主义和政治动荡。（避免出现落第秀才） 共同富裕撬动消费和投资的增长，实现内循环为主的双循环 增加国民福利，让中国人投入更有意义的工作和学习，实现高质量发展 合成谬误：每个人都是有道理的，但是合在一起起到了颠覆性的效果。 共同富裕：北欧的例子黄文政：推动计划生育变革、哈佛教统计学、颠覆统计学底层逻辑、华尔街对冲基金、未加入美国国籍、致力于拯救中华民族（游说改变计划生育政策）、《新时代中国特色社会主义道路下的全民基本收入》：未来起点收入（给年轻人发钱） 北欧： 税全球最高 人均GDP最高 亿万富翁密度最高 如何做？谁得利？谁受损？ 中央地方的财权、事权和债务再分配 区域再分配：转移支付 代际再分配：让中老年人向年轻人让利 行业再分配：挤压原本比较肥的行业（金融、电力） 国际再分配：央行资产负债表的替代 阶层再分配：反腐、扶贫、医疗教育住房 人民币汇率水平适当高估，从长期巨额顺差变成大体平衡略有逆差。 “未来起点收入方案”：补贴年轻人，促进机会平等、提高人口素质、增强我国人口竞争力。 钱从哪来？ 税收：查税，警税合作，藏富于民 富人的税收是小头，大头来自于国债扩张（现代货币理论） 社会结构转型带来巨大内循环商机 懂得资产配置，将能从共同富裕中获利 从金字塔社会向橄榄形社会的转变将带来中产阶级和国内消费市场的快速扩大。（富人消费市场大还是中产消费市场大？） 如果共同富裕能实现，中国国内市场规模等于美欧日之和。 新增中产主要在哪里？ 新中产的形成与未来商机： 哪些生意的市场规模扩张到十倍以上？哪些商品和服务？ 汽车动力电池需求；富贵病医疗需求；高铁航空出行需求；旅游度假需求；优质教育；线下消费；养宠物情感需求；优质高端养老服务需求 这些需求扩张未必带来利润率大幅上升，因为多数供给也会随之扩张。 需求上涨而供给无法扩张的东西，价格会有明显上涨：如茅台，顶级医疗 劳动力更贵，机器人普及 旅游业","link":"/economy-finance/seminar-talks/future-start-income/"},{"title":"全球发展不平等，中国在其中扮演了什么角色？","text":"海南绿色金融研究院为世界银行原首席经济学家Branco Milanovic教授与翟东生老师组织了一个线上对话。 Branco 是个很有思想的学者，他提出的“大象曲线”用以描述和解释全球化时代的财富再分配效应及其政治后果，是国际政治经济学中的经典概念。 讲座部分Global Income Inequality and the Role of China 全球收入不平等和中国的作用 大概分成三个部分： 1950年前，英国日本等经济增长，而中国下降。不平等增加。 1950-2000，二战后美国在不平等中占据重要地位，不平等并未加剧。 1980年后中国印度崛起，这些国家之前贫穷，不平等减少。 虽然中国等国家的崛起减少的国家间的不平等，但是国内的不平等仍在加剧。 中国对减少全球不平等起着积极作用。 但是随着中国越来越富有，作为中上收入国家，其实中国是会加剧全球不平等。 印度非洲对减少全球不平等越来越重要。 Inequality in US 美国的不平等美国自由主义资本中的系统性不平等： 资本收入对总收入的重要性提升，资本集中度提高。 高收入人群劳动收入也很高。不像传统的资本家只有资本收入。高等级人群在劳动力市场和资本市场都很富有。（homoploutia） 人们门当户对的结合。选型婚配。（homogamy） 富有的人对政治控制程度的增高。 大学入学率与父母收入正相关。 Inequality in China 中国的不平等中国的不平等在2010年之后稳定并逐步下降。 城镇不平等和总不平等高度相关。 三个可解决的不平等，相比之下，美国较小： 省份收入的不平等。可以提高GDP用财政政策解决。 城乡地理间收入的不平等。可以通过劳动力流动和解决户口解决。 腐败带来的不平等。反腐政策解决。 三个可困难的不平等： 资本收入增加。越高收入的人，资本收入越多。 高等级人群在劳动力市场和资本市场都很富有。近十年此指标增长较高。homoploutia。 社会流动性降低，教育与父母收入相关。 提问部分（翟东升） 质疑一下中国的发展未来会限制世界不平等，同时更依赖印度非洲等地区对不平等作出贡献。 仅从GDP角度分析可能是这样，但是还是应该考虑一些其他因素。 中国正在转型为资本提供者。 从2013年来，中国把越来越多的钱投入到全球外围地区。所谓的一带一路。 如果中国对人类命运共同体贡献成功，还是可以减轻全球不平等。 在未来我们会通过长期促进全球外围地区发展来推动平等。 homoploutia，其实是一个可以解决的办法。 新时代中国特色社会主义市场经济，整个世界的体制正在趋同。 什么样的资本主义应该实行？ 美国的撒钱（helicopter money）更像共产主义来。 欧洲的高福利其实使人懒惰。 如何平衡成本和收益？ 对欧洲福利进行调整，构建一种社会福利体系。 欧洲很多的福利被用于或者浪费于公共系统，或者让老年人多活一个月，这是性价比很低的。 公共部门为了挽救人们生命和财富分配的干预应该致力于减少年轻人之间的不平等。 年轻人无法选择，但是年长的人应该对自己的行为负责。 所以财力应该节省出来给年轻人。 中国的资本增长主要在于地产，年轻人要承担这个后果。 在中国我们应该补偿这种代际剥夺。 抛去资本所有权的集中，更应该谈论数据所有权的集中。 1971年之前资本指黄金，之后指大国的主权信用。 1980后我们见证着资本稀缺性的下降。 目前最重要的是数据的所有权。 在美国是公司高管们拥有这些数据。 回复部分（BM） 一带一路可以为非洲带来增长，但是技术上讲，还是会不可避免的加剧不平衡。 代际不平衡类似人口老龄化，这种不平衡在美欧都有。 年轻人进步的机会比其父母更少。 资本收入包括数据等资产。资本的种类的确在变化。","link":"/economy-finance/seminar-talks/global-inequality-china/"},{"title":"肖钢讲座：我国多层次资本市场建设","text":"主讲人：肖刚（中国证监会前主席） 日期：2020-11-19 十四五金融工作八大任务： 建设现代央行制度（1983建立，农业改革到企业改革，为金融发展提供历史性条件） 构建金融支持实体经济的体制机制 国有商业银行改革 市场枢纽，注册制，常态化退市，提高融资比重 金融双向开放 完善监管体系、透明度、法制化 整治乱象 防范风险 1. 主要内容（PPT总结归纳而来）1.1. 重要意义多层次资本市场强调资本市场枢纽功能（举例2013钱荒)： 微观主体活力、稳健货币政策、资本市场功能 $\\rightarrow$ 相互支撑 社会的需要推动政策的变动。 多层次资本市场是我国特有术语： 以金融工具类别来分：股债衍生品 以交易场所来分：场内场外市场。交易所市场，产权交易市场（中央最新文件，并列为新市场体系） 以发行方式来分：公募大众的市场，私募小众的市场 多层次资本市场重要意义： 改善融资结构服务实体经济 促进科技创新 满足财富管理需求 提高直接融资比重防范金融风险 扩大开放提升影响力 助力内循环与双循环： 资源配置能力 风险分担能力 市场约束能力 “技术创新+资本市场”是产业升级的强大推动力。 存在两大问题： 科技成果转化专业化服务体系不健全， 高校、科研与市场供需对接不畅。 信用评价体系缺乏差异化、针对性。知识产权融资有限、质押率偏低、不良率较高。 投贷联动有待完善。 1.2. 私募市场私募基金服务实体经济（2019年3季度末） 累计投资境内未上市未挂牌企业股权、新三板企业股权和再融 资项目数量达到11.18万个，形成股权资本金6.36万亿元。所投资账面价值分布，股权类资产占比46.2%。 2018年向境内未上市挂牌企业股权本金新增1.22万亿元，相当于同期新增社会融资规模的6.3%。 在投中小企业项目5.98万个，本金2万亿；在投高新技术企业 3.1万个，本金1.27万亿。 资金主要来源于：企业&gt;资管计划&gt;居民 1.3. 交易所市场中国上市公司海外收入比较低。 A股再融资政策调整： 非公开发行对象数量上限分别由10名和5名（创业板） 统一调 整到35名。 非公开发行价格由原先不得低于定价基准日前20个交易 日均价的90%，改成80%，吸引投资者，降低发行难度。 非公开发行锁定期缩短，原规定控股股东36个月、普通 投资者2个月不得转入，分别减半，改为18个月和6个月，不再受减持规则限制。 A股行业表现： 高增长、不稳定：5G、新能源、军工、半导体、互联 网、券商、创业板、中证1000。牛市涨得多，熊市跌 得多。 高增长、很稳定：医疗、医药、消费、白酒、食品饮 料、家电。牛市涨得多，熊市跌得少。 低增长，不景气：煤炭、纺织、有色、钢铁、能源、 化工、交通。牛市涨得少，熊市跌得多。 高股息、低增长：机械设备、电力、金融、地产、上 证50。牛市涨得少，熊市跌得少。 北向资金： 北向资金占外资持股近七成（2019年底）。 对A股有明显领先性，多次精准抄底“聪明钱”，作为投资决策的一个参考。 北向资金类型： 配置型资金，主要跟踪MSCI等指数 套利型资金（个股、指数、汇率间的利差），如离岸人民币汇率走高，北向资金流出，A股下跌 北向资金择股： 行业发展成熟，在产业链上具有高溢价。 所处行业壁垒高，短时间很难有新的竞争者。 持续稳定的高ROE. 什么是股票发行注册制： 注册制不是登记备案制，股票发行上市仍要经过审核同意，但与现行核准制不同。 只要不违背国家利益和公众利益，企业能不能发行、何时发行、以什么价格发行，均应由企业和市场自主决定。 以信息披露为中心，企业必须披露充分和必要的投资决策信息，审批部门不对企业资产质量和投资价值进行判断，更不“背书”，也不对信息披露的真实性负责，但要对招股说明书的齐备性、一致性和可理解性负责。 发行人是信息披露第一责任人，中介机构承担对发行人信息披露的把关责任，投资者自主作出投资决策并自担投资风险。 实行宽进严管，政府职责重在事中事后监管，严惩违法违规，保护投资者合法权益。 推进注册制改革的核心在于理顺市场与政府的关系。 注册制的巧妙之处就在于既能较好地解决发行人与投资者信息不对称所引发的问题，又可以规范监管部门的职责边界，避免监管部门的过度干预，不再对发行人“背书”，企业业绩与价值、未来发展前景均交由投资者判断和选择，股票发行数量与价格由市场各方博弈，让市场发挥资源配置的决定性作用。 监管部门则集中精力履行好事中事后监管职责，维护好市场秩序。 创业板面临挑战： 发行、定价市场化程度 再融资、退市效率，优胜劣汰功能 前沿产业与未来科技企业不足，缺乏顶尖巨头新兴产业。 “易进易出、快进快出、大进大出+明星企业” 1.4. 上市公司相关制度我国退市的现状：退市率低 成因分析： 强制退市标准尚不完备，且可执行性较差。 退市执行力度不够，为规避退市行为提供了条件。 股票发行被人为调控，造成上市资源稀缺。 投资者权益保障不足，加大了退市难度。 直接融资渠道有限，上市公司退市后生存困难。 1.5. 债券市场防范化解债券市场风险。 两大风险：企业违约风险、部分中小金融机构和非法人产品的杠杆风险。 总体思路：坚持市场化、法治化原则，有序可控打破刚性兑付，打击逃废债行为，加强政策协调，完善监管制度，切实保护投资者利益。 政策措施： 健全市场化、法治化信用风险处置机制。如银行理财、券商资管计划等非法人产品管理人在破产重整中的法律地位。 合理控制债券市场杠杆水平。 规范债券市场信用评级，统一准入管理，建立以投资者为主导的市场化评介制度，推动投资人付费服务模式，引入评级机构强制退出机制。 加强监管协调，监测预警，信息共享和失信联合惩戒。 1.6. 衍生品市场增强实体企业利用衍生品抵御风险的能力 我国商品期货共70个品种，自2013年以来，成交量连续五年占全球一半以上。 研究表明，实体企业运用衍生品套期保值，能降低风险，提高企业价值，在经济下行期更为明显。 持续推进衍生品市场改革，扩宽服务实体经济范围。 提升企业参与衍生品市场的意愿与能力，加快国际化步伐，提升我国定价影响力。 1.7. 资本市场基础设施狭义：证券市场参机构之间用于清算、结算或记录、支付证 券的多边系统。 支付系统、中央托管机构、证券结算机构、中央对手 方、交易数据库。 广义：上述五类以外扩大到：证券、期货、保险、黄金交易 所、征信系统、法律和监管环境、公司治理、会计准 则、反洗钱以及金融安全网。 1.7.1. 如何提高基础设施建设与改革重点难点：平衡安全、效率、成本以及证券行业发展的相互关 系。安全是前提，效率是关键，成本是基础，行业 发展是支撑。四者有机整体，相辅相成。 改革内容： 基础设施集约化：债券市场3家托管结算机构 场外交易数据库建设 区域性股权市场登记、托管、评估系统。 托管模式：直接持有模式和间接持有模式。间接持有法律关系：共有权（德国），信托关系（英国），证券权益（美国） 基础设施开放：与国际结算规则接轨、DVP制度境内外联结风险评估与防范，基础设施走出去。 基础设施风险防控：跨境跨市场风险，股票/债券质押、回购、期权市场，场内场外市场。流动性风险与交收违约风险。研究债券中央借贷机制，引入安全、效率高的流动性工具，完善担保品管理，全面推广逐日盯市，精细化，标准化，自动化管理，完善法律，建立折价过户、拍卖、债务抵消等快速处理机制 基础设施统筹监管 2. 给肖主席个机会讲一讲（没时间讲PPT） 我们的资本市场是在1990年建立，（1989动乱之后），那个时候资本主义国家封锁中国经济。 成立交易所释放了坚持改革开放的信念。 我国金融学过多的运用了西方金融学的理论，但是我国金融学的内涵与西方并不一样。应当创造性地研究提出中国特色的体制。 国外学界对中国经济发展肯定，但是对金融制度抱有负面看法。那么为什么经济成功金融不行？ 3. Q&amp;A 社会需求推动政策改革还是国家改革推动社会进步？ 社会的需要推动政策的变动。 怎么能不让科创板走创业板老路？ 上交所科创板针对战略新兴产业。 创业板起初主办化，包容性较差。 创业板在科创板上市时政策变化风险较大，先看科创板改革结果。 创业板也增强了保荐责任（推荐亏损企业必须跟投） 深交所所长：什么时候能恢复深交所主板上市？ 八卦很久没有实质性内容。 政府会不会拉一个牛市。 不能出现08、15年的情形，否则会影响改革的进行。 Andy：人生经验故事？ 没干货，终身学习 如何和香港制度接轨？合作？（这个人问得不清不楚） 建设国际金融枢纽 创新要素流动方面存在障碍（人才资本土地知识产权等）","link":"/economy-finance/seminar-talks/xiaogang/"},{"title":"Icarus 主题设置","text":"本文内容都与当前使用的主题 Icarus 相关。 具体的主题设置可以见此链接，对应的 markdown 源代码可以见此链接。 布局设施侧边栏设置sidebar中某个侧边栏的sticky为true来让它的位置固定而不跟随页面滚动。 _config.icarus.yml12345sidebar: left: sticky: false right: sticky: true 样式更改字体更改觉得自带 Ubuntu 字体太丑了。 更改字体需要更改两个文件。 这个文件是告诉浏览器使用哪个字体的。 /Users/rui/Documents/Note/node_modules/hexo-theme-icarus/include/style/base.styl12// line 9$family-sans-serif ?= 'Open Sans', 'Noto Serif SC', 'Microsoft YaHei', sans-serif 这个文件是告诉浏览器下载哪个字体的。 /Users/rui/Documents/Note/node_modules/hexo-theme-icarus/layout/common/head.jsx123// line 54// 输入需要下载的字体即可default: fontcdn('Open+Sans:wght@400;600&amp;family=Source+Code+Pro', 'css2'), 同时部署时需要把文件拷进去。 参考： github hexo blog web font 적용하기 博客相关问题一揽子记录 主题定制之后是一定会对主题进行定制的，但是近期没有时间，而且欠缺一些前端方面的知识。 这里记录一些见过的对 lcarus 定制的连接，用于学习和获得灵感。 List： hexo及icarus主题个性定制：文章显示较为好看，而且加了不少插件。 hexo icarus 테마 프로필영역 css 수정：这个是加了一个新主题。 Icarus 主题自定义：一些细节改动 Hexo博客icarus主题定制 功能开启或添加加入博客评论区 Gitalk有很多插件可供选择，这里我们使用了 Gitalk，本质上是在 Github 上面新开一个 Repo，然后在 issue 区记录评论，然后通过 OAuth App 读写并显示到博客中。 具体的设置可以参见 Icarus 的用户评论插件。 插件设置好之后可能会出现一些问题： gitalk授权登录后报错403：一般来说是版本或 Proxy 的问题，需要升级版本或者更换 proxy，我选择了后者。参考 Gitalk 的 issue 和这篇博客。 文章批量初始化：我由于文章数较少，是一个一个手动初始化的，日后可能会需要用上批量初始化。 加入博客评论区 GiscusIcarus 并没有对 Giscus 的支持，所以还是要从 index 文件层面进行改动。 整体来说愿意与 Gitalk 比较相似，不过不是在 issue 区记录评论，而是在 discussion 区记录。 回答可以按照层级展开，个人比较喜欢。 首先访问 Giscus 官网，获取对应的 html 代码，如下所示 1234567891011121314&lt;script src=&quot;https://giscus.app/client.js&quot; data-repo=&quot;xxxxxxxxx&quot; data-repo-id=&quot;xxxxxxxxx&quot; data-category=&quot;xxxxxxxxx&quot; data-category-id=&quot;xxxxxxxxx&quot; data-mapping=&quot;pathname&quot; data-reactions-enabled=&quot;1&quot; data-emit-metadata=&quot;0&quot; data-input-position=&quot;bottom&quot; data-theme=&quot;light&quot; data-lang=&quot;zh-CN&quot; crossorigin=&quot;anonymous&quot; async&gt;&lt;/script&gt; 之后只需要将其放到原有的主题中放置 comments 的地方，替换原有代码即可。 在 Icarus 中对应的文件是 node_modules/hexo-theme-icarus/layout/common/comment.jsx。climbing 参考了以下几篇文章： Giscus: The New Commenting Engine for My Website How to use the github discussions to add a comment feature to your static website built with hexo? 此外如果想要在 config 文件中控制 Giscus 的话，可以做以下更改 _config.icarus.yml1234567891011121314giscus: enable: true data_repo: xxxxxxxxx data_repo_id: xxxxxxxxx data_category: xxxxxxxxx data_category_id: xxxxxxxxx data_mapping: pathname data_reactions_enabled: 1 data_emit_metadata: 0 data_input_position: bottom data_theme: light data_lang: zh-CN crossorigin: anonymous comment.jsx123456789101112131415161718{(() =&gt; { if (config.giscus.enable === true) { return &lt;script src=&quot;https://giscus.app/client.js&quot; data-repo={config.giscus.data_repo} data-repo-id={config.giscus.data_repo_id} data-category={config.giscus.data_category} data-category-id={config.giscus.data_category_id} data-mapping={config.giscus.data_mapping} data-reactions-enabled={config.giscus.data_reactions_enabled} data-emit-metadata={config.giscus.data_emit_metadata} data-input-position={config.giscus.data_input_position} data-theme={config.giscus.data_theme} data-lang={config.giscus.data_lang} crossorigin={config.giscus.crossorigin} async&gt; &lt;/script&gt; } })()}","link":"/software-tools/hexo/Icarus/"},{"title":"Hexo常用功能说明","text":"本文档会用作Hexo基本用法记录演示。 新建博文1$ hexo new &quot;My New Post&quot; More info: Writing Generate static files + Run server123$ hexo clean # Clean local files$ hexo generate # Generate static files$ hexo server # Build local server More info: Server More info: Generating 远端部署 (use Gitbuh Action)The procedure is in .github/workflows/deploy.yml When this local git folder has been uploaded to github, the deploy.yml would be executed. More info: Deployment 文章折叠1&lt;!-- more --&gt; 文章间引用站内文章引用语法如下。 1{% post_link file_name Title_of_link %} Insert figures不同于markdown的图片引用方法，Hexo有着自己的语法。 图片文件夹位于_post目录下 1{% asset_img test.jpg%} 同时可以自定义图片大小。 语法与html语法相同。 1&lt;div style=&quot;width:70%;margin:auto&quot;&gt;{% asset_img test.jpg%}&lt;/div&gt; markdown的语法需要配置之后才可以使用，具体配置的方法见这篇文章 1![Test](test.jpg)","link":"/software-tools/hexo/hexo-functions/"},{"title":"Hexo-Trials","text":"本文记录使用Hexo中遇到的很多坑，以后可能会重复遇到，记录在此，以观后效。 功能&amp;支持Markdown image insertion grammar support具体配置的方法 123456_config.ymlpost_asset_folder: truemarked: prependRoot: true postAsset: true 运行&amp;测试坑Local server存在local server无法打开或打开极慢，但是GitHub Action正常部署的情况。 此时使用全局代理可解决。 经查原因为在编译HTML时，难以获取all.css文件，fontawesome.com需要代理访问。 将该网站加入代理规则即可解决。 写作排版坑公式支持使用 \\\\ 时不换行，原因是hexo使用的markdown引擎造成的。 其将第一个 \\ 识别为转义符号。 解决方法是换一个引擎，如下： 12npm uninstall --save hexo-renderer-markednpm install --save hexo-renderer-kramed 参考： https://jdhao.github.io/2017/10/06/hexo-markdown-latex-equation/ https://lanlan2017.github.io/blog/eb86e892/ 行内公式无法编译换用 kramed 作为 markdown 的渲染引擎之后，行内公式有时无法渲染。 一般情况下是因为其对 $\\ast$ 和 $_$ 的支持有些不同。 在行内公式时优先会考虑 markdown 语法（这两个符号会优先认为是在定义斜体），其次才是公式支持，所以这种情况下无法编译。 解决方法是对文件 /node_modules/kramed/lib/rules/inline.js 进行修改。 12// em: /^\\b_((?:__|[\\s\\S])+?)_\\b|^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/,em: /^\\*((?:\\*\\*|[\\s\\S])+?)\\*(?!\\*)/, 这样可以解决下划线的问题。同时 markdown 中星号使用 \\ast 代替。 此外在部署的时候同时记得将远程的文件替换或修改即可。 参考： https://suixinblog.cn/2018/10/hexo-latex.html","link":"/software-tools/hexo/hexo-trials/"},{"title":"Hexo文档更新时间设置","text":"Hexo 在使用远程部署时，默认 update_option: mtime, 即以最后修改时间作为更新时间。 这个问题导致每次编译时，文章提交到远程，所有的文章都显示更新，且时间相同。 具体解决方法则是在 Front-matter 中加入updated:项，则编译过后的更新时间以此为准。 目前看来没有可以使其自动更新的方法。 批量加入更新时间这里对之前原有的 post 我们可以按照生成时间 date 来批量添加 updated： 具体事项过程即在 _post 目录下运行以下代码： 1for file in `ls .`; do value=`gawk '/date:/{print &quot;updated: &quot;$2&quot; &quot;$3}' ${file}`; echo ${value}; gsed &quot;3 a\\\\${value}&quot; -i ${file}; done; 需要注意的是，awk 和 sed 命令在 MacOS 下使用方法与 Linux 不同，此处应该使用 gawk 和 gsed。 手动更新时间在 _config.yml 中设置 update_option: date，则更新时间与文章创建时间一致。 虽然这样起不到“更新时间”的作用，但是至少不会无缘无故的对未更新的文件进行更新。 如果出现一些重要的文章修改，手动更新时间添加 updated: 即可。","link":"/software-tools/hexo/hexo-updated-time/"},{"title":"腾讯云相关记录","text":"本文记录在腾讯云服务器上部署 Hexo 搭建的博客时遇到的问题。 腾讯云相关远程连接需要在本地使用 ssh 连接云端 root 账户或者普通账户。 首先在腾讯云里 ubuntu 下管理员账户名为 ubuntu 而不是 root。 第一次链接出现管理员账户无法使用密码和密钥连接其普通账户也无法使用密码登录的情况。 这时需要在腾讯云实例的控制台中对 ssh 进行相关设置。 具体来说修改以下文件 /etc/ssh/sshd_config。 12PasswordAuthentication yes # 开启密码登录权限PubkeyAuthentication yes # 使用密钥登录 之后重启服务 sudo service sshd restart 即可。 之后使用 ssh 和下载好的密钥进行连接，详见官方文档。 使用 nginx 部署 Server详见这篇文档 其中涉及到不少对 Nginx 的操作，下面记录一些基础命令。 12345# 首先使用 Nginx 命令的时候需要使用管理员权限。## 开启服务器sudo systemctl nginx## 重新启动更新设置sudo systemctl restart nginx.service 其中 Nginx 的配置文件位于 /etc/nginx/nginx.conf，更改过后重启即可。 服务器 SSL 证书安装部署可以参考腾讯云的这篇文档。 主要就是将证书上传再再 Nginx 中设置。 需要注意的是颁发证书对应的域名一定要和真是域名相同，不要少前缀。 本地 ssh 连接服务器长时间不操作断开问题具体来说修改以下文件 ~/.ssh/config。 增添以下内容。 12345Host * # 断开时重试连接的次数 ServerAliveCountMax 5 # 每隔5秒自动发送一个空的请求以保持连接 ServerAliveInterval 5 参考这篇文章。 使用 webhook 对 repo 的更新进行监控详见这几篇文档： 使用Github的Webhooks+Node完成网站的自动化部署 使用 GitHub Webhook 实现静态网站自动化部署 使用Github的webhooks进行网站自动化部署 然而事实是我这次并没有从 webhook 的方向来部署，而是从 github action 中 ssh 到服务器进行操作。 具体的部署步骤放在服务器的 deploy 文件中。","link":"/software-tools/hexo/tencent-cloud-hexo/"},{"title":"黑苹果使用记录","text":"黑苹果使用记录，很多功能调教起来还是较为麻烦的。 win 与 macOS 下显卡切换之前一直使用的是 RX470 显卡，由于最近玩2077实在是带不动，所以考虑换卡。 由于最近狂潮，AMD 显卡大涨一波价，于是乎买A卡总觉得很不值。 于是入手了索泰1080至尊Plus（1800rmb， 2020年12月）。 但是由于N卡不支持黑果，所以目前目标为 黑苹果下把显卡独立显卡屏蔽并使用集成显卡 UHD630 作为显卡。 Windows 下正常使用1080。 其实操作很简单，理论上只需要在 config 文件中加入wegnoegpu屏蔽外部显卡即可。 简述一下现在的设置情况： clover 的 config 下，添加 whatevergreen 驱动里的命令 wegnoegpu，便可正常进入 macOS。 主板 bios 设置为集显优先，则用集成显卡进入 clover。 主板插 DP 线，显卡插 HDMI 线（这个很重要且很玄学！我最开始是相反方式插着，结果无法进 macOS 系统。（显示器是有一个DP口）） windows 下进入会有两个信号源，集显和独显。在设备管理器中关闭集显不管用（集显的DP口仍有输出。） 在显示设置中设置只显示特定桌面（只显示独显连接的桌面）","link":"/software-tools/macos/hackintosh/"},{"title":"MacOS 功能与设置","text":"不得不说在多年的MacOS使用过程中还是有很多不方便的地方。 本帖记录一些经常遇到的问题与解决方法。 外接显示器音频设置MacOS是不可以通过HDMI/DP控制外接显示器的音量输出的。 但是黑苹果是一定要外接显示器，同时使用显示器自带的扬声器是极为方便的，所以这是一个需要解决的问题。 目前来看有三种解决方案： Soundflower配套方案 Monitor Control插件 Sound Control插件 其中Monitor Control在我的黑苹果上只可以调节亮度无法调节声音。 Sound Control试用期内可以使用（单独条件app声音），但是试用期过后无法通过调节全局声音来调整显示器输出。 Soundflower方案之前在黑苹果上一直使用，后来重装系统之后出现了蜂鸣现象。 考虑到SF方案体验很不错，于是尝试解决蜂鸣现象。 先简要介绍，SF方案需要下载Soundflower插件与相应的客户端Soundflower Bed（菜单栏插件）。 简单配置即可使用，此处省略，可以使用全局的音量调节调整显示器音量。 但是使用过程中存在播放一定时间后，出现蜂鸣Humming/Bizz，在之后便无音量输出，由于SF项目已经多年未维护，所以在issue里看大家讨论的结果。 问题出现的直接原因是Buffer size过小，溢出时就会出现此问题。 治标不治本的方法时直接选择最大的Buffer size，但是时间过长时还是会出现此类问题。 从大家的讨论中得出，问题出现的根本原因是Soundflower可能存在内置时钟与设备时钟不符，才导致缓冲区最终会溢出。 一篇2014年的帖子给出了方案. 新建一个”聚合设备”（名称必须用英文），时钟源设置为”Soundflower(2ch)”，实体设备显示器勾上“漂移修正”，然后就可以拿给Soundflower用了。 问题解决。 外接2K显示屏开启HiDPIMacbook外接大部分2K显示器时并不会开启HiDPI，需要手动开启。 开启方法很多帖子写的很详细了，详见此链接。 印象中是有一些坑，待下次再设置的时候更新。 Google Software Update 自启动详见这篇文章。","link":"/software-tools/macos/macos-functions/"},{"title":"主动学习与交互决策过程","text":"之前导师让调研一下“基于主动学习技术的可交互策略学习方法”，此处整理成文便于留存。 调研背景介绍调研关键词：主动学习，可交互，策略学习，降低标注成本 相关领域主动学习首先我们指出广义场景下主动学习有两种含义，技术面和任务面（不冲突，侧重点不同）。 但是不论是指技术还是任务，当我们提到主动学习的时候，总是伴随着对某项成本降低的期望。 技术面：一种以某种原则选取数据的技术（此种含义不必须与人交互） 任务面：一种以某种原则选取数据从而节省监督学习标记成本的任务（标记任务需要与人交互） 策略学习一般来说，策略是在指定任务中为完成任务目标而实行的行动准则。 目前来看，有两种被广泛使用的策略学习方法： 强化学习（需要与环境交互） 模仿学习（需要与专家系统交互） 具体调研的方向故此调研的调研重点置于主动学习AL与强化学习RL与模仿学习IL的交叉方向上面(以降低成本为目的)。 此处的分析及本文的调研以问题为导向，不特别深入到具体技术的归类。 问题分类（调研结构以此为标准）： 传统AL数据标记任务下的问题（监督学习中设计相关策略减少标记成本）（标记任务中与人交互） 使用RL/IL的policy作为AL标记任务的选取策略（AL的任务面） RL场景下的问题（有时与环境交互复杂昂贵） 定义一种数据选取的原则来指导policy的学习（AL的技术面） IL场景下的问题（与专家系统交互复杂昂贵） 定义一种数据选取的原则来指导policy的学习（AL的技术面） 另一种分类（在此调研中可找到相应模块）： 在策略学习中无人类参与 Active Reinforcement Learning 在策略学习中有人类参与 Imitation/Reinforcement Learning in Conventional AL Process Interactive Reinforcement Learning Active Imitation Learning 三类相关方向1. 传统AL数据标记任务背景简介在AL的任务面下， AL与RL有着相似的任务框架，都可以看作MDP。 两类问题描述如下： RL AL（任务面） Task Decision tasks Instance selection tasks with supervised learning Purpose Learn best policy to maximize the total return Construct best labeled condition (coming with best prediction results) Goal Policy Labeled condition 我们将两类问题的相关概念进行对比如下： RL AL（任务面） agent Apply [current state =&gt; action] selector Apply [current labeled state =&gt; selection] state Describe current condition labeled state Current labeled and unlabeled data action The response from the agent under the certain state selection The response from the selector under the curtain labeled state. (A set of unlabeled data) environment Apply [state + action =&gt; next state + reward] oracle/interaction Apply [labeled state + selection =&gt; next labeled state]. Oracle annotate the selection set. reward Given by the environment after an action is taken policy The guideline for the agent to take optimal actions query strategy The guideline for the selector to make optimal selections value Evaluation of the current state/action/state-action-pair score Evaluation of the current potential selections model (not necessary) The learned formula of the environment model The learned formula of the oracle 我们发现RL中的Policy其实对应着AL中的选取策略。 在此方向上，我们可以发现两者的区别 相似处：RL policy和AL strategy都应该同时考虑exploration&amp;exploitation。 不同处：RL学习policy，但是AL通常使用预先启发式设定的strategy。 问题与思路存在的问题： AL通常使用预先启发式设定的strategy其实很多时候并不是最优，而且对不同场景不一定适用。 这就带来了一个问题，这个预先设定好的heuristic有时候并不是最优，对不同场景不一定适用。 直观的方法： 从RL与AL的相似性出发，我们或许可以运用强化学习学习policy的方法来学习一个strategy。 目前主要有三类工作（数据标记任务，所以需要与人简单交互）： 在一个相关的领域学习策略 之后不伴随策略在数据集间的迁移 之后伴随策略在数据集间的迁移 直接在目标领域学习策略 KEY WORDS： Learning to actively learn, Data-driven AL, Meta-active learning (ability to work with all kinds of data) 现有工作列表在一个相关的领域学习策略，之后不伴随策略在数据集间的迁移： Learning active learning from data [2017, NIPS]: LAL. In pool-based AL setting. Random forest as basic classifiers. The state consist the current classifier parameters and a randomly selected data (Monte-Carlo). Train a random forest regressor that predicts the expected error reduction for a candidate sample in a particular learning state. The regressor works as the value function of the state action pair. Train the policy on the representative dataset and use it on the target dataset (without updating). Learning How to Actively Learn: A Deep Imitation Learning Approach [2018, Annual Meeting of the Association for Computational Linguistics]: ALIL. In pool-based AL setting. The task is named entity recognition in a cross lingual setting. The state consists of the labeled and unlabelled datasets paired with the parameters of the currently trained model. An action corresponds to the selection of a query data point, and the reward function is the loss of the trained model on the hold-out evaluation set. Imitation Learning directly learns the map (policy) from state to action in a supervised manner. The policy is then used on the target dataset (without updating). Learning to Actively Learn: A Robust Approach [2020]: Address that previous works in this area learn a policy by optimizing with respect to data observed through prior experience (e.g., metalearning or transfer learning). This approach makes no assumptions about what parameters are likely to be encountered at test time, and therefore produces algorithms that do not suffer from a potential mismatch of priors. 在一个相关的领域学习策略，之后伴随策略在数据集间的迁移： Learning how to Active Learn: A Deep Reinforcement Learning Approach [2017, Arxiv]: PAL(Policy based Active Learning). In stream-based AL setting where the policy/strategy is to decide whether to query the current instance. The task is named entity recognition in a cross lingual setting, where transfer the learned strategy/policy to the target domain (where no enough data to learn a strategy). The policy is learned by deep Q-network. The state is the current instance the previous constructed dataset. The reward is given by a hold-out set. Then the learned policy is transferred to the target dataset with policy updating. 直接在目标领域学习策略： RALF: A reinforced active learning formulation for object class recognition [2012, CVPR]: First to consider AL as a MDL process. They use Q-learning to learn the adaptive combination between exploration and exploitation. Use the overall entropy as the reward in each iteration. The state is the strategy combination condition and the action is the trade-off parameter. They also used a guided initialization for Q-table. Active Learning by Learning [2015, AAAI]: ALBL. By Hsuan-Tien Lin (NTU). Connect AL with the well-known multi-armed bandit problem. Choose strategies in the AL process by estimating the performance of different strategies on the fly. Use EXP4.P as the core solver. The action is to choose a specific strategy. Learning Loss for Active Learning [2019, CVPR]: Attach a small parametric module, named “loss prediction module”, to a target network, and learn it to predict target losses of unlabeled inputs. The “loss prediction module” could be considered as the value function of the state-action pair. The state is the current model parameters and the action is the selected instance. The idea to directly predict the loss is similar to [Learning active learning from data [2017, NIPS]], but this work doesn’t need to train the policy in advance. The policy is trained during the AL process. Learning How to Active Learn by Dreaming [2020, ACL]: The follow-up work for [Learning How to Actively Learn: A Deep Imitation Learning Approach]. Recent data-driven AL policy learning methods are also restricted to learn from closely related domains. This method fine-tune the initial AL policy directly on the target domain of interest by using wake and dream cycles. Cross-domain and cross-lingual text classification and named entity recognition tasks. Wake learning is an AL process, and the dream learn is a policy updating process. The current weak model was used as a weak annotator to train the policy in the dream phase. 此类方法在其他类型标注任务中的相关应用: Multi-modal Active Learning From Human Data: A Deep Reinforcement Learning Approach [2019, ICMI] Reinforced active learning for image segmentation [2020, ICLR] Learning to Actively Learn Neural Machine Translation [2020, CoNLL] Deep Reinforcement Active Learning for Medical Image Classification [2020, MICCAI] 分析与看法首先这一节我们面对的还是传统的AL场景，面对的主要是数据标注的任务。 相比于传统的AL启发式方法，此类学习Policy的方法对算力的要求很大，虽然标称的表现也会好，但是如果真的实际使用则需要权衡。 浅见： 对不同任务设计特异性的AL策略（不论是启发还是学习一个策略）是当下的趋势。 对于一些复杂任务，基于RL策略的迁移或许是一种良好的解决方案。 2. Reinforcement Learning 场景背景简介强化学习相关知识与概念在此不赘述。 具体可以参见这篇文章。 强化学习面对的问题场景就是一个广义的学习一个智能 agent 的场景。 一般通过与环境的交互来学习 agent 的 policy。 环境交互的过程中一般会黑盒地出状态的转移以及相应的奖励。 Policy 的训练依赖于观测的结果。 一般是在观测到的 trajectory 中进行训练: \\{S_0,A_0,R_0,S_1,A_1,R_1,S_2,A_2,R_2,...,S_{n-1},A_{n-1},R_{n-1},S_n,R_n\\} （主流的强化学习方法分为 model-based 与 model-free 两种，区别在 policy 的训练是否依赖对环境的状态转移建模。） 训练的过程中存在一个问题：若持续以当前的学到的策略与环境进行交互（exploitation），那么学习的结果则容易陷入一个局部最优当中。 所以在强化学习中 exploitation 需要伴随 exploration，这样可以避免陷入局部最优并且找到更好的策略。 最简单的分配方式是 $\\epsilon-greedy$，按照一定比例分配 exploitation 和 exploration。 如果 exploitation 过多，则容易陷入局部最优，如果 exploration 过多，则较难有充足的数据训练一个较好的policy。 所以说 exploitation 之外的 exploration 也十分重要。 通常情况下RL假设与环境的交互是即时完成且没有花费的。 然而现实情况下，与环境的交互，通常是需要付出代价的（时间或算力或实际成本）。 在 large, high-dimensional environment 中，这个成本更为高昂（复杂的模拟计算等）。 此时我们希望我们每一次的交互都能被更好的利用起来： 在 exploitation 中用更好的训练方式（大部分强化学习在研究的的） 在 exploration 中选择更好的交互样本，更高效探索（active） 于是RL则与AL选取更优样本的逻辑产生了交叉，主要是聚焦于探索的场景下。 《人工智能：一种现代的方法》21.3中的主动强化学习提到了这一现象。 我们想更优更高效地 exploration（Agent 不必关心那些它知道不需要的而且可以避开的状态的具体效用）。 此处我们称这种选取为 active reinforcement learning。 此处的 Active 不一定一定要有人的存在。 这里的 Active 含义: the agent seeks out novelty based on its own “internal” estimate of what action sequences will lead to interesting transitions. 问题与思路存在的问题： RL与环境交互会产生交互成本。 较多的探索会影响模型的训练，同时也会造成一定的cost损失。 直观的方法： 借鉴AL的思路，制定或学习探索的方式，更高效的探索，使相同数量的探索下训练的policy更好。 目前主要有以下几类工作： ARL: 整体框架仍是RL的框架。加入探索的heuristic。（不与人交互，只与环境交互） ARL-1: 与环境交互获取状态转移和奖励时产生cost ARL_2: 只在与环境交互获取奖励时产生cost，状态转移不产生cost。 IRL: 以RL的框架为基础，添加interactive的部分。（需要与人交互） KEY WORDS： active exploration, active reinforcement learning, interactive reinforcement learning 现有工作ARL-1:与环境交互获取状态转移和奖励时产生cost。(No human interactions) 由于需要主动的选择来与环境进行交互，一般来说是要选取我们认为环境能反馈更多信息的样例。 通常的做法是在对环境的建模上来衡量informativeness。 具体衡量的方法与AL中的方法相似（QBC/Entropy/Information Gain, etc.） 所以说绝大部分的工作都是基于Model-based RL。 Active Reinforcement Learning [ICML, 2008]: Before this work, many ideas are trying to explore the actions which agents has experienced the fewest number of times in the past. In this paper, the agent determines the sensitivity of the optimal policy to changes in transitions and rewards. It then focuses its exploration on the regions of space to which the optimal policy is most sensitive. The process is similar to ADP. Use Taylor’s approximation to model the local sensitivity of the utility on current policy. VIME: Variational Information Maximizing Exploration [2016, NIPS]: Agent should take actions that maximize the reduction in uncertainty about the dynamics (transition). Use BNN to learn the model. This can be formalized as maximizing the sum of reductions in entropy of transition probability. The exploration to the uncertain state of the models should reward more. Model-based active exploration [2019, ICML]: Better state-action pair makes the model ensemble changes more. The utility of the state action pair is the disagreement among the next-state distributions given s and a, in terms of JSD (Jensen-Shannon Divergence), of all possible transition functions weighted by their probability. (用可能的transaction分布变化作为criteria。) Self-supervised exploration via disagreement [2019, ICML]: The idea is similar to Model-based active exploration. Generate an intrinsic reward, defined as some difference measure (i.e., KL-divergence, total variation) across the output of different models in the ensemble drives exploration. (Such exploration might be ineffective, as the policy may visit regions of the state space which have no relation to solving the task.) Explicit explore-exploit algorithms in continuous state spaces [2019, NIPS]: Applicable in large or infinite state spaces. Exploration and exploitation are controlled by a threshold. Exploration is guided by the disagreement in the model set. And the models updated after get the exploration trajectories. Exploitation is guided by one of the model in the model set. Ready Policy One: World Building Through Active Learning [2020, Arxiv]: View MBRL exploration as an active learning problem (select trajectory from trajectory space), where we aim to improve the world model in the fewest samples possible. Acquiring data that most likely leads to subsequent improvement in the model. The exploration metric is based on reward variance computed from a (finite) collection of models. SAMBA: Safe Model-Based &amp; Active Reinforcement Learning [2020, Arxiv]: From Huawei UK. Safe-RL setting, every trajectory comes with a cost. Aim at minimizing cost, maximizing active exploration (bi-objective), and meeting safety constraints. Increases in regions with dense training-data (due to the usage of CVaR constraint) to aid agents in exploring novel yet safe tuples. The exploration use an expected leave-one-out semi-metric between two Gaussian processes defined, for a one query state-action pair. (Model change) ARL-2:只在与环境交互获取奖励时产生cost，状态转移不产生cost。 Specify the cost C: Active reinforcement learning with monte-carlo tree search [2018]: The reward needs to be paid to observe. The received reward has the discount for the querying cost. Active Measure Reinforcement Learning for Observation Cost Minimization [2020]: The policy applied with Q-learning. Add whether to query as part of the action in Q-table, so the number of action would be twice as large as the ordinary Q-table. After each query, it jump to the queried state and update a transition model. Otherwise, it go to the state according to the transition model. (没太懂这篇文章的intuition) IRL:与环境交互获取状态转移和奖励时产生cost，人类交互不产生cost 此类问题的背景仍旧是搜索空间大，搜索耗时且复杂。 所以仍然是在与环境交互时产生cost，希望使用人类专家在训练时的介入来缓解这个问题。 人类专家的交互仍然是为了使exploration更高效。 但是需要强调，此处的问题并没有对与人类专家交互的数量与质量进行假设。 Exploration from Demonstration for Interactive Reinforcement Learning [2016, aamas]: A model-free policy-based approach, uses human demonstrations to guide search space exploration. The demonstrations are used to learn an exploration policy that actively guides the agent towards important aspects of the problem. In the proceeding of a episode, when the agent reaches a informative state (computed by leverage and discrepancy), it queries the action from the state with the closest leverage to the current state, then update the policy parameters. A self-play with the environment followed in this episode, and the parameters are keep updating. Interactive Teaching Strategies for Agent Training [2016]: A work by Microsoft. Include two agent: student(learn from RL) and teacher(fixed human policy). Contains two module: student initiated and teacher initiated. Query when the student has a low Q-value difference on the current state. (the student is uncertain about which action to take) Agent-Agnostic Human-in-the-Loop Reinforcement Learning [2016, NIPS]: Develop a framework for human-agent interaction that is agent-agnostic and can capture a wide range of ways a human can help an RL agent (e.g. Q-values, action optimality, or the true reward.) 分析与看法ARL和普通RL的工作界限其实并不是很明显，因为很多 RL 的工作也是需要处理 exploitation 和 exploration。 此处只是挑了一些很明显使用到 AL 相关思路的文章，如果要详尽调研则较为困难。 IRL 其实比较有趣，它主要是想通过人的参与让与环境的交互更高效，人在这里只是起到画龙点睛的作用，或许这种交互可以用到很多 RL 系统。 3. Imitation Learning 场景背景简介模仿学习相关知识与概念在此不赘述。 具体可以参见这篇文章。 简单来说，模仿学习会在学习的过程中与专家交互，同时获得专家对指定 state 所作出的 action \\pi_*(s)。 模仿学习的目标为学习一个 policy，使其与专家的 policy 尽可能接近（模型生成的状态-动作轨迹分布和输入的轨迹分布相匹配/获得同样的 reward 等）。 通常情况下，根据应用场景的不同，IL可以分为以下几类（By ICML 2018 IL Tutorial）: Behavior cloning Direct policy learning (multiple step BC) Inverse reinforcement learning (assume learning R is statistically easier) 与RL类似，IL同样是在观测到的trajectory中进行训练: \\{S_0,A_0,S_1,A_1,S_2,A_2,...,S_{n-1},A_{n-1},S_n\\}。 不同的是，在 trajectory 中并没有每一步的 reward。 问题与思路存在的问题： 当我们想要通过模仿来学习策略的时候，唯一的学习来源来自于 Expert 对所询问的 stat e的回答（相应 action）。 这种与 Expert 的交互是昂贵的。 直观的方法： 询问专家时，挑选更有价值的 state 来询问相应的 action。 目前的工作也是基于 IL 的分类，存在于以下两类： Direct Policy Learning Inverse reinforcement learning 在 IL 中，一部分工作使用一定数量与专家交互下 policy 的表现作为评估，类似于 AL 中的学习曲线。 所以说一部分 IL 天然的可以看作 AL 的问题（以 AL 选取策略降低与专家交互成本）。 （另一部分使用一定数量与环境交互下 policy 的表现作为评估，是以人的参与降低与环境交互成本，类似之前提到的 IRL。） KEY WORDS： imitation learning, active imitation learning, active inverse reinforcement learning 现有工作Under Direct Policy Learning: 此类工作的目标是学一个 state 到 action 的 map，是一个监督学习的过程，选取的方式基于学习到的 supervised model。 Active Imitation Learning: Formal and Practical Reductions to I.I.D. Learning [2014, JMLR]: Under the subclass Direct Policy Learning. Construct a committee by bootstrap sampling (posterior over policies). Select states by density weighted QBC strategy and get the corresponding action. The density is from trajectory from the simulated environment. Maintain a supervised labeled state-action pairs. Query-Efﬁcient Imitation Learning for End-to-End Simulated Driving [2017, AAAI]: Maintain a safety classifier to decide which state needs to be queried. Use supervised loss between the action given by trained policy and reference policy on the visited states. Under Inverse reinforcement learning: 由于目标之一是学习 value function，所以通常是以当前的 value function 来构建选取 state 的逻辑。 Active Learning for Reward Estimation in Inverse Reinforcement Learning [2009, ECML PKDD]: Take the entropy of the value prediction on the state as the sample strategy (where the reward is unsure). i.e. the disagreement on the learned reward functions. It query the specific action from the oracle on the selected state. (162) Where to Add Actions in Human-in-the-Loop Reinforcement Learning [2017, AAAI]: Selecting the state s to query the action where adding the next action most improves the estimated value of state. Policy Optimization with Demonstrations [2018, ICML]: Use discriminator between human demonstrations and trajectory from environment to make the exploration close to the human’s action. The purpose is to use the current demonstration to train a policy as good as possible. (Demonstration is collected in loops, could be considered as an AL process.) Interactive Teaching Algorithms for Inverse Reinforcement Learning [2019, IJCAI]: Focus on how could a teacher provide an informative sequence of demonstrations to an IRL learner to speed up the learning process. The main idea is to pick a demonstration with maximal discrepancy between the learner’s current policy and the teacher’s policy in terms of expected reward. 分析与看法Imitation Learning 天然和 AL 有很多相似的地方。 AL 是想模仿 oracle 的黑盒模型，IL 是想模仿 expert 的黑盒策略。 人在 IL 中的重要程度就像人在 AL 中的重要程度一样，因为人类是系统唯一可以获得监督信息的地方。 所以说本节所在意的并不是环境的交互成本，而是人类的交互成本，这就使 AIL 与 IRL 产生了区别（问题的定义不同）。 总结本文以两种维度介绍了主动学习与可交互策略相关的问题，并以第一种维度进行了展开（第二种维度同样可以找到相应模块）。 其中对于人类专家存在的交互式的策略学习，当前已经有了一些综述或者前瞻的文章。 这些工作主要介绍了不同交互的方式与目的，并不一定包含减少标记成本的模块。 Power to the people: The role of humans in interactive machine learning [2014, AI Magazine] Scalable agent alignment via reward modeling: a research direction [2018]: From Deepmind. Leveraging Human Guidance for Deep Reinforcement Learning Tasks [2019, IJCAI] A Survey on Interactive Reinforcement Learning: Design Principles and Open Challenges [2020, ACM DIS] 最新进展&amp;发展趋势本文并未对相关技术的最新进展展开探讨，但是从问题的角度来讲的确出现了一些新问题。 包括但不限于： 由 active RL 到 active safe RL IRL 中考虑交互的方式（不同形式的指导，或者相同形式指导的不同展现方式） 但是现在来看问题的大类并没有发生什么改变。 交互的成本只有两类：与环境交互成本，与人类专家交互成本。 在这次简要的调查中并没有发现其他类的问题。","link":"/machine-learning/active-reinforcement/"},{"title":"主动学习(Active Learning)，看这一篇就够了","text":"本文是对自己 Awesome-Active-Learning 项目的一个宣传及简要介绍，不定期更新。 An English Version Of This Post: Awesome Active Learning. 0. 写在最前在自己对主动学习（Active Learning）的学习和研究过程中，一直断断续续收集整理了很多不同方向的文章和研究进展，日积月累，整理的内容开始完善和系统化，于是将其开源成 awesome active learning 项目。 对于该项目，我的初衷是：维护一个包含主动学习方方面面的知识库，这样研究者和工程师可以较快锁定相关工作并展开科研。 同时，也由于个人的力量有限，希望大家可以一起对主动学习的相关进展进行追踪梳理和总结。如发现遗漏或错误，或想要增补一些新的内容，欢迎大家在该项目下 pull request 或者通过邮件 ruihe.cs@gmail.com 联系我。 知识库的整体结构如下图所示，希望大家能在这个知识库项目中找到你需要的关于主动学习的一切信息。 ​ 0.1. 关于本文本文旨在对主动学习（AL）领域从一个以问题为导向的角度，展开阐述当前主动学习的研究和应用。 此处的以问题为导向包含两个方面，一是指主动学习期望解决的实际问题，二是指主动学习应用中的技术问题。 本文的重点是对这些问题进行基本的描述和文献总结，从而让大家对主动学习的适用场景有一定的认识。 一些关于理论和技术上的分类也会有所提及，但是详细内容不会在本文中展开。 此外，本文不涉及列举具体场景下具体算法的文献，若阁下对所描述的问题场景有兴趣，可以移步 github 项目并查阅相关文献列表。 0.2. 推荐阅读的 Survey/Review作为一个相对成熟的领域，目前已有不少相关的文献综述和领域调研。 此处推荐以下两篇，其余的可以去我们的项目中查看完整的列表： Active Learning Literature Survey [2012]：作者为 Burr Settles。这是最著名也是相对较早的一篇调研，至今已有数千次引用，初步了解可以看这一篇。 A Survey on Active Deep Learning: From Model-Driven to Data-Driven [2020]：较新的一篇文章，主要是在技术角度对主动学习的方法进行分类，个人认为这篇文章需要在对具体问题有足够清晰地了解之后再看，否则过早陷入对技术的纠结其实不利于对领域的认知。 0.3. 项目关键章节传送门对于一些 AL 的关键方向，我们在此设立传送门，可以直达我们项目中的相关章节（英文版）。 主动学习策略分类：在 Pool-based 场景下的主要方法分类。 AI 中的 AL：用主动学习来尝试解决 AI 各问题下的标注/查询成本昂贵问题。 AL 的应用方向：科学研究应用和工业界应用两个方面。 AL 使用中的实际考虑：当 AL 假设不完全成立时，或者遇到新的问题场景时如何解决。 AL 内在的问题：AL 这种选取模式带来的不可避免的问题。 深度主动学习：如何在深度神经网络上使用主动学习。 1. 什么是主动学习？（若已有背景知识可跳过）监督学习问题中，存在标记成本较为昂贵且标记难以大量获取的问题。 针对一些特定任务，只有行业专家才能为样本做上准确标记。在此问题背景下，主动学习（Active Learning, AL）尝试通过选择性的标记较少数据而训练出表现较好的模型。 下面是从 Burr 的 Survey 中选取的一个简单的例子，图(c)中使用主动学习策略仅选取30个样本作为训练集训练出的逻辑回归模型即可达到90%的准确率，而图(b)中随机选取的30个样本训练出的模型却相对表现较差。 从整个学习过程来看，在相同数目的标记样本下，主动学习选取的表现要好于随机选取的表现。这种可以描绘出整个学习过程的曲线也一般用于对主动学习方法进行评估。 ​ 主动学习最重要的假设是不同样本对于特定任务的重要程度不同，所以带来的表现提升也不全相同。 选取较为重要的样本可以使当前模型以较少的标记样本数得到较好的表现。 在这一过程中，主动学习的本质是对样本的重要性（/信息度/期望带来的表现等）等进行评估。 绝大多数的工作都是围绕如何评估样本来展开。 但是随着领域的发展，相关文献的增多，主动学习一词可能在强调不同的东西。总的来说，当我们谈起主动学习的时候，我们指的是： 从问题的角度：通过以某种主动策略构建较小训练集来减少标记成本的机器学习方式。 从策略的角度：以某种方式对未标记样本重要性的评估。 从训练的角度：一种交互式的标记、训练、评估流程。 2. 主动学习的问题/任务场景标记成本高昂的情况在很多任务下都存在，所以主动学习的潜在应用问题还是比较广泛的。本节，我们会对大部分主动学习涉及到的问题和任务场景进行简要介绍。 2.1 基础问题场景这里我们只讨论最基础的分类和回归问题。在这两类问题下，有着三种不同的主动学习场景： Pool-based scenario：此类场景通常提供一个未标记的数据池，主动学习策略在数据池中选取相应样本进行标记。 Stream-based scenario：此类场景中，数据以数据流的形式输入，主动学习策略需要确定对当前数据进行标记还是直接用现有模型预测。 Query synthesis scenario：此类场景较为少见，一个未标记的数据池通常也被提供，但是主动学习策略并不是在数据池中挑选样本进行查询，而是自行生成新样本进行查询。 所以我们就有了如下几个子问题： Pool-based Stream-based Query synthesis Classification PB-classification (most works) SB-classification (rare) Regression PB-regression SB-regression (rare) (rare) 其中目前据大多数的基础研究都是基于 pool-based classification，目前大多数的文献分类也是基于此问题而分类的。 此处我们以介绍清楚问题场景为主，具体对于每一个问题的具体方法，请参考： Basic AL Problem Settings 2.2 复杂问题场景此处的复杂主要是指任务相较于简单的分类回归任务复杂。在机器学习的领域中，有不少衍生问题，包括但不限于多分类问题，多标签问题，多任务问题。作为监督学习问题，这些问题同样可以使用主动学习来缓解标记压力： Multi-class active learning: 在分类任务中，每个样本的标签取值可以从多种取值里选取（不小于两种）。 Multi-label active learning: 在分类任务中，每个样本可以同时存在多个标签。 Multi-task active learning: 多个任务需要被同时处理，比如同时进行两个分类任务，或同时进行一个分类和一个回归任务。 Multi-domain active learning: 与多任务学习较为相似，多领域学习则是在不同的领域（数据分布/数据集）上学习相同的任务。 Multi-view/modal active learning: 多视角或多模态学习中的样本可能会有不同形式的表征。比如不同角度拍摄的同一个物品，来源于同一视频的音轨和视轨等。 针对这些问题，主动学习方法一般都会有特定的适配和修改，仅仅使用简单场景上的方法可能不会有较好的表现。 2.3 与其他 AI 领域结合的问题场景由于篇幅限制，主动学习与 AI 结合的领域和工作可以参见以下链接： AL Aids AI。 相较于之前提到的场景，AI 领域其实有不少更加复杂的问题。这些问题可能不能简单的用分类回归来表述，更多的时候是多种问题的结合。此类问题一般较为细致，定义也相对狭隘。针对这些具体问题，主动学习也有很多结合。 这些问题包括但不限于： Computer Vision (CV) Image Segmentation Object Detection Natural Language Processing (NLP) Sentence Classification Named Entity Recognition Domain adaptation/Transfer learning Metric learning/Pairwise comparison/Similarity learning One/Few/Zero-shot learning - Graph Processing etc. 3. 主动学习的技术角度的分类在上一节中我们以问题为导向介绍了绝大多数可以使用主动学习来解决的问题，可以说涵盖了大部分最重要的主动学习的相关工作。 这一节的介绍并不是以问题为导向，而是从技术角度对现有方法进行分类。 我们选用了两个角度： 设计原理角度：希望读者对不同种策略有一定的认识。 适用模型角度：希望读者可以直接找到适用于自己模型的策略。 3.1 从设计原理上分类这个角度是目前大部分 review/survey 的分类角度。 由于很多小问题中的相关文献较少，在我们的项目中，我们在对应的问题介绍下直接对相应方法作了简要分类。 本文中，我们的分类主要作用于最常见和最广泛的 Pool-based Classification 场景。 绝大多数著名的主动学习工作都是在此场景下进行的。 同样的，这里我们只简述设计原理，具体的文献列表参见我们项目中的这一章节。 在这里，我们把主动学习的设计原理（i.e.主动学习对样本的评估方法）分为五大类： 评估方法 简述 评论 Informativeness 一般来说指模型对选取样本取值的不确信程度 忽略了数据分布的影响 Representativeness-impart 考虑了选取样本是否可以对数据分布起到代表作用 通常来说和informativeness一起使用，此类方法和批选取方法可能会有重叠 Expected Improvements 考虑选取样本能为当前模型带来多少性能提升 此类评估通常较为耗时 Learn to score 不人为启发式地设计选取策略，而是学习一个选取策略 Others 有一些工作较难分类到上述的类别中 此处我们也列出这几类方法之下的小类： Informativeness Uncertainty-based sampling Disagreement-based sampling Expected Improvements Representativeness-impart sampling Cluster-based sampling Density-based sampling Alignment-based sampling Expected loss on unlabeled data Learn to Score Others 此外，批选取的方法（batch active learning）也有众多研究，其与 representativeness-impart sampling 有一定的交叉，具体可以参阅以下链接： Batch mode 3.2 从适用模型上分类很多种主动学习选取策略都标榜自己是全模型通用（model-free）的，但是实际使用中，他们可能仅仅适用于一个或一类模型（model-dependent）。 此处我们总结了一些 model-dependent 的策略在特定模型上的工作。 由于业界的模型实在是过于多样，此处我们仅归纳了几类常用模型，且工作并不一定很全，之后这个章节也会更新。 具体的文献列表请查阅我们项目中的这一章节。 这里我们仅列出一些我们总结到的模型： SVM/LR Bayesian/Probabilistic Models Gaussian Progress Decision Trees Neural Network 此外，如何设计适用于神经网络的主动学习策略和框架也是一个比较重要的问题。我们也对相关的深度主动学习进行了调研，详情请查阅以下链接： AL with Models 4. 将 AL 应用时的实际考虑我们之前讨论的是主动学习可以应用于什么问题场景，本节我们讨论的是实际应用主动学习的时候可能会遇到什么技术问题。 具体的内容列表请查阅我们项目中的这一章节： Practical Considerations 简要来说，大部分的主动学习工作对数据，标注专家，问题规模等都是有着一般性的假设。 比如“在一个几千样本上的平衡分类问题中，可以通过询问一个准确率百分之百的标注专家来获得标注数据”。 但是这些假设在实际运用中可能并不总能保真，所以本节我们纳入了很多对实际问题的考虑。 具体的考虑如下： Data (imbalanced, biased, feature missing, etc. Oracles (multiple labeler, diverse labeler, etc.) Scale (large-scale) Workflow (cold start problem, stop criteria, etc.) Model training cost Query/feedback types Performance metric Robustness More assumptions 可以说在这些问题上的研究拉近了主动学习到实际运用的距离。 5. AL 的实际应用主动学习已经是一个较为成熟的技术，已经有很多研究工作将其应用于不同学科和领域。 这里我们将其分为两部分：科学研究应用和工业界应用。 具体的文献列表请查阅以下链接： AL Applications 5.1 科学研究应用目前的工作主要聚焦于以下几个学科： Biology Materials Astronomy Chemistry Math and Statistics Geology Experiment Design / Experimental Condition Selection 5.2 工业界应用工业界应用相对较广泛，此处列举几个较为常用的领域，其他内容请参阅我们的项目。 Remote Sensing Medical Research Drug Discovery Labeling System Spam Detection etc. 6. 相关资料6.1 软件包在github上已经有不少基于Python的主动学习软件包： Name Languages Author Notes AL playground Python(scikit-learn, keras) Google Abandoned modAL Python(scikit-learn) Tivadar Danka Keep updating libact Python(scikit-learn) NTU(Hsuan-Tien Lin group) ALiPy Python(scikit-learn) NUAA(Shengjun Huang) Include MLAL pytorch_active_learning Python(pytorch) Robert Monarch Keep updating &amp; include active transfer learning DeepAL Python(scikit-learn, pytorch) Kuan-Hao Huang Keep updating &amp; deep neural networks BaaL Python(scikit-learn, pytorch) ElementAI Keep updating &amp; bayesian active learning lrtc Python(scikit-learn, tensorflow) IBM Text classification Small-text Python(scikit-learn, pytorch) Christopher Schröder Text classification DeepCore Python(scikit-learn, pytorch) Guo et al. In the coreset selection formulation PyRelationAL: A Library for Active Learning Research and Development Python(scikit-learn, pytorch) Scherer et al. DeepAL+ Python(scikit-learn, pytorch) Zhan An extension for DeepAL 6.2 Tutorials之前也有一些关于主动学习的研讨班内容： active-learning-workshop: KDD 2018 Hands-on Tutorial: Active learning and transfer learning at scale with R and Python Active Learning from Theory to Practice: ICML 2019 Tutorial. Overview of Active Learning for Deep Learning: Jacob Gildenblat. 6.3 领域内研究学者参见我们的项目，排名不分先后（有待补充）","link":"/machine-learning/al-all-in-one/"},{"title":"Awesome Active Learning","text":"This is the introduction for the Awesome-Active-Learning project. Hope you can find everything you need about active learning (AL) in this repository. 这篇博文同样有一份中文版本: 主动学习，看这一篇就够了。 This is not only a curated list, but also a well-structured library for active learning. The whole repository is constructed in a problem-orientated approach, which is easy for users to locate and track the problem. At the mean time, the techniques are discussed under the corresponding problem settings. Specifically, this repository includes: 1. What is Active Learning? 2. Reviews/Surveys/Benchmarks 3. Problem Settings 3.1. Basic Problem Settings (Three basic scenarios) 3.2. Advanced Problem Settings 3.3. Tasks in other AI Research Fields 4. Theoretical Support for Active Learning 5. Practical Considerations to Apply AL 6. Real-World Applications of AL 7. Resources 8. Groups/Scholars The hierarchical structure of this repository is shown in the following figure, and you can find the paper-list in the corresponding sub-pages: ShortcutsThese shortcuts could quickly lead you to the information you want. Link Note Taxonomy of Strategies The types of AL strategies, in general pool-based scenario. AL Aids AI Use AL under other AI research problems. AL Applications The scientific and industrial applications of AL. Practical Considerations The practical issues in applying AL when the assumptions change. Intrinsic Issues in AL The intrinsic issues of AL. Deep AL AL with deep neural networks. ContributingIf you find any valuable researches, please feel free to pull request or contact ruihe.cs@gmail.com to update this repository. Comments and suggestions are also very welcome! 1. What is AL?High labeling cost is common in machine learning community. Acquiring a heavy number of annotations hindering the application of machine learning methods. Active learning is one approach to relief this annotation burden. The intuition is that not all the instances are equally important to the desired task, so only labeling the more important instances might bring cost reduction. It is very hard to find a formal definition of general AL within a single optimization function. It would be better to define specific AL under specific problem settings. Hence, we only point out the essences of AL in this section. When we talk about active learning, we talk about: an approach to reduce the annotation cost in machine learning. the ways to select the most important instances for the corresponding tasks. (in most cases) an interactive labeling manner between algorithms and oracles. a machine learning setting where human experts could be involved. 2. Reviews/Surveys/BenchmarksThere have been several reviews/surveys/benchmarks for this topic. They provided a good overview for the field. Reviews/Surveys: Active learning: theory and applications [2001] Active Learning Literature Survey (Recommend to read)[2009] A survey on instance selection for active learning [2012] Active Learning: A Survey [2014] Active Learning Query Strategies for Classification, Regression, and Clustering: A Survey [2020][Journal of Computer Science and Technology] A Survey of Active Learning for Text Classification using Deep Neural Networks [2020] A Survey of Deep Active Learning [2020] Active Learning: Problem Settings and Recent Developments [2020] From Model-driven to Data-driven: A Survey on Active Deep Learning [2021] Understanding the Relationship between Interactions and Outcomes in Human-in-the-Loop Machine Learning [2021]: HIL, a wider framework. A Survey on Cost Types, Interaction Schemes, and Annotator Performance Models in Selection Algorithms for Active Learning in Classification [2021] A Comparative Survey of Deep Active Learning [2022] Benchmarks: A Comparative Survey: Benchmarking for Pool-based Active Learning [2021][IJCAI] A Framework and Benchmark for Deep Batch Active Learning for Regression [2022] 3. Problem SettingsIn this section, the specific problems which active learning is trying to solve are described. The previous works are organized in a problem-oriented order. The methods are categorized for the corresponding settings in the subpage. Three levels of problem settings: Basic Problem Settings Under the basic scenarios: Pool-based/Stream-based/Query synthesis Under the basic tasks: Classification/Regression Advanced Problem Settings Under many variants of machine learning problem settings Tasks from other Research Fields With more complex tasks from other research fields 3.1. Basic Problem Settings (Three basic scenarios)There are three basic types of scenarios, almost all the AL works are build on these scenarios. The scenarios are different in where the queried instances are from: pool-based: select from a pre-collected data pool stream-based: select from a steam of incoming data query synthesis: generate query instead of selecting data For the most basic AL researches, they usually study on two basic tasks: classification regression The details and the list of works could see here. 3.2. Advanced Problem SettingsThere are many variants of machine learning problem settings with more complex assumptions. Under these problem settings, AL could be further applied. Multi-class active learning: In a classification task, each instance has one label from multiple classes (more than 2). Multi-label active learning: In a classification task, each instance has multiple labels. Multi-task active learning: The model or set of models handles multiple different tasks simultaneously. For instance, handle two classification tasks at the same time, or one classification and one regression. Multi-domain active learning: Similar to multi-task, but the data are from different datasets(domains). The model or set of models handles multiple datasets simultaneously. Multi-view/modal active learning: The instances might have different views (different sets of features). The model or set of models handles different views simultaneously. Multi-instance active learning: The instances are organized into bags and training labels are assigned at the bag level. 3.3. Tasks in other AI Research FieldsIn many AI research fields, the tasks can’t be simply marked as classification or regression. They either acquire different types of outputs or assume a unusual learning process. So AL algorithms should be revised/developed for these problem settings. Here we summarized the works which use AL to reduce the cost of annotation in many other AI research fields. Computer Vision (CV) Natural Language Processing (NLP) Transfer learning/Domain adaptation Metric learning/Pairwise comparison/Similarity learning One/Few/Zero-shot learning Graph Processing etc. (The full list of fields could see here) 4. Theoretical Support for Active LearningThere have been many theoretical supports for AL. Most of them are focus on finding a performance guarantee or the weakness of AL selection. (This section has not finished yet.) 5. Practical Considerations to Apply ALMany researches of AL are built on very idealized experimental setting. When AL is used to real life scenarios, the practical situations usually do not perfectly match the assumptions in the experiments. These changes of assumptions lead issues which hinders the application of AL. In this section, the practical considerations are reviewed under different assumptions. The details and the list of works could see here. Assumption Type Practical Considerations Data Imbalanced data Cost-sensitive case Logged data Feature missing data Multiple Correct Outputs Unknown input classes Different data types Data with Perturbation Oracle The assumption change on single oracle (Noise/Special behaviors) Multiple/Diverse labeler (ability/price) Workflow Cold start Stop criteria Scale Large-scale Training cost Take into account the training cost Incrementally Train Query types Provide other feedbacks other than just labels Performance metric Other than the learning curves 6. Real-World Applications of ALWe have introduced that AL could be used in many other AI research fields. In addition, AL has already been used in many real-world applications. For some reasons, the implementations in many companies are confidential. But we can still find many applications from several published papers and websites. Basically, there are two types of applications: scientific applications &amp; industrial applications. We summarized a list of works here. 7. Resources7.1. Software Packages/Libraries Name Languages Author Notes AL playground Python(scikit-learn, keras) Google Abandoned modAL Python(scikit-learn) Tivadar Danka Keep updating libact Python(scikit-learn) NTU(Hsuan-Tien Lin group) ALiPy Python(scikit-learn) NUAA(Shengjun Huang) Include MLAL pytorch_active_learning Python(pytorch) Robert Monarch Keep updating &amp; include active transfer learning DeepAL Python(scikit-learn, pytorch) Kuan-Hao Huang Keep updating &amp; deep neural networks BaaL Python(scikit-learn, pytorch) ElementAI Keep updating &amp; bayesian active learning lrtc Python(scikit-learn, tensorflow) IBM Text classification Small-text Python(scikit-learn, pytorch) Christopher Schröder Text classification DeepCore Python(scikit-learn, pytorch) Guo et al. In the coreset selection formulation PyRelationAL: A Library for Active Learning Research and Development Python(scikit-learn, pytorch) Scherer et al. DeepAL+ Python(scikit-learn, pytorch) Zhan An extension for DeepAL 7.2. Tutorials Title Year Lecturer Occasion Notes Active learning and transfer learning at scale with R and Python 2018 - KDD Active Learning from Theory to Practice 2019 Robert Nowak &amp; Steve Hanneke ICML Overview of Active Learning for Deep Learning 2021 Jacob Gildenblat Personal Blog 8. Groups/ScholarsWe also list several scholars who are currently heavily contributing to this research direction. Hsuan-Tien Lin Shengjun Huang (NUAA) Dongrui Wu (Active Learning for Regression) Raymond Mooney Yuchen Guo Steve Hanneke Several young researchers who provides valuable insights for AL: Jamshid Sourati [University of Chicago]: Deep neural networks. Stefano Teso [University of Trento]: Interactive learning &amp; Human-in-the-loops. Xueyin Zhan [City University of Hong Kong]: Provide several invaluable comparative surveys.","link":"/machine-learning/awesome-active-learning/"},{"title":"原则读书笔记","text":"《原则》 Ray Dalio 极简简介桥水基金创始人 Ray Dalio 在本书中向我们介绍了其多年的人生经历，并由此引出他的主要观点——原则的建立和使用。 书中极其详细的分条陈述了 Ray 的生活原则和工作原则。 在展示原则之外，Ray 希望本书促使读者以最合适的方式发现改进反思自己的原则，这将有助于做出更好的决定，并让别人更好理解你。 个人看法极简评价这本书断断续续看了一个多月吧。 是冲着 Ray 的名气买的，读完之后确实受用。 受限于目前的阅历和社会分工，其实很多要点暂时还离我比较遥远，以后一定是会重读此书的。 个人觉得这本书最厉害的一点不是给我们展现 Ray 的厉害的原则，而是在对他的原则进行阐述和解释过程中，教导我们原则的重要性以及如何构建自己的原则，这是最有价值的地方。 这本书更像是一本人生说明书或者指导手册，以一本工具书的形式，提出了大道理，也提出了很多解决方案。 我认为处于社会不同位置的人都能从此书中学到不少东西。首先是作为一个“自然人”对自己的管理，要用原则规范提升自己。其次是作为一个“社会人”在工作岗位中作为管理者或者被管理者，如何基于原则高效的沟通和行动。 本书也在很多阐述的地方不经意的提起一些很常见的思维误区，有利于大家反思自己的思维方式。 这本书不仅是 Ray 的人生准则，其实也包含很多他相信的“道理”，需要批判看待。 自己的感想 原则构建 要清楚的知道自己的原则。 清楚原则还不够，需要“知行合一”。如果存在矛盾，则需要记录，并改进原则。 个人成长 使自己“可信”，首先要把事情做成功。 不固执己见，客观看待自己以及其他人，否则会一次次的栽在自己或其他人的弱点上。 历史在不停地重现，只是可能不重现在你的身上。性价比高的做法是从他人身上学习，这就需要头脑开放。 对待问题 对问题准确的定义十分重要。 不能容忍问题，否则前功尽弃。 在团队工作中，Ray 有一个非常好的诊断问题的流程值得学习。包括但不限于问题是哪来的，有多严重，谁该负责，如何改进，甚至包括对问题的诊断应该进行到什么程度。我觉得这一部分其实可以单独写一篇博客来学习一下。 团队管理 在团队管理中，很需要对成员开放式思考进行构建。 分清建议和批评的区别。 有趣的发现 历史真的是在不断重复。 例如1982年墨西哥违约，债务危机，美联储增加货币供给，美国大通胀。但是经济没有崩溃，通胀率下降同时经济增长加速。这是因为资金撤出借债国，回流美国，美元升值，通缩压力。所以美联储可以在不加剧通胀的情况下降息。（我指的是资金回流美国这一点重复，例如2020新冠疫情。） Ray对王岐山有着极高的评价，两人似乎很熟，有空可以八卦一下。 Ray 的原则导言 独立思考并决定：你想要什么？事实是什么？面对事实，你如何实现自己的愿望？请保持谦逊和心胸开阔，以便你能动用自己的最佳思维。 以可信度加权的方式做决定。 遵照原则做事。 以系统化的方式来做决策。 考察影响你的那些事物的规律，从而理解其背后的因果关系，并学习有效应对这些事务的原则。 生活原则此处仅记录一些相对较大的条目： 拥抱现实，应对现实： 做一个超级现实的人； 真相（对现实的准确理解）是任何良好结果的根本依据； 做到头脑极度开放、极度透明； 观察自然，学习现实规律； 进化是生命最大的成就和回报； 理解自然提供的现实教训； 痛苦+反思=进步； 考虑后续与再后续的结果； 接受结果； 从更高的层次俯视机器。 用五步流程实现你的人生愿望： 有明确的目标； 找出问题，并且不容忍问题； 诊断问题，找到问题的根源； 规划方案； 坚定的从头至尾执行方案； 谨记：如果你找到了解决方案，弱点是不重要的； 理解你和其他人的“意境地图”与谦逊性。 做到头脑极度开放： 理解你的两大障碍； 奉行头脑极度开放； 领会并感激：深思熟虑的意见分歧； 和可信的、愿意表达分歧的人一起审视你的观点； 识别你应当注意的头脑封闭和头脑开放的不同迹象； 理解你如何做到头脑极度开放。 理解人与人大不相同： 明白你与其他人的思维方式能带来的力量； 有意义的工作和有意义的人际关系不仅是我们做出的美好选择，而且是我们天生的生理需求； 理解大脑里的主要斗争，以及如何控制这些斗争； 认识自己和他人的特性； 无论你要实现什么目标，让合适的人各司其职以支持你的目标是成功的关键。 学习如何有效决策： 要认识到：影响好决策的最大威胁是有害的情绪；决策是一个先了解后决定的两步流程； 综合分析眼前的形势； 综合分析变化中的形势； 高效地综合考虑各个层次； 综合分析现实、理解如何行动的最好工具是逻辑、理性和常识； 根据预期价值计算做决策； 比较更多信息带来的价值和不做决定造成的成本，决定优先顺序； 简化； 使用原则； 对你的决策进行可信度加权； 把你的原则转换成算法，让计算机和你一起决策； 在深刻理解人工智能之前不要过度信赖它。 按节讨论/摘录以下是大篇幅的摘抄，取自文章中的三个大章节。 不能保证重要信息全记录。 摘抄的原因是看到这些话的时候有一星半点的赞同或感触。 此处的摘录比较糙，以后有空重读的话可以精炼消化一下。 1. 我的历程 不管我一生中取得了多大的成功，其主要原因都不是我知道多少事情，而是我知道在无知的情况下自己应该怎么做。 不幸的是，大多数的人并不能清楚地解释他们的原则。 尽管使用他人的原则不一定是一件坏事，但不假思索地采用他人的原则有可能将你置于风险之中。 “正确的失败”：痛苦的失败中获得教训。“错误的失败”：因为失败而被踢出局。 当政府决策者向你承诺他们不会允许货币贬值发生时，不要相信他们。 拥有有意义的工作和人际关系要比赚钱好得多。有意义的工作是一项我能全身心投入的使命；有意义的人际关系是指我既深深的关心对方，对方也深深的关心我。 把赚钱作为你的目标是没有意义的，因为金钱并没有固定的价值，金钱的价值来自他能买到的东西，但是金钱并不能买到一切。 要一个孩子是我迄今做过的最艰难的决定，因为当时的我显然不知道养孩子将是一种个什么样的经历，而且这个决定是不可撤回的。但事实证明这是我做出的最好的决定。 每当出现收益率曲线倒挂，对冲通胀的资产价格都会下跌，经济下滑。 以一国货币计价的债务，可以在该国政府的帮助下成功重组，而且在各国央行同时提供刺激时，通胀和通缩能够互相对冲。 我学到了一种很好的恐惧犯错的意识，这把我的思维定式从认为“我是对的”变成了问自己“我怎么知道我是对的”。 使人们相互对立的观点公开化，并对其进行分析，让我对人们的思考方式有了更多的了解。通常我们遵循我们的自然秉性做事时，考虑不到自身的弱点，走向失败，成功的人改变他们的做法，使他们能够继续利用优势，并弥补自身的不足，而不成功的人则不会这样做。 只有当你能承认甚至接受自身弱点是，你才能做出对自身有益的改变。 面对你都需要胆识看起来相互矛盾的东西时，耐心地做出选择。 “靠水晶球谋生的人注定要吃碎在地上的玻璃”：最重要的事不是预知未来，而是知道在每一个时间点上如何针对可获得的信息做出合理的回应。 确立一个“风险中性”的基准仓位，在进行慎重的冒险时偏离这个仓位。 我从来不愿仅仅因为投资产品会买的好就去设计他们，尤其是常规投资产品，我想要的只是进行市场交易，建立人际关系，设身处地地为我们的客户服务。 成熟意味着你可以放弃一些好的选择，从而追求更好的选择。 所有了不起的投资者和投资策略都是有弱点的，在弱点呈现时就对其失去信心是一种常见的错误，就像在其有效时对其过于迷恋一样。 拥有吧事情探究明白的能力，要比拥有某件事的具体知识更重要。 追求商业卓越和追求个人价值的实现，不一定是相互排斥的，而是可以相辅相成的。 一个管理者能够实现的最大成功就是能够阻止他人在没有你的情况下把事情做好。次好的情况是你自己能把事情做好。最糟糕的事连你自己都做不好。 我问他在火箭学方面有没有背景，他说没有。“我刚开始读这方面的书。”他说。这就是塑造者典型的思考和行为模式。 王岐山：“有能力的人居安思危。安然无忧的是愚人。假如冲突能在变得尖锐之前被解决的话，世界上就不会有英雄了。” 当我就权力制衡这个问题询问他时，他以尤里乌斯·凯撒推翻罗马元老院和共和国为例，说明确保任何人的权利都不能凌驾于制度之上是多么重要。 发现自己的性格，过与性格相适应的生活，才是最幸福的。 2. 人生原则章节 创造伟大事物的人不是空想者，而是彻底地扎根于现实。 不要固守你对事物“应该”是什么样的看法，这将使你无法了解真实的情况。 通过快速试错以适应现实是无价的。 你的未来取决于你的视角。你在一生中取得什么样的成就，取决于你如何看待事物，以及你关心什么人、什么东西（你的家庭、社区、国家、人类、整个生态系统）。 没有痛苦就没有收获。 大多数人在痛苦时不愿反思，而一旦痛苦消失他们的注意力就会转移，所以他们难以通过反思得到教益。 不管在生活中遇到什么情况，如果你能负起责任，进行良好的决策，而不是抱怨你无法控制的东西，你将更有可能成功并找到幸福。 对人来说，最难做的事情之一是客观地在自身所处环境（即机器）中看待自身，从而成为机器的设计者和管理者。 大部分人犯下的最大错误是不客观看待自己以及其他人，这导致他们一次次的栽在自己或其他人的弱点上。 在你不擅长的领域请教擅长的其他人，这是一个你无论如何都应该培养的出色技能，这将帮你建立安全护栏，避免自己做错事。 不要混淆你的愿望和事实。 不要为自身形象而担心，只需要关心能不能实现你的目标。 不要把不好的结果归咎于任何人，从自己身上找原因。 5个步骤：目标、问题、诊断、方案、践行。你要成功就必须做好每一步，而且按照顺序一步一步来。这个过程是层层递进的：认真做好每一步，你将获得必须的信息，以便进行到下一步并将其做好。 你只需要知道什么时候需要这些技能，你能从哪里学到这些技能。 尽管你几乎可以得到你想要的任何东西，但不可能得到你想要的所有东西。 不要混淆目标和欲望（不要拿欲望当目标）。 不要把成功的装饰误认为成功本身。 在逆境中，你的目标应该是守住自己的成绩，尽量减少损失，或者直面不可挽回的损失。 承认弱点并不是像弱点投降，而是克服弱点的第一步。 要精准的找到问题所在。如果问题的原因是某种固有的弱点，你也许需要寻求他人的帮助，或者改变你扮演的角色。 不要把问题的某个原因误认为问题本身。 容忍问题和找不到问题一样。只要你没有战胜问题的意志，你就处于毫无希望的境地。你必须养成一种对任何性质的恶习都绝不容忍的习惯，无论其是重是轻 把你的方案写下来，让所有人都能看到，并对照方案执行。 建立清晰的衡量评估标准来确保你在严格执行方案。理想的做法是让其他人客观评估并报告你的进度。 每个人都至少有一个最大的弱点阻碍其成功，找到你的这个弱点并处理它。 成功有两条路：（1）自己拥有成功所需的要素，（2）从其他人那里得到成功所需的要素。 要有效行事，你就绝不能允许“想要自己正确”的需求压倒“找出真相”的需求。 很自然，人们无法理解自己看不到的东西。 当你住处某个人的心里弱点时，对方的反应通常像你指出他的身体缺陷一样感到不舒服。 出现意见分歧的各方通常始终坚信自己是对的，而且往往以彼此发怒而告终。 亚里士多德把悲剧定义为：人的致命缺陷导致的可怕结果。在我看来，自我意识和思维盲点这两大障碍就是人的致命缺陷。 听听其他人的观点并加以考虑，绝不会削弱你独立思考、自主决策的自由，只会让你在决策时有更宽广的视角。 要做到头脑开放，你必须高度接受自己错了的可能性，可以鼓励其他人告诉你错在哪里。 谨记，你是在寻找最好的答案，而不是你自己能得出的最好的答案。最好的答案不一定是你想出来的。 我定义的“可信”的人有两个特征：曾反复的在相关领域成功找到答案（三次），在被问责的情况下能对自己的观点做出很好的解释。 当两个人的观点截然相反时，很有可能一个人是错的。搞明白是不是你错了，对你有好处。深思熟虑的意见分歧中，你的目标并不是让对方相信你是对的，而是弄明白谁是对的，并决定应该怎么做。 你应该考虑和思索各种互相冲突的可能性，也应根据了解到的情况，随时迅速地调整自己的想法，接受可能正确的东西。一种检验你做的好不好的方式是，把和你有分歧的人的观点，向对方复述一遍，瑞过他觉得你服输的对，就说明你做的很好。 很多人都会和你产生分歧，但你不应该考虑所有人的观点。跟任何人都头脑开放不一定有好处，你应该花时间和你能找到的最可信的人探讨观点。 头脑封闭的人 不喜欢看到自己的观点被挑战。他们通常会因无法说服对方而沮丧，而不是好奇对方为什么看法不同。 喜欢做陈述而不是提问 更关心是否被理解，而不是理解其他人 经常说“我可能错了，但这是我的观点”，暗示自己是开明的 阻挠他人发言 难以同时持有两种想法，使自己的观点独大 缺乏深刻的谦逊意识 头脑开放的人 更想了解为什么会出现分歧 真诚地相信自己可能是错的，提出真诚的问题 经常觉得有必要从对方的视角看待事物 知道何时做陈述，何时提问 喜欢倾听而不是发言，鼓励其他人表达观点 考虑其他人观点的同时保留自己深入思考的能力 时刻在心底担忧自己是错的 假如你是一个头脑封闭的人，又在自己有盲点的领域形成了一种观点，结果可能是致命的。 重视证据，并鼓励其他人也这么做。 知道什么时候应当停止为自己的观点辩护，信任自己的决策程序。 如果你不了解人（包括你自己）的特性就对他们抱有期待，你肯定会遇到麻烦。 当我的潜意识给我想法和提示时，我不是马上按照其行动，而是先用我的理性意识去分析他们。 你能做的最重要的决定之一是决定问谁。 不要听到什么信什么，要区分事实和观点。 所有东西都是放在眼前看更大，所以你应该跳出去以看到全局，有时候可以过一段时间再做决定。 “当你问一个东西对不对而对方告诉你并不完全对时，那它大致是对的。” 不要做完美主义者。完美主义者花太多时间关注边缘性的微小因素，影响对重大因素的考虑。 用“基线以上”和“基线以下”来确定谈话位于哪一层。当一段分析混乱，令人迷惑时，通常是因为谈话者限于基线以下的细节之中，而没有重新把细节与要点联系起来。 知道什么之后不要去押注，和知道什么注值得押同样重要。 先把你的“必做之事”做完，再做你的“想做之事”。 3. 工作原则章节 创意择优：1. 开诚布公的亮出你的观点。2. 针对分歧认真讨论。3. 遵循所形成的共识，消除过去的分歧。 为人要正直，也要求他人保持正直 若不想当面议论别人，背地里也不要说，要批评别人就当方面指出来。 大多数人做工作都希望出最少的里而赚尽可能多的钱。 学校里学习最好的学生可能往往是那些最不善于从错误里学习的人，因为他们已经习惯把做错题当成失败的代名词，而不是把犯错看成学习的机会。 不要纠结于一时的成败，要放眼于达成目标。不要纠结于“埋怨”还是“赞美”，而要专注于“准确”还是“不准确”。 没有人能客观的看待自己。 问题一般源自两个主要原因：简单的误解和根本上存在分歧。求取同步就是以开放而自信的心态修正双方立场的过程。 知道怎样求取共识和掌控分歧 把可能的分歧摆到桌面上 区别苍白的抱怨和有助于改进工作的诉求 要记住每个故事都有另一面 保持开放的心态，同时也要坚定果断 区别心态开放和封闭的人 远离心态封闭的人，不管他们知道多少，心态封闭的人都会浪费你的世界。如果你必须与他们打交道，要注意除非他们变得开明，否则无任何裨益。 提防那些羞于承认自己无所不知的人。他们可能更关注外在形象，而不是达成目标。 确保工作负责人以开放的心态对待问题和他人的意见。 意识到求取共识是双向的责任。通常沟通困难在于人们的思维方式不一样。 有一些简单的技巧会很有用，比如重复以便你刚听到的别人的观点，确保你理解正确。 假设你自己或者是没有沟通好，或是没听清，为不实先去责怪对方。要吸收自己沟通不理的教训，避免再犯。 实质重于形式：问题重于沟通方式。 自己要通情达理，也期待别人通情达理。 其建议、提问题与批评是不一样的，别混淆 提建议并未下结论说有错误，只是想确保考虑了风险，有没有疏忽（并不是真的忽略） 有人把建设性问题当成职责，反弹强烈，这是不对的。 如果由你主持会议，应把握好会话 明确主持人和会议的服务对象，如果没有明确的主持人，可能会陷入丧失方向和低效的境地。 表达清晰，以免造成困惑。 根据目标和优先次序决定采用什么样的沟通方式。 谨防“跑题”，列出议程让大家看到进展。 坚持对话的逻辑性。 不要因集体决策而丧失个人责任。对个人职责的分派要十分明确。 你必须让人用两分钟不受打扰地解释自己观点，再插话表达自己意见。 当心讲起话来不容置疑的“快嘴王”，你的责任是讲清楚事情，有一点没讲清楚都不要继续往下降。作为主持人可以说“抱歉我比较迟钝，希望你慢点说” 让对话慎始善终，进行总结，结束讨论。同时，人们不必对任何事情都意见一致。 运用沟通手段，分出优先次序。 3-5人的效率高于20人。边际效益递减。 珍惜志同道合者，既然有人在你最重要的方面价值观相同，也与你有实践价值观的相同做法，就要确保与这些人为伍。 如果你发现自己无法调和相互间的主要分歧—尤其是价值观层面的—要考虑是否值得维持这种关系。 最佳决策应该是创意择优中按观点的可信度高低得出来的。有可信度的观点来自多次成功解决了相关问题的人，能够有逻辑地解释结论背后因果关系的人。 如果你自己无法成功完成某件事，就不要想着知道别人如何完成。 关注可信度最高，与你观点不一致的人，尽量理解其推理过程。 若某人并无经验，但是所讲道理符合逻辑且可以经受压力测试，一定要试一试。 关注推理过程而非结论。 没经验的人也不乏好点子，不会限制在过去的套路。 考虑你要扮演老师、学生、同事中的哪个角色？说教、提问还是辩论？ 学生理解老师比老师理解学生更重要。 要了解人们提出意见的过程和逻辑 要仔细考虑向谁提问，问可信度强，准备好的人。如果自己可信度不强，不要分享自己的观点。 让每个人都可以肆意评论其他人的观点，此举低效且浪费时间。 梳理员工工作记录。 每个人都有权利和义务去设法了解重要的事情。 要关注决策机制是否公允，而非是否如你自己所愿。 相互达成协议时不能忽略原则。 不要吧发牢骚、提建议、公开辩论的权力与决策权混淆 不要对重大分歧不闻不问 别被小事烦扰 不要被分歧束缚：提交上级、或者投票 一旦做出决定，所有人必须服从，即便有不同意见。 在一家健康的机构中，应该是员工与低层次的自我进行竞争，而不是员工与员工互相竞争。 你最重要的决策是选好工作的责任人 最重要的责任人是在高层负责订立目标、规划成果和组织实施的人 负最终责任的人应是对行为后果承担责任的人。 确保每个人都有上级领导，要有人对他问责。 学习成绩不能充分证明这个人是否具备你想要的价值观和能力 评估常识、眼界、创造力、决事能力，学习成绩价值有限。 警惕不切实际的理想主义者 考虑薪酬时，要提供稳定性，也要让人看到机会 依人发薪，而不是依工作岗位发薪 想着把蛋糕做大，而不是给自己把蛋糕切大。 准确评价人，不做好好先生。 对人的观察不要讳莫如深。开诚布公的考察他们。 评估人时，你可能犯的两个最大错误是：对自己的评估过于自信，无法取得共识。 换岗是为了人尽其才，有利于整个团队。 不断把结果和你的目标进行对照。制定量化评价工具。 应对每个问题的手段都要服务于两种目的：让你与目标更为接近，能够对机器进行培训和测试。 如果出现问题，要在两个层面进行讨论：机器层面为什么，案例层面怎么办。 管理者必须确保自己负责的领域运转有效。他可以通过以下方式：把员工管理好；或者下沉去做本不该自己做但下属做不好的工作；把管理不好的领域提交给上级管理。 出售管理者的标志是他不必亲自做任何事。管理者应视自己陷入细枝末节为不良信号。 了解员工及其工作的动力，因为人是你最重要的资源。 明确职责。记住谁付什么责任，防止角色错位。如果问题出乎你的意料，可能是因为你远离了你的下属和工作流程或者你对下属和留存可能导致的不同后果缺乏足够认识。 让问责过程透明，而非私下问责。同时欢迎别人问着你，因为没有人能客观看待自己。 像公司的拥有者那样思考，要求你的同事也这样做。 不要担心你的员工是不是喜欢你，不要让他们告诉你如何做事，你要操心的是尽可能的做出最佳决策，因为不管你做什么很多人都会说你做得不对。 不要发号施令让别人服从你，要努力为人所理解，并理解他人，以达成共识。 当心那些混淆目标和任务的人，因为如果他们分不清楚就不能信任他们并给他们委派职责。 在无法充分完成时，这时将问题提交给上级解决，把问题提交给上级并不意味着失败而是一种责任。 在分析问题时要非常具体不要泛泛而谈，不要用我们他们这种不指名道姓的说法，掩盖个人责任。每个人都必须有一位有可信度的热情，高标准的人来监督。 每个人都必须有一位有可信度的，奉行高标准的人来监督。 不要仅盯着你自己的工作，还要关注如果你不在场工作会如何展开？ 如果某些职位不是全职的，且需要高度专业化的知识，我宁愿交给顾问或者外部人。 描绘一幅金字塔形的组织架构，图任何两条有塔顶向下连接塔底的线不应产生交叉。遇到跨部门或附属部门的问题是让金字塔交汇点上的人处理。 保持适当的监控让谎言没有可乘之机。 要不断思考如何产生以小搏大的杠杆效应，我一般在工作中运用50:1的杠杆，意思是说，每当我用一个小时与下属讨论工作，他们都要花大约50个小时的推进相关项目。 几乎做每件事所花费的时间和资金都比你预期的要多。 给人员分配任务是最好把各项任务记在检查清单上。完成的任务划掉，这可以做任务的提醒也可以做任务的确认，但是每个人不是只是完成在检查清单上的任务。 为了促进真正的行为改变，必须内化学习或养成习惯。 把原则阐述清楚，运用各种工具和行为准则来推进实施，形成信任公平的氛围，使任何结论都可以通过跟踪其背后的逻辑和数据来加以评估。 为了取得成功，所有机构都必须建立制衡机制。要确保公司里没有任何人比体系更强大，也没有任何人重要到不可替代。 在所有决策方法中，构建创意择优是最佳的方法。让创意择优发挥作用，人们需要做三件事：坦承自己最诚实的想法；让大家公开讨论理性的表达分歧，以便大家进行高质量的辩论，拓展思路，尽量形成最优的集体决策；用创意择优来处理所有不同意见。","link":"/reading-note/principles/"},{"title":"翟东升人民币国际化课程","text":"翟老师金融货币市场的课程。 第一讲：为什么关于人民币汇率的悲观预测错了这一讲的目的是破除一定对于汇率的谬误。 现象：2014-2016汇率破7。所以多数持牌企业用人民币海外投资，资金成本昂贵但是收益低。只有当5%速度贬值时才能获利。 看空人民币的5个错误理由： 国内已过增长期 房地产泡沫不可持续，资产价格高估，只能通过汇率贬值调整 地方政府债务大，泡沫破裂会导致汇率下跌 过早的去工业化问题（东南亚，一带一路），产业和资本的转移。 中国GDP为50%，但是m2是美国的150%以上。 错在哪里（实证数据）： 经济增速和汇率相关吗？ 顺周期货币和逆周期货币表现不同。外围地区顺周期，中心地区逆周期。 中国变为顺周期。 资产泡沫与汇率波动相关性？ 跨国资产价格并不具有一价定理（回归均值） 政府并不存在保汇率保房价问题 地方政府债务并没有那么高。（债务/GDP） 同时债务率和汇率之间负相关。只有有钱的人才能借到更多钱。 衡量债务风险的不是债务率，若以本币借债，本质上是一种隐形税收。 格林斯潘：“女士们先生们，大家不用瞎担心，这些债其实我们永远都不用还了。” （e.g.翟币） 资本输出汇率不一定贬值。 资本输出会让货币更加稳定，外部获取收益，导致长期强势。 m2与汇率的关系。 不公平的对比，评估美国要用m3（美国直接融资），中国m2（银行信贷）。 货币发行增速和汇率不相关。 资本大鳄的做法既不道德也不慎重。 为什么人民币快速升值: 中国产业升级 中国老龄化 划重点，汇率并不是由以下这些点决定： 经济增速 房价波动 债务率 对外投资、产业迁移 货币发行增速 第二讲：长期汇率由什么决定 短期汇率由市场情绪决定，不具有可预测性 中期汇率（3-5年）由政府调控影响 长期汇率（5-10）情况下，市场内在力量影响巨大 影响长期汇率的直接因素（实证数据验证）：一个国家可贸易品的价格水平直接影响汇率。 可贸易品价格水平由什么决定（两个表层的经济因素）： 技术水平（但只能解释20%左右的变化） 人口年龄结构，老龄化（能解释65%左右的变化） 越老龄化，汇率越强。供给不变，需求下降，价格下降。（e.g.日本） 影响汇率的六个深层因素（越高越强） 国家能力：公共部门能力与效率问题 开放度：积极融入国际贸易（出口） 能源大宗商品（以物为主）：明显顺周期特征 制造业为主（以人为主）：逆周期特征明显 要素特征 人的核心竞争力（脑力/体力） 人还是物 文明类型 坚挺的文明有两大类 新教文明 东亚文明：骨子里不信神（实用主义的神），努力证明自己的财富 软的文明： 南亚东南亚：小乘佛教，印度教，不鼓励生产也不鼓励消费（供给需求都萎缩） 天主教东正教伊斯兰教：不鼓励教徒寻求财富，鼓励寻求快乐（供给小于需求，则借债） 人群智商 宗教严肃度：询盘 海外投资的汇率风险哪些国家的汇率可能大规模贬值，详见深层因素。 第三讲：为什么中国被动成为最大外汇储备国巨额的外汇储备不是大国的标志而是附庸国的象征。 外汇储备源自对汇率波动的恐惧。 发展中国家汇率波动和受到冲击非常大。 中国的外汇储备是如何形成的为了快速工业化，招商引资，对人民币需求增多，有升值压力，但是又不能允许快速升值来保障制造业。 于是中国人民银行使用外汇占款买入外汇，再由国家外汇管理局买其他国家国债。 对冲流动性（4万亿美金外汇储备30万亿人民币外汇占款，货币乘数为4的话就是120万亿广义货币，此时为了避免通胀），这些策略都是有成本的： 发行央票 中央政府财政存款放在库底（利息低） 提高存款准备金率 央行拿外汇储备干什么去了买国债。中国外管局是美元的主要空头力量： 猜想：外汇来自资本顺差和贸易顺差，还是要控制风险。 外管局将一部分美元换成其他货币进行投资降低风险。 以中国外汇储备来预测美欧元汇率 外汇储备无助于汇率稳定外汇储备是追求汇率稳定的结果。 持有巨额外汇（代价大，且并不安全）： 巨量外汇占款，基础货币扩张，资产价格泡沫，央行对冲成本积累。 只能持有国债，低收益。高科技产品公司和资源无法买到。 并不能维护稳定例子：50亿美金做空巴西雷亚尔（5000亿美金储备） 要不然汇率暴跌 要不然储备暴跌，通货紧缩，经济萧条 本国居民逃离 面对做空正确的策略：有序地贬值。 中国不再需要外汇储备不需外汇，只需要一些黄金。 技术进步，产业升级，老龄化都可以减轻人民币汇率的贬值压力。 我们需要担心的是汇率的长期强势。 第四讲：政府意志与人民币汇率波动的历史探讨政府在货币定价中扮演的角色及作用的方式方法。 2005年前，汇率为出口导向型工业化提供支持此时需要本币相对低估。以1:1.5（1980）到1:8.7（1994）。 见到实体产业迁移消长变迁的时候，要对以后几年潜在金融危机汇率动荡债务危机警惕。 1994-2005年，汇率相对稳定。 亚洲金融危机时人民币拒绝贬值，导致工业化早期出现了出口困难。 （用自身的经济困难换区地区稳定。） 日本却因为主动贬值丧失了地区领导权。 汇率主动温和升值是好事吗？自主渐进可控的人民币升值背景： 美国认为人民币低估，要求人民币升值。 人民银行如果要保持汇率，有通胀压力，要扩充外汇占款。 出口制造部门反对汇率升值。 导致许多人搞套息交易。 从香港搞廉价美元贷款，付给深圳公司，换成人民币，然后利差息差获利。 巨量热钱流入中国。 邀请全世界人来赌人民币升值，形成一定的泡沫，上证指数1000点到6000点。 长期人为的扭曲付出了沉重代价。 后人视角给当时的决策者提建议： 汇率波动走势具有随机性。 但是使月周级别不可预测。 减少热钱流动。 后金融危机时代，人民币汇率何去何从2008之后渐进升值，但是大大减缓。 2014人民币被低估的程度不高了。 2014-2016又贬值了4%左右（时间换空间，防止被做空）。 此时需要把热钱留出去。 外商持续投资减少，撤离中国（超国民待遇消失）。 2016不要对人民币贬值恐慌，认识到中国可贸易部门竞争力提升。 2017人民币由市场升值 恐慌性升值，逼空 2018贸易战，贬值预期 没有安抚市场，任由市场波动。 对美出口下降，但是人民币贬值，总出口稳定。 2020-2021汇率明显上涨 新冠疫情，出口率先恢复。 CFETS汇率指数，与一篮子货币进行稳定化，与美国经济脱钩（换锚）。 汇率还是要基于经济基本面。 划重点 东亚金融危机，中国赢得周边国家信任，为周边外交开拓了市场空间，日本丧失地区领导权。 汇率的渐进升值，把市场参与者的风险转移到了政府手里，市场调控应该注意。 政府塑造的中短期汇率及其波动方式的能力很强，但是长期人为扭曲将会付出沉重代价。 高明的汇率政策既要照顾实体经济的需要，又要充分考虑金融市场的内在规律。 第五讲：铸币税及其国际国内再分配效应铸币税的前世今生如今货币创造成本越来越低。 中国综合税率占GDP比例较低，但是如果把铸币税和土地财政纳入考量，则综合税率不再低。 勤俭节约的美德“过时”了无锚货币时代，不应再痴迷储蓄： 储蓄越多，向锚货币交的税越多。 2012年前，货币增速快，钱的购买力在不断降低。 无锚货币时代，用好杠杆做好投资。 中国人民存在海外的储蓄在不断地蒸发。 越难复制的资产，升值保值能力极高。学区房，高年份茅台，艺术品等的增值快于货币增发速度。 外汇占款带来的货币扩张帮助了谁？剥夺了哪些阶层？补贴了哪些阶层？出口部门获利，剥夺内地居民，补贴东南沿海。 中央政府通过税收反哺内地是极其必要的。 中国央行从中国人民手中收取铸币税之后转手送给了美欧日央行，以此换取中国出口制造业的发展空间。 为了扶持东南沿海企业家，才会有货币超发，才会有天量外汇储备。 离开了政府提供的优质公共产品，这些企业家的表现不一定好。 2020年出口繁忙，人民币升值，出口企业不挣钱因为汇率变动。 离开了政府保姆般的呵护，很多企业没本事挣到那么多钱。 很多先富起来的人将政府看成他们聚敛财富的负面因素，于是： 2015看空人民币 投资移民走了 外资企业也是明显的获益者群体：以折扣价获得土地。 利用制造业服务业投资，通过土地获得资产升值的红利。 中国底层消费力的重要变化外汇储备稳中有降，征收广义铸币税规模大大下降。 金字塔底层的群体有了一定的消费能力。 劫富济贫：美国梦的真相美国从全球征收了多少铸币税？ 资产负债表增量/GDP：较小 贸易逆差-海外投资金收益：通过财政收入产生数倍GDP 美联储扩表，剥夺全球储蓄者。 0.9（2008前）=》2=》4.5（2014）=》缩表（2017初）=》扩表（2019.10）=》4.2（2020.2）=》无限量化宽松=》7.5（2020末）=》15（2025？推测） 铸币税补贴了谁？ 小部分补贴了美国的穷人。 主要补贴了，美国的资本所有者，股市大涨。 所以要推动人民币国际化，征收全球铸币税，吸引全球储蓄者放进我们国债池。 划重点 2012年前，为了支持本国制造业国际竞争，用土地财政和铸币税补贴了出口产业，造成了不同资产收益率的巨大差异，也造成了不同地区发展水平和不同阶层收入水平的巨大差异，形成了强大的国内再分配效应。 美国的无限制量化宽松，伤害了美国底层和全球外围国家，但是令他最富有的人大大获益。 中国要通过人民币国际化获取一定的全球铸币税，保护自己的财富。 第六讲 人民币国际化的相关问题不平衡的全球货币格局美欧 GDP 占比42%左右，但是其货币却占83%。 中日韩 GDP 占比27%左右，但是其货币却只占8%。 人民币国际化的进展2009年，周小川呼全球改革货币体系。 人民币国际化提上日程。 人民币国际化指数在2010年左右只有0.02%，2020年已在4%左右。 在贸易上和投资上增长比较快，储备较低。 人民币面临的问题是国债池子太小。 随着中国高端制造业发展，人民币计价份额也会像日元一样上升。 人民币国际化的改革努力放松人民币跨境使用的管制。 政府的主动动作。 开放性大宗商品交易所，意味着全世界的资源出口国可以不必绕道美元进行交易。 2015年前，人民币国际化路径和日元相似，提升本币在贸易结算份额，通过香港离岸市场发展日元海外市场。 2015年，香港市场人民币波动形成了人民币做空预期，中国政府不得不拉高隔夜拆借成本，抑制投机浪潮，意识到日本模式缺陷。 2015年后多管齐下，步步为营： 人民币大宗商品交易 人民币对外直接投资 国债市场对外开放 上海金融中心建设 金融业外资准入放开 一带一路倡议 中资公司的跨国发展 2020，稳慎推进人民币国际化。 未来： 中央层面协调不同部门法规，让企业家乐于便于使用人民币 人民币上印全球通用语言 超大面额钞票，便于金融不发达地区 抛弃宏观债务率迷信，统一中国国债市场，增加交易活跃度和便利性 发行特别国债来置换人口流出区域地方政府的高息债务，节省利息成本 统计局统计公布数据时以人民币为单位，同时鼓励其他经济体使用人民币作为备用单位 美国对人民币国际化的态度2010年之后最初几年，中国小心翼翼，“推动人民币跨境使用”。 搞经济金融事务的人看好人民币国际化 政策和战略研究的人不看好 美国政策界乐于见到搞人民币国际化，但是认为人民币无法俘获信任， 主权信用无法相信 必须放开货币汇率管制，实体经济可能受损 数字货币带来人民币国际化弯道超车的机遇中国推出数字人民币： 在swift系统下，美国的制裁会造成挤兑破产。 于是中国需要解决卡脖子问题。 猜想： 如果能顺利推广到境外，可以农村包围城市，代替一部分美元现钞。 清晰坦率解释，取代地下流通的美元，提供资源帮助国家建设货币系统，实现货币现代化。 一带一路中，将数字人民币使用和援助相挂钩。 铸币税一部分好处恰当的分享給发展中国家人民 人民币国际化的前景及其影响 货币网络效应对人民币国际化影响 中心化网络导致强大货币存在兑换优势 人民币国际化需要先挤兑其他小币种的份额 人民币国际化对中国经济的影响 汇率水平上升，国际购买力翻倍 经济规模增大，本土消费市场会成为第一大市场 人民币国际化代价 汇率水平敏感的行业消亡 划重点 人民币国际化未来需要多管齐下，稳慎推进，久久为功。 全球货币市场存在赢家通吃的网络效应，人民币国际化在本世纪中期才会轮到挑战美元份额。 数字人民币将帮助人民币国际化弯道超车。 人民币国际化会造成产业转移，这也是我国劳动力结构变化的内在要求，应该顺应这个市场规律而行。 实操课：全球货币体系知识如何转化为利润人民币汇率的预判及其分析方法长期： 中国快速迭代产业进步+老龄化趋势=坚定看多人民币 中期（3-5年）： 中央政府政策导向以及与美国的关系。 短期（1年）： 关注中国新生儿数量，数量下跌的话大胆做多人民币。 如果政府鼓励生孩子且取得实际效果，可以适当看空一点人民币。 超短期（1-2月）： 关注美元指数波动。 美元在两种情况下会走强两种情况： 当美国率先与欧洲和东亚出现经济复苏和走出困境中。 当全球经济陷入大麻烦时。流动性躲到短期美元国债。 如果美国出现政变，或者遭受核武器，或者资产泡沫破裂，导致股市暴跌，资产价格失序，那这个时候美元价格会涨还是会跌？ 其实是会大幅上涨。事情越大涨得越凶。 次贷危机和新冠疫情为例。 半球模型与全球资产配置的大周期理论半球模型（形似健身房物件）可以用这个模型近似全球资本主义货币体系的构造。 当一只脚踩下去，气体会像边缘扩散。 当中心出现减息，资金便宜了，资金向外围溢出，资本主义外围地区出现局部和暂时的繁荣。 反过来的话，繁荣消失，金融危机。 这只脚的踩踏和提起形成美元价格相对一篮子货币的波动（美元指数）。 过去50年，美元指数有着明确的周期，平均16-17年一次，下跌十年反弹六年。 美元下行周期，可以从中心借入便宜资金追逐资本主义外围的高风险资产。 美元上行周期，做空外围，把从外围挣到的钱连本带息还回中心，利润躲进无风险资产，美元短期国债。 三位一体风险定价体系下列图中纵轴是资金的价格收益率risk 横轴是时间或者说债券的久期。连成一条线是美国国债收益率曲线。 横轴的单位从时间久期切换为地点，这个世界的中心外围体系，越中心资金价格越便宜。 横轴切换为资产类别。房地产风险最大收益率最大。 长周期资产配置蕴含较高收益率的模型美元处于下行周期，把资金投入全球外围地区，在这一轮美元下行过程中最受益的工业化的经济体。 指标上看哪一个国家的国际收支平衡表上堆积越来越多美元储备（热钱往哪里流）。 所需做的去王公贵族居住的最最贵的房产和土地，然后不要动，操作周期在10年以上。 当美元指数上涨的时候，所需做的是把当地房产卖掉，获得当地货币，卖掉换回美元，换回美国短期国债，只追求活下去。 回溯过去50年的美元周期1970年中期跑到拉美，在布宜诺斯艾利斯、里约热内卢、墨西哥城，买入最贵的房地产。 1980年代初，卖出当地房产和货币，回归短期国债。 1980年代中，指数再次下行，做右肩交易（趋势确定），在东亚日韩港台新加坡买入最贵的房产。 1990年代中，美元上涨，卖出当地房产和货币，回归短期国债。 2001到03年开始，又大幅下行，在中国投资，北京四合院，上海小洋楼。 2015年美元指数上涨，周而复始。 那些外围国家的房地产值得投资问题切换成，那些外围国家在美元下行周期能成为工业化浪潮的赢家。 最理想的候选国家，应该具有以下条件： 强政府，最好不要玩选举。民主制度是奢侈品，只有实现工业化才可以享受，否则劣质民主或者民主倒退。 外交采用务实的外交政策，与各个大国搞好关系（同时中美）。 经济上重商主义，鼓励出口，让自己家制造业越办越好。 人口劳动力素质比较好且有一定规模，人均智商偏高。 文化上最好不怎么信教，追求世俗的财富和成就。 地理上最好有一些值得开发的港口，因为海运成本较低，腐败与否不重要。 如果找到一两个发展中国家符合一两个或者大部分条件，需要长期关注其政治变革和政策组合。 这种国家一旦进入工业化轨道，办工厂的人未必能积攒巨大财富。 这种工业化积攒的财富最终以各种形式流入当地统治阶级和王公贵族。 导致统治阶级聚集的大城市资产价格大幅上涨。 如何识别泡沫反身性索罗斯的操盘策略很多时候是在美元指数上涨周期做空外围泡沫比较严重的经济体。 索罗斯是用主观认知和客观事实之间的循环联动性，寻找自我实现的预言，自我增强的趋势，利用反身性来识别泡沫。 实体经济和虚拟经济之间的背离问题两个案例，注意关注类似情形，警惕危机： 东南亚制造业在1990年遭到中国出口工业的竞争而被掏空，而房价股价持续走高。而汇率却铆住美元，结果产生金融泡沫。 21世纪头十年，中东欧地区经济体融入欧盟统一市场，抢夺原本属于南欧实体产业的市场空间，南欧货币融入欧元无法通过汇率调整反应经济基本面变迁，出现泡沫，欧债危机。 为什么很多发展中国家喜欢搞固定汇率和联系汇率制度？ 出口制造行业低端，无法经受汇率波动，于是铆定主要的贸易对象。 注意未来也会有国家与人民币挂钩。 从发达工业国还是发展中穷国，哪个挣钱容易？两种国家： 发达国家钱多富裕但是人精明。中国人挣辛苦钱。 穷国穷，没什么钱，人单纯，知识匮乏，政策漏洞多，定价错误多。 在发展中国家布局相对更容易。 但是道德上不建议这样做。 两种策略有一些发展中国家有大企业，具有国际性业务。 例如能源、采矿、服务业制造业。 他们有公允价值。 在他们本币大幅贬值时，这些企业股票价格或迟或早等比例上涨。 如果判断出本币贬值过程，及时进场加杠杆做多这些大企业。 本币贬值会带来某些无风险收益。 划重点 实操课答疑八个共性问题： 美元指数，17年左右这个周期，鲁棒性怎么样，可靠性怎么样，是不是可信？ 大家不要迷信数字，仅三个周期不足以支撑。 美元强势周期一个比一个弱，可能是和美国优势在弱化有关。美国 GDP 占全球GDP比例降低。 一带一路一定程度扰乱了美元周期，中国输出万亿美元级别到外围地区。 如果形成中美平行格局，就要同时考虑美国和中国的周期。 2021年夏季，美元处于自身周期的什么阶段，到底是上行周期还是下行周期？ 现在处于下行周期的早期阶段。 无论美联储什么表态，美联储都无法大幅加息缩表，美国联邦政府受不了。 美联储主席追求连任时，政府影响很大。 2012年后如果卖出中国的房产是不是有点亏？比如深圳房子一直在涨是不是错过了许多机会？他的收益率和美国国债短期收益率相比太大了。 不完全是美元指数的涨跌周期，滞后性。 美元指数明显趋势性上涨之后才离开发展中地区。 这样看所以应该2015年左右脱手房产（虽然深圳继续涨，但是这个和美元周期的收益关系不是太大）。 许多国家房产交易比较麻烦，还要委托出租。有没有别的交易标的替代房子？ 古董和艺术品，尤其是有历史的国家，顶级艺术品，但是需要防伪。 猜想：伊朗可能苦尽甘来，目前各类资产便宜，有可能融入中俄体系（波斯文明的复兴，买回曾经荣耀）。 按强政府人口智商工业化等方面来看，哪些国家值得投资： 2011年开始鼓吹投资越南，房产上涨，汇率没有下降，但是目前行情是下半场。 柬埔寨（强政府有待考虑）孟加拉国（国际收支平衡表看来不错）。 缅甸要看他的政治安全前景（比如埃塞俄比亚提前享受了昂贵的民主制度。） 推荐投资领域的书： 《股票作手回忆录》：杰西·利维摩尔，最后左轮自杀。 《富可敌国》《More money than god》。 国际收支平衡吧及其他重要宏观国别数据如何获得？ 付费：路透社、彭博、万德 DataBank、OECD、UN 以目前的身价实力，和目前中国资本项目管制的政策有点远。能不能教一些炒股，或者在中国国内市场中间可行的现实的资产配置的招？ 无所谓心态 练看盘，所有股票，量价组合，大涨之前，涨的过程中，大跌前。 平时不要浪费时间看，该做什么做什么。 大盘暴跌的时候再看股票，看哪些恐慌杀跌。 2016年初：小天鹅，账上钱多，基本面好，满仓。 识别庄股。 好股票：有好事，市场没有充分认识到，没有体现在价格因素。或者现在大家过度悲观。","link":"/economy-finance/didongsheng/rmb-courses/"},{"title":"浪潮之巅","text":"一本介绍大型科技公司浪潮起落的书。 作者吴军 内容简介个人看法极简评价一本了解世界互联网产业发展的必读书籍。 以浪潮反应产业的起伏，对于个人择业或者行业研究有着一定的参考价值。 感想看法 这本书还是有些过时，很多内容需要更新。比如微软和苹果公司的关系。 竟然还写了红杉资本的联系方式，有趣。 由于Google 很明智地没有在互联网泡沫高峰期疯狂地扩展，而是实实在在地、低调地做好自己的搜索引擎，因此它早期烧钱的速度非常之慢。2000 年的时候它没有急着上市，避免了绝大多数互联网公司大起大落并且最终关门的厄运，同时最早期的优秀人才没有拿了钱就走掉，因此Google 的骨干完好无损。感觉这个可以用来研究没来得及上市的蚂蚁金服的发展上。 制定规则的重要性一次次体现。 让人思考商业模式和技术的权衡，或许商业模式更加重要。 一个巨大的公司拓展新生代业务看起来还是比较难的。 与大学的合作研发看起来性价比极高，远高于自建独立实验室。 对于商业来说，技术并不是最重要的。 “存在即合理”，的确能不断的找到这句话的印证。 创业公司的员工或许都能以一当十？ 其中有很多华为公司的部分，当时华为并不像现在一样人尽皆知，但是业内已经是名声响亮了。华为的成功有其必然性，其创办之初就剑指最先进的技术而不是二流技术。 流量并不是一切，很多中国的公司证明了这一点，无数次的烧钱没有转化为实际收入，都只是抱薪救火。所以说商业模式，预期产值或许更重要。 公司的掌舵人决策者实在是太为重要了。 技术并不是无敌的。 身体真的是创业者最大的本钱。今日孙剑博士去世，不由得重新感慨这一点。R.I.P. 业绩下滑时，裁员是很明智的决策。 革命是创业的重要部分，而不紧紧是“进化”。风投，或者说创业者（创业其实是在风投自己），要能准确的评估技术及其前景。 当公司已经足够大的时候，其市场占有率（增长前景）就难以翻番了。 斯坦福的校友组织的确比较有趣。 投资人对于公司的影响极大，公司总是要对股东负责。 2B 产品受经济大环境的影响太大，收入相对不如 2C 产品来得稳定。这个可能在经济周期中有一定的指导意义。 资金利用率&amp;自动化程度对企业的经济效益影响极大。 近期吹嘘了很多 web3.0 的概念，个人并不太知道这是什么。目测是一种元宇宙+数字货币+vr的一种缝合怪结合体？我们回顾 web2.0（打造通用平台，再由社会上的开发力量和广大用户补充内容），web3.0 的口号目前仍是以技术为导向还不是以问题和需求为导向，似乎就有一点像 web2.0 真正意义出现之前的那一波噱头了。那到底什么是 web3.0 呢？ 个人一向推崇经济和金融的意义，因为他们对于实体经济起到了不可替代的资源调度作用。虽然还有很多逐利的资金追求泡沫和杀猪盘，但是仍然不能因此忽略它的绝大多数的积极意义。 当总有什么新词新指标来为新东西背书的时候，可能要留心这是不是一个泡沫了。 危机之下，求生或许是所有公司的本能，成功断臂求生可能才有未来的发展。 很多看似无关的技术都或许会因为一个产业结合在一起，整体还是要看产业对社会的效率有多大的提升。 有一些东西对用户而言的沉没成本极高，即使有更好用的新东西，也不会轻易转换。 有意思的是，作者指出“阿里巴巴只要它不犯大的错误，现在找不出一家公司可以挑战它的商业地位”。然后我们都知道，他犯了大的错误。然而这个错误真的是错误吗？见仁见智，只能说他过于自大却忘记了国内的监管环境，所以才有了后来的制裁。 “因果倒置”这种事情有时候还真的不一定能分辨出。所以马云的电子商务是因还是果呢？或许互为因果，但是很多时候要追究事物发生的原因。 不靠泡沫挣钱的时候，经济体才能繁荣发展。 内容摘录ATT ATT 不紧不慢地向上走过了百年，才爬到顶点，走下坡路却只要十年时间。（注：今天的 ATT 是由当年小贝尔公司之一的西南贝尔公司几次以小吃大合并出的类似于水电公司的设施服务公司，这类公司在美国统统称为 utility 公司，毫无技术可言。） 在很长的时间里，美国国际长途电话的价钱不是由市场决定的，而是由 ATT 和美国联邦通信委员会（FCC）谈判决定的 ATT 的总裁们并不真正拥有公司。他们之中不乏有远见者，但是根本左右不了董事会。 一般来讲，一个公司当前的股价，已经反映了当前和几年后的盈利能力。如果想让股价快速增长，公司的盈利就必须高于大家的预期 在工业史上，一种新技术代替旧的技术是不以人的意志为转移的。人生最幸运之事就是发现和顺应这个潮流。 IBM IBM 公司可能是世界上为数不多的成功地逃过历次经济危机，并且在历次技术革命中成功转型的公司。 IBM 能成为科技界的常青树，要归功于它的二字秘诀——保守。毫无疑问，保守使得 IBM 失去了无数发展机会，但是也让它能专注于最重要的事，并因此而立于不败之地。 我们可以将第二次世界大战作为机械时代和电子时代的分水岭。 社会的需求对科技进步的作用要超过十所大学。计算机就是在这个背景下被发明的。美国研制计算机的直接目的是在第二次世界大战时为军方计算弹道的轨迹。在流体力学中，计算量常常大到手工的计算尺无法计算的地步，因此，对通用计算机的需求就产生了。 在我个人看来，小华生对世界最大的贡献不是将 IBM 变成一个非常成功的公司，而是将计算机从政府部门和军方推广到民间，将它的功能由科学计算变成商用。 IBM PC 第一年的营业额大约是两亿美元，只相当于 IBM 当时营业额的 1% 左右，而利润还不如谈下一个大合同。要知道，卖掉十万台 PC 可比谈一个大型机合同费劲儿多了。因此，IBM 不可能把 PC 事业上升到公司的战略高度来考虑。 因为 IBM PC 的主要部件，如处理器芯片、磁盘驱动器、显示器和键盘等等，或者本身是第三家公司提供的、或者很容易制造，而它的操作系统 DOS 又是微软的。因此，IBM PC 很容易仿制。IBM PC 唯一一个操作系统的内核 BIOS 是自己的，但是很容易地就被破解了。在短短地几年间， IBM PC 的兼容机入雨后春笋般地冒了出来。 盖茨可不是一般的人，他的心志非常高远，他不会允许别人动微机软件这块大蛋糕，虽然此时的微软的规模远没法和 IBM 相比。盖茨明修栈道，暗渡陈仓，一方面和 IBM 合作开发 OS/2 ，挣了一点短期的钱，另一方面下大力气开发视窗操作系统（Windows） 到八十年代末，由于微机性能每十八个月就翻一番，微机慢慢开始胜任以前一些必须要大型机才能做的工作。这样，微机开始危及到大型机的市场。 IBM 出现了严重的亏损，有史以来第一次开始大规模裁员。 1993 年，从未在 IBM 工作过的郭士纳受命危难，出任 IBM 的首席执行官。他成功地完成了 IBM 从一个计算机硬件制造公司到一个以服务和软件为核心的服务性公司的转变，复兴了这个百年老店，并开创了 IBM 的十年持续发展。他的第一招用他自己的话讲是将 IBM 溶解掉，通俗地讲，就是开源节流。他首先裁掉了一些冗余的部门和一些毫无前途的项目。 对比几乎同时代 ATT 将公司拆分的做法，郭士纳完全是反其道而行之。他的目的是打造一只 IT 服务业的航空母舰。在公司内部，它引入竞争机制，一个项目可能有多个组背靠背地开发。为了防止互相拆台、加强合作，郭士纳将每个人的退休金和全公司的、而不是以前的各部门的效益挂钩。 郭士纳砍掉了一些偏重于理论而没有效益的研究，并且将研究和开发结合起来。一旦一个研究项目可以实用了，他就将整个研究组从实验室挪到产品部门。到后期，他甚至要求 IBM 的所有的研究员必须从产品项目中挣一定的工资。这种做法无疑很快地将研究转化成产品。但是这样做无疑会影响 IBM 长线研究和基础研究，为了弥补这方面的损失，IBM 加强了和大学的合作，在几十所大学开展了科研合作或者是设立了奖学金。 今天，IBM 成为了世界上最大的开源操作系统 Linux 服务器的生产商。 IBM 就是这样，时不时地调整内部结构，将一些非核心的、长期效益不好的部门卖掉，同时扩大核心的利润高的生意。 今天，它仍然是世界上人数最多、营业额和利润最高的技术公司。在可以预见的未来，它会随着科技发展的浪潮顺顺当当地发展，直到下一次大的技术革命。 Apple 麦金托什是世界上第一种可以买得到的、拥有交互式图形界面并且使用鼠标的个人电脑。它的硬件部分性能略优于同期的 IBM PC 机，而它的操作系统领先当时 IBM-PC 的操作系统 DOS 整整一代。 苹果的股票九十年代开始是上升的，这就是电影中的阿甘觉得持有了苹果的股票就不用为钱发愁的原因。如果那个电影晚拍几年，导演就不得不找另一家公司的股票给阿甘了。 我和硅谷很多创业者聊过，发现他们对自己的公司，哪怕再小的公司，在感情上也象对自己的孩子一样亲。 他已经认识到了苹果封闭式的软硬件，从成本上讲，无法和微软加兼容机竞争，也无法为用户提供丰富的应用软件。乔布斯做了两件事，他在苹果的微机中逐渐采用了英特尔的通用处理器，同时采用 Free BSD 作新的苹果操作系统的内核。这样相对开放的体系使得全社会大量有兴趣的开源工程师很容易地为苹果开发软件。 乔布斯看到了两点最重要的事实，第一，虽然已经有了不少播放器，但是做的都不好，尤其是当音乐数量多了以后，查找和管理都很难。要知道，从一千首歌里面顺序找到自己想听的可能要花几分钟时间。另外，要把自己以前买的几十张 CD 上的歌倒到播放器上更是麻烦；第二，广大用户已经习惯戴着耳机从播放器中听歌而不是随身带着便携的 CD 唱机和几十张光盘。因此，它不需要花钱和时间培养出一个市场。基于这两点的考虑，乔布斯决定开发被称为 iPod 的音乐和录像播放器。 虽然它六百美元的价格实在贵了点，但是根据电器十八个月降一半价钱的规律，iPhone 很有可能成为今后普及的手机，成为苹果即 iPod 以后新的成长点，它甚至会冲击传统的手机行业。 计算机工业的生态链 现在，每十八个月，计算机等 IT 产品的性能会翻一番；或者说相同性能的计算机等 IT 产品，每十八个月价钱会降一半。虽然，这个发展速度令人难以置信，但几十年来 IT 行业的发展始终遵循着摩尔定理预测的速度。 个人电脑工业整个的生态链是这样的：以微软为首的软件开发商吃掉硬件提升带来的全部好处，迫使用户更新机器让惠普和戴尔等公司收益，而这些整机生产厂再向英特尔这样的半导体厂订货购买新的芯片、同时向 Seagate 等外设厂购买新的外设。在这中间，各家的利润先后得到相应的提升，股票也随着增长。各个硬件半导体和外设公司再将利润投入研发，按照摩尔定理制定的速度，提升硬件性能，为微软下一步更新软件、吃掉硬件性能做准备。华尔街的投资者都知道，如果微软的开发速度比预期的慢，软件的业绩不好，那么就一定不能买英特尔等公司的股票了。 反摩尔定理使得 IT 行业不可能像石油工业或者是飞机制造业那样只追求量变，而必须不断寻找革命性的创造发明。因为任何一个技术发展赶不上摩尔定理要求的公司，用不了几年就会被淘汰。 有些愿意冒风险而追求高回报的投资家将钱凑在一起，交给既懂得理财又懂得技术的专业人士打理，投给有希望的公司和个人，这就渐渐形成了美国的风险投资机制。办好一个高科技公司还需要有既志同道合又愿意承担风险的专业人才，他们对部分拥有一个公司比相对高的工资更感兴趣，因此就有了高科技公司员工的期权制度。 奔腾的芯：英特尔－Intel 1981 年，IBM 为了短平快地搞出 PC，也懒得自己设计处理器，拿来英特尔的 8086 就直接用上了。这一下子，英特尔一举成名。 美国历史频道（History Channel）在节目中评论了中日甲午战争。美国的历史学家认为，这是两个不同时代军队之间的战争，虽然双方武器相差不多，战争的结果不会有任何悬念，因为一个在专制的农业时代后期的军队很难打赢一个兴起的工业化国家的军队。英特尔和摩托罗拉之间的竞争也是如此。 市场的倾向说明了用户对兼容性的要求比性能更重要。因此，英特尔在精简指令上推出 80960 后，就停止了这方面的工作，而专心做”技术落后”的复杂指令系列。在整个九十年代，工业界只有英特尔一家坚持开发复杂指令集的处理器，对抗着整个处理器工业。 总的来讲，英特尔并没有想彻底把 AMD 打死。因为留着 AMD 对它利大于弊。首先，它避免了反垄断的很多麻烦。 个人认为，在个人微机以外，今后最重要的市场是游戏机市场。现在的游戏机早已不单单是为玩游戏设计的了，它们成为每个家庭的娱乐中心。IBM 等公司至少在目前在这个领域是领先的。IBM 已经垄断了任天堂、Sony 和微软三大游戏机的处理器市场。 英特尔对世界最大的贡献在于，它证明了处理器公司可以独立于计算机整机公司而存在。在英特尔以前，所有计算机公司都必须自己设计处理器，这使得计算机成本很高，而且无法普及。 IT业的罗马帝国：微软－Microsoft 我在学校的老板贾里尼克院士以前在IBM当任要职，因此我们经常去IBM作报告，每次去以前贾里尼克都要确认我们报告的每一页内容是已经公开发表过的。原因很简单，IBM有世界上最好的科学家和工程师，他们可以用比你还快的速度将你没有发表的想法实现、申请专利并发表。 在这次双雄会上，乔布斯犯下的错误有两方面，首先，他自己没有意识到操作系统在今后整个微机工业中的重要性，否则他不会过早地给别人看他还没上市的设计；第二，也是更重要的，他给谁看都可以，就是不该给盖茨这个人看。 完成了在研发上的布局，盖茨要在市场上尽可能用它落后的DOS坚持到微软新一代操作系统开发出来。微软的做法概括起来是两句话，薄利多销和来者不拒。 但是，领先下的苹果犯了一个致命的错误—走封闭式道路和纯技术路线。当IBM因为反垄断的限制，不得不容忍兼容机厂家克隆自己的产品并抢走越来越多市场时，苹果正在为自己没有遇到同样的麻烦而高兴。 苹果既做硬件又做软件，很难平衡两者的速度。软件做得太快了硬件就跟不上，硬件做的太快了有没有合适的软件可用。在历史上，苹果有几款计算机一推出来时速度就已经落后了；还有几款比如早期PowerPC推出来时速度奇快但没有什么应用软件可用。 网景选择了和微软一拼，因为它觉得至少目前它还有技术和市场上的优势。后来证明这种技术上的优势根本不可靠，这也是我将技术排在形成垄断的三个条件之外的原因。 如果说乔布斯是锋芒毕露，聪明写在脸上；盖茨就是一个平衡木冠军，聪明藏在肚子里。 微软打败网景和Real Network等公司的绝招是免费提供和对手竞争的产品。但是这一招对雅虎不灵，因为雅虎的服务本身就是免费的。 公司增发期权不计入成本。因此，雅虎不断地增发期权给员工，而只需付给员工很少的现金工资。员工手中的股票期权，在华尔街的炒作下，以火箭速度往上涨 我非常喜欢黑格尔地一句话：凡是现实的都是合理的，凡是合理的都是现实的。(All that is real is rational; and all that is rational is real.)虽然这句话常常被误解成为当今不合理的现实来开脱，其实，如果我们动态地看待现实性和合理性，可以把这句话理解成，现在存在的现象，当初产生它的时候必然有产生它的原因和理由。如果这个理由将来不存在了，终究有一天它也会消亡。 进入新的世纪以来，微软的行动明显放慢，它的扩张一再受阻。但是，个人微机的这次浪潮还没有过去，处在其浪潮之巅的微软即使不做任何事，仍然是世界上最赚钱的公司。它始终是所有公司最可怕的竞争对手。它能否成功地第二次创业，很大程度上取决于客厅信息化是否能形成下一次技术革命的浪潮，而它又是否能在客厅争夺战中最终胜出。 互联网的金门大桥：思科 一个成功的公司的早期员工是非常宝贵的财富。他们一般是一些非常爱冒险的人，否则他们不会选择加入新开办的甚至是还没有投资的小公司，他们技术和能力非常强，常常每个人可以独挡一面，因为早期的公司要求员工什么都得能干。 但是，他们也有他们的弱点。他们虽然善于开创，但不善于或者不愿意守成，而后者对于一个大公司发展至关重要。他们做事快，但是不够精细，因为在公司很小时，抢时间比什么都重要。 如何留住早期员工，并且调动他们的积极性，便成为了每一个上市的科技公司的难题。 华为公司比思科成立晚四年，早于 Juniper 八年。华为创办时起点就很高，当时邮电部下面的一些研究所还在和 ATT 等跨国公司谈二流技术的转让和合作，任正非直接就定位当时国际上最先进的技术，并且短短几年就开发出了当时具有国际先进水平的 08 程控交换机。 一旦一项产品可以由中国制造，那么它的利润空间就会薄到让美欧公司退出市场。现在，思科和华为的竞争就是在这种阴影笼罩下。 英名不朽：杨致远、菲洛和雅虎公司 雅虎及其追随者们，不仅把互联网办成了开放、免费和盈利的，而且刺激了电子商务的诞生。 但是大多数创业者连产值都不考虑，觉得只有有了流量就有了一切，今天仍然有人持这种观点。 雅虎对网络泡沫的形成起到了推波助澜的作用。虽然它自己没有直接烧投资者的钱，但是无数小网络公司都是靠烧钱在维持，这如同抱薪救火，薪不尽火不灭。 到2000年大选后，终于没有新的投入进来了，互联网泡沫应声而灭。 一个技术公司，无论是过去的ATT，还是现在的Google，都会尽可能地采用技术而不是人工来解决问题，当然所有的技术都有自己的局限性和不足，一个崇尚技术的公司的态度是解决这些问题而不是倒退到手工操作。至今，雅虎仍然会手工地调整搜索结果，和Google完全用计算机排名不同。 硅谷的见证人：惠普公司 虽然惠普从来没有领导过哪次技术革命的浪潮，但是作为硅谷最早的公司，惠普见证了硅谷发展的全过程，从无到有，从硬件到软件，惠普的历史从某种程度上讲就是硅谷历史的缩影。 这种现象在投资大师巴菲特看来是很荒唐的，安捷伦疯涨，说明惠普卖赔了，惠普应该跌才是。 惠普的墨盒和吉列的刀片有个很大的区别。刮胡子刀片是一分价钱一分货，吉列的刀片比廉价低质量的确实好不少，而且刮胡子刀片是一种特殊的商品，对它的马虎不得，用一个劣质刀片刮破脸可不是件好玩的事。 如果一个公司不能挑选好掌舵人，以后替换掉他成本也是很高的。 没落的贵族：摩托罗拉 过分注重技术和品质使得摩托罗拉在商业上的灵活性远不如诺基亚和三星等竞争对手。 摩托罗拉对世界最大的贡献是它在八十年代初发明的民用蜂窝式移动电话，也就是早期说的大哥大，现在说的手机。 美国在标准之争上的失败间接影响的摩托罗拉手机今后的竞争力。 摩托罗拉长期以来都是一个了不起的技术公司，它长于技术，但是过分相信技术的作用。铱星计划在技术上是无与伦比的，但是，过于超前市场的技术不仅导致成本过高，而且维护费用也是巨大的。 摩托罗拉试图打造一个通用的操作系统作为它今后手机开发的统一的平台。这个想法本来不错，但是摩托罗拉选错了平台，选中了 Java 。 硅谷的另一面 坦率地讲，我对这些沉溺于创业梦想的人泼凉水的时候多于鼓励的时候。虽然我知道他们更需要鼓励，但是在硅谷这个环境中，他们已经得到了无数的鼓励。 创业者还必须精力过人，因为他们必须能熬得住几年每天在简陋的车库里工作 16-20 小时的苦日子。他们又必须是多面手，因为在创业初期他们必须干所有的脏活。 但是光有好的团体和技术又远远不够，他们有商业头脑而且必须找到一个能盈利的商业模型（Business Model） 硅谷几十年经验证明，那些初出茅庐能干具体事情的年轻人，可能比一个经验丰富但是已经眼高手低的权威对公司更有用。 由于 FDA 的保护，创业的小公司要打破原有制药公司的垄断是件很难的是。这就是我们很难看到小的生物公司成功的原因。 短暂的春秋：与机会失之交臂的公司 太阳和微软之争，其实就是企业级的操作系统之争。对太阳来讲，取胜的关键在于是否能将它在 Unix 上的技术优势转换为市场优势。 太阳公司当时不自觉地满足于捏 SGI、DEC 和 HP 这些软柿子、并沉溺于在硬件市场上的胜利，忽视了来自微软的威胁。但当 2000 年互联网泡沫破碎时，它以服务器和工作站为主的硬件业务便急转直下。 施密特当时是太阳公司主管软件的副总裁，他从太阳失败的教训中总结出了反摩尔定理，我们已经介绍过。施密特认识到依靠硬件的利润是不断下降的，而 IT 服务业的利润则是恒定的（并随着通货膨胀而略有增加）。 当大量杰出人才离开、同时公司业绩大幅下滑时，马可尼里没有果断地大量裁员。他总给自己一个借口，我们好不容易招到这么多人（在网络泡沫时代，找工程师是很难的），如果现在裁掉冗员，万一市场好起来，我从哪里去招人。 这正应了茨威格的话，在命运降临的伟大瞬间，市民的一切美德——小心、顺从、勤勉、谨慎，都无济于事，它始终只要求天才人物，并且将他造就成不朽的形象。命运鄙视地把畏首畏尾的人拒之门外。命运——这世上的另一位神，只愿意用热烈的双臂把勇敢者高高举起，送上英雄们的天堂。 网景选择了和微软一拼，因为它觉得至少目前它还有技术和市场上的优势。后来证明这种技术上的优势根本不可靠，这也是我将技术排在形成垄断的三个条件之外的原因。在微软方面，它也正式向网景公司宣战。 网景公司在它的浏览器畅销到网络用户时，没有居安思危，它没有注意去控制互联网的内容，这样一来它失去了保护自己和反击微软的可能性。本来它最有可能成为雅虎。 即使最初网景看不到索引和组织互联网内容的重要性，但是到 1994 年底，当雅虎的流量首次达到一百万次访问时，网景也应该意识到这一点了。如果那时候网景公司走门户网站（Portal）之路，没有人能阻挡它成为后来的雅虎。 幕后的英雄：风险投资 高回报的投资一定伴随着高风险，但反过来高风险常常并不能带来高回报。任何一种长期赚大钱的金融投资必须有它内在的动力做保证。风险投资也是一样，它内在的推动力就是科技的不断发展进步。 风险投资的关键是能够准确评估一项技术，并预见未来科技的发展趋势。 我通常把科技进步和新的商业模式分成进化（Evolution）和革命（Revolution）两种，虽然它们的英文单词只差一个字母，意义可差远了。创业必须要有革命性的技术或者革命性的商业模式。 风投公司介入一个新兴公司后的第一个角色就是做顾问。这个顾问不仅需要在大方向比如商业上给予建议，而且还要在很多小的方面帮助创始人少走弯路。 一个风投公司要想成功，光有钱，有眼光还很不够，还要储备许多能代表自己出去管理公司的人才。这也是著名风险投资公司比小投资公司容易成功的原因之一，前者手中攥着更多更好的管理人才。 越是成功的风投公司，投资成功上市的越多，它们以后投资的公司相对越容易上市、再不济也容易被收购。因此，大多数想去小公司发财的人，选择公司很重要的一个原则就是看它幕后的风投公司的知名度。 信息产业的规律性 当某个领域发展成熟后（而不是群雄争霸时期），一般在全球容不下三个以上的主要竞争者。虽然每个领域的领头羊占得市场份额不尽相同，但是通常都是比其他所有公司的总和还多。 当市场上一旦诞生了一个新的猴王，它就成为了这个市场规则的制定者和解释者，这时，市场就不可逆转地向着有利于这个主导者的方向发展。其它公司即使在技术上、管理上或者其它方面有一点优势，都不足以抵消主导者在规则制定和解释方面的优势。靠着制定和解释规则，在很短的时间里这个王者就占了这个领域在全世界的大部分市场。 在通信领域，规则比技术更有利于一个公司占领市场。 为什么在信息产业的公司比传统工业的容易形成主导优势呢？这里面有两个关键的原因。首先是不同的成本在这两种工业中占得比例相差太大。传统行业研发成本低，但各种制造成本和销售成本是非常高的。研发成本可以通过规模经济来抵消，而制造成本则不能。 科技领域则大不相同，制造的成本只占营业额的很小一部分而研发成本占大多数。对微软和甲骨文来讲，制造一份软件拷贝的成本和一百万份没有什么区别。 虽然生物制药公司和信息科技公司一样有着低制造成本的特点，但是世界上没有一种万灵药能治所有的病，甚至对于同一种病也不存在一种药能医治所有的人，因此就有很多大的生物制药公司并存。它们每个公司专门研制针对不同疾病的药物。因此前面所讲的主导性的公司在生物制药领域并不存在。 一个主导者愿意强调自己是行业的领导者，这样可以给投资者和用户信心，但是永远会否认自己有垄断地位，以免给自己找麻烦。它们在提交到证监会和其它政府部门的官方文件中甚至会列举一些小的不能再小的竞争对手，表示自己在公平竞争。 诺威格定理：当一个公司的市场占有率超过 50% 后，就无法再使市场占有率翻番了。这几乎是任何人都懂的大白话，但是却道出了许多跨国公司兴衰的根源。 当它占领了大部分市场时，它的成长就受制于整个行业的发展了。而华尔街依然期望着这个新兴公司不断创造奇迹。这时候，该公司就必须寻找新的成长点，才能不断超越华尔街的预期，公司就不得不天天为营收忙碌 通用汽车失败的根源在它根深蒂固的思维方式：它一直认定自己是个汽车公司，一定就要以汽车公司为主。这好比在围棋盘上，通用汽车有一条经营了很长时间却已经没有气的大龙，和一片布局完美可扩展空间大的实空，通用汽车总是舍不得牺牲掉自己经营多年的大龙而错误地放弃前景看好的实空，最后满盘皆输。 严格地讲，苹果其实不能算是一个计算机公司，而是一个注重创新的消费电子公司。在苹果眼中，计算机不过是新的电子产品的一种，当然苹果要把它做得越新、越酷越好。 红杉风投认为一个公司的基因在创办的一个月内就定型了，这也许有些夸张，但是一个成型的公司改变基因的可能却是非常小。越是以前成功的公司越是容易相信自己固有的基因是最优秀的。 高科技公司的摇篮：斯坦福大学 从斯坦福夫人身上我们看到一位真正慈善家的美德。慈善不是在富有以后拿出自己的闲钱来沽名钓誉，更不是以此来为自己做软广告，慈善是在自己哪怕也很困难的时候都在帮助社会的一种善行。 一旦某个项目有了商业价值，并且可以由公司资助时，政府会渐渐减少并且最终停止对这些课题的资助，因为政府（纳税人的代表）认为没有必要和工业界做重复的事，更没有必要和工业界竞争。在这一点上，美国政府和中国政府与日本政府有很大的不同。 只要一个教授能完成教学任务，并且发表足够多像样的论文，斯坦福并不限制它的教授到外面的公司兼职，甚至在一段时间里全时离开学校创办公司或者在公司里担任要职。 可以想象如果佩奇和布林不是斯坦福的研究生而是什么其它学校的，他们很难有机会直接向一位工业界领袖推销自己的发明。大家可以试想一下，在中国，一位普通的清华大学或者北京大学的研究生有没有可能通过学校直接见到华为的创始人任正非，斯坦福能做到这件事是它了不起的地方。 科技公司的吹鼓手：投资银行 罗斯柴尔德家族十九世纪中后叶达到高峰，但是犯了几次致命的错误，又遇到几次灭顶之灾，便从此一蹶不振的。该家族犯的第一个错误就是低估了美国的发展。十九世纪后半叶，美国因为种族矛盾尖锐，后来又爆发了内战，罗斯柴尔德家族对美国不看好，撤走了在美国的大部分业务，失去了追随美国发展的大好时机。（我常常和别人讲，错过现在中国发展的快车，就像一百五十年前错过美国发展一样可惜。） 由于美国银行业的基础是私有银行，抵抗金融危机的能力就很有限，在 1907 年的金融危机中，美国的银行业几乎崩溃。这时，由著名银行家摩根发起，联合了各大银行，在总统威尔逊的支持下，美国建立了联邦储备银行系统（Federal Reserve System），简称美联储。 投资公司，虽然他们又称投资银行（Investment Bank），但是直到 2008 年 10 月，它们都不是真正意义上的银行，因为他们既不能接受存款，也不能向联邦储备银行借钱，它们是替别人买卖有价证券、期货、不动产和任何有价值的商品。 有人觉得雷曼和美林只要再坚持五天就可以逃过一劫，其实，只要它们两家公司一天不死，这个计划就不会从保尔森的口袋里拿出来。 摩根斯坦利是银行业中计算机化的先驱。早在 1962 年，它就通过计算机来分析股市并且建立了很多用于金融的数学模型（Quantitative Financial Analysis Model），并且获得了很大的成功。由于摩根斯坦利在金融界的影响力，其它金融公司也纷纷效仿，从此开创了一种用数学模型分析市场的新领域。这对八十年代后对冲基金的兴起起了先导作用。 和百度相反，中石油在香港的上市堪称败笔。首先它作为全球最大的融资行动，却选择了一家二流的承包商瑞士联合银行和不入流的中信。这两家承包商为中石油作出了天价的融资股价，以至于长期持有它的巴菲特马上套现。 融资过多和过少都是有危害的。过度的融资不仅导致原有股东的利益被压缩，而且由于在短时间里流入市场的股票太多，股价很难稳定。融资过少的危害也很明显，很多公司就是因为融资不足而在经济进入低谷时无资金摆脱困境而关门。 对于那些价值不大的科技公司，一旦它们未能达到预期，华尔街则会毫不留情的打压到底，以起到杀鸡示猴的作用。 要想不受华尔街的影响，唯一的办法就是不上市。这就是 Google 在盈利很久以后迟迟不肯上市的原因 金融业在整个经济活动中起着血液的作用。健康的金融环境和秩序可以帮助科技公司的成长。但是由于金融业和巨大的利益联系在一起，因此贪婪、投机甚至非法的欺骗行为是金融业永远也摆脱不了的阴影。 挑战者：Google 公司 我们问佩奇是如何想到Page Rank算法的。他说”当时我们觉得整个互联网就像一张大的图( Graph ) ,每个网站就像一个节点，而每个网页的链接就像一个弧。我想，互联网可以用一个图或矩阵描述， 我也许可以用这个发现做个博士论文。 虽然今天的Google 和其他搜索引擎相比当初的Google 已有了长足的进步，但是这种进步基本上属于量变。搜索引擎领域迄今为止的质变只有 Google 取代AltaVista 那一次。 佩奇知道，只有把互联网的内容送到千家万户就行了，至于互联网的内容是谁的并不重要。 巴菲特有一个最简单有效的选择股票的办法，就是到大小百货店、加油站和食品店看一看老百姓都在买什么，这比听华尔街分析师瞎掰乎要准确得多。巴菲特因此而选择了可口可乐、宝洁、强生、百威啤酒、沃尔玛和卡夫食品等公司技资，都获得了极高的回报。在巴菲特看来，广大消费者才是一切商业的衣食父母。佩奇和布林也深深体会到，广大最终的用户(网民和广告商)才是为Google 带来生意的人，因此， Google 的产品一直是针对广大用户，既非像IBM 那样针对企业，又非像苹果那样针对精英。 Google 在早期并没有刻意追求营业额和利润，而是想方设法扩大自己的用户群。除了在技术上要比对手做得好以外，还将自己的网页做得特别干净，这样在到处是铺天盖地的横幅广告和弹出式广告的互联网上，显得非常超凡脱俗，便吸引了很多用户。 由于Google 很明智地没有在互联网泡沫高峰期疯狂地扩展，而是实实在在地、低调地做好自己的搜索引擎，因此它早期烧钱的速度非常之慢。2000 年的时候它没有急着上市，避免了绝大多数互联网公司大起大落并且最终关门的厄运，同时最早期的优秀人才没有拿了钱就走掉，因此Google 的骨干完好无损。 他解释 Google 的人才战略时说，好的博士生不仅有创造力，而且有最高的自觉性。硕士生同样的聪明，但是主动性要差一些。硕士能把你领到别人到过的地方，而博士可以把你带到以前无人去过的地方。 Google 是一家思维方式与众不同的公司，它认为，杀鸡一定要用牛刀。一个本科生能完成的事，如果我能找到一个硕士生来做，那么一定比同类公司做得好! 在Google 里面实际上是贯彻一种”瑞士制造”的指导思想， Google 自己把它称为”Google 的品质” 。 随着自由派总统奥巴马的当选和任职，美国和中国的蜜月期终于过去了。自由派人士占主导的Google总部对中国的互联网管制方式一直难于接受，这中间的矛盾全压在李开复身上。终于有一天，他不堪重负，选择了离开。缺少了李开复这个润滑剂， Google 和中国的关系变得很难调和，最终导致了2010 年初Google 退出中国的事件。 成功的转基因：诺基亚、3M、GE 芬兰政府给诺基亚的支持是在人才上的支持。芬兰在欧洲近乎于一个社会主义国家，它为民众提供从小学到大学全部的免费教育。这就为诺基亚提供了人才保障。同时，作为一个小国，芬兰政府知道它不可能在世界经济的方方面面都领先，因此全国只专注几个产业，当然，移动通信是一个。而在芬兰的大学和研究所里，也就很少看到跟它产业结构不太相关的专业。 到了数字手机时代，不同手机的话音质量相差不像以前那么大，这时手机的功能就变得非常重要了，再到后来手机的外观都变得重要起来。 诺基亚由一个地区性的木工厂发展到全球最大的手机厂商，原因可以简单概括为长期探索、抓住机遇和制定规则几个字。 3M 允许员工用 15% 的时间干任何自己喜欢做的事，后来这个做法被 Google 学去了，变成了 Google 的百分之二十项目。 3M 公司在适当的时候强制淘汰一些看似还在挣钱但是前景不是很好的产品。 因为企业级的产品和服务受经济大环境的影响太大，收入相对不如消费者产品来得稳定。 了解 GE 公司历史的人都知道这个充满传奇色彩的公司是由著名发明家爱迪生创立的，是将电最早介绍和普及到世界上的公司。 3M 是靠硬性的制度维持其创新，而 GE 是靠自己不断开创新的产业。 印钞机：最佳的商业模式 所有成功的大公司都有好的商业模式，很多大公司的兴起，不是靠技术的革新而是商业模式的转变。 在这台印钞机中，自动化程度必须到达一个阈值，它才能自动运转起来。而当它的自动化越高，成本就越低。 长城公司自己开发微机全部软硬件，自己在广东建厂，自己采购元器件，有自己的仓储，最后还发展了一大批批发和零售代理商。大家不难看到，长城公司处处走的是和戴尔公司相反的路线。当长城计算机公司很自豪地拥有了这一切时候，它的资金利用率已经比戴尔低了很多，而它的产品的价格却高了很多。 互联网 2.0 到2000年世界上流量最大的网站全部是门户网站，在美国是雅虎、MSN 和Excite 等在中国则是新浪、搜狐和网易三大门户网站。 互联网2.0的公司不应该过多主导内容和服务，不应该参与和用户的竞争。以YouTube 为例，它host 的内容是用户（包括个人和专业的传媒公司）提供，它自己并不制作和拥有内容， 与其他提供内容的用户竞争。 互联网2.0公司的一个普遍特点就是专注于打造通用的平台，而由社会上的开发力量和广大用户补充成完整的服务，从Wikipedia 到Facebook 都是如此，它们相当于软件时代的甲骨文和微软。 金融风暴的冲击 科技像是我们这个社会的大脑，而金融则是血液。没有了科技，这个社会就会是混混沌沌的，就像中世纪的欧洲；而没有了金融，这个社会就停止了运转。 金融危机的直接和表层的原因是次贷（Subprime Loan） 及与之相关的金融衍生物CDS ，而间接和深层的原因则是违反经济规律地只消费不创造，从而坐吃山空。 当人们为格林斯潘神奇的控制经济的能力叹服时， 实际上他是用一个更大的房市泡沫解决了相对小得多的互联网泡沫的问题。 金融危机首先将淘汰掉这些无法适应新环境的恐龙级的公司，因为这些长期部借新的贷款来偿还旧的贷款利息过活的跨国公司，现在已无法贷到新款，或者贷款的成本太高。 大量不健康的公司会使得整个社会的投资效率低下， 经济生活变得死气沉沉。这些公司的消失，为健康的公司腾出了宝贵的市场，健康而有活力的公司将成为未来直接的收益者。 金融危机使得以前一些不可能的兼并成为可能，并因此打造出一些竞争力更强的大型公司。 经济危机时，风险投资家们不得不主动收缩战线，弃卒保车。它们通常的做法是迅速关闭成活可能性小的公司，把资金集中投入给可以生存下去的公司中，保住一些重点投资。对于那些运营不错的公司，反而可能得到比宏观经济好的时期更多的资金，虽然它们日子也颇为艰难，却无疑是遇到了一个非常好的发展机会。 我的建议是，不求大的发展，但求生存。必须要保证12到18个月后还有资金，还活着。现在看来，能坚持一到两年的小公司，发展得都比金融危机前要好。 这些公司因为没有利润，甚至没有营业额，根本无法按照传统的市盈率（ P／E ） 比来估价，于是他们伙同华尔街发明了用股价／ 流量比来对这些网络垃圾估价。 云计算 在甲骨文的Network Computer 失败后， 第一次互联网的泡沫也破灭了，大家对互联网的作用也产生了怀疑，一切基于客户端的想法又占了上风，因此很长时间这种基于Web 应用的概念没有人提了。 亚马逊发现，它不仅仅可以为商家提供网站托管服务，还可以为任何需要建立网站的公司提供。这样，任何一个想通过互联网提供服务（包括电子商务） 的公司和个人都不需要自己建网站，而只要租用亚马逊的计算资源即可。这就是亚马逊理解的云计算，它和IBM 的理解完全不同，但是没有矛盾。 云计算本身是一个非常复杂的系统工程。它的普及首先离不开巨型数据中心的建设和全球高速光纤主干网的铺设，这就好比电的普及离不开发电厂和输电网一样。在Google，这些工程称为全球基础架构，它本身也需要很多关键技术，甚至一些看上去IT无关的技术，比如制冷技术。全球基础架构设计和实施的好坏可能会导致运营成本上成倍的差别。 云计算要想得到充分的发展和全世界计算机产业的认可，不是单凭任何一家公司的力量就能做到的. 下一个Google 很多公司靠炒作概念，而利润平平，在一段时间里，股价炒得很高，他们依靠发行新股获得了充足资金，然后盲目扩大业务。虽然风光一时，但光环迅速消失。 即使今天出现了一个比微软Windows 好一倍的操作系统，用户也没有要换操作系统的欲望，因为微软的够用了 。够用了这条很消极而枯燥的原则，让所有想通过做一个操作系统取代微软的努力都变得无效。 云计算不仅是今后最有希望的产业， 而且是上面很多产业，比如3G 移动通信的支柱。 可以毫不夸张地讲，阿里巴巴已经完全占领中同电子市场的制高点，而且只要它不犯大的错误，现在找不出一家公司可以挑战它的商业地位。 马云自己都认为是阿里巴巴开创了中同的电子商务市场。从效果上看这一点并不错，但是有些因果倒置。事实上是，中同非常糟糕的批发和零售商业环境，要求必须出现一家阿里巴巴这样让商业变得容易的公司。 腾讯的做法和Google 非常类似，通过一款产品抓住终端用户，然后通过一些服务从每一个用户身上挣钱。 美国取代英国成为世界第一强国的时候，依靠的不是任何资本泡沫，而是爱迪生和西屋（发明电和交流供电）、贝尔（发明电话）、福特（发明汽车）、怀特兄弟（发明飞机）等这样一大批在世界文明史上占有重要位置的发明家，以及洛克菲勒（石油）、卡内基（钢铁）、杜邦（ 化工）等搞实业的工业巨子。 当人们不再把房市、股市作为最快的挣钱手段时，就是中国可以诞生下一个Google 的时候了。","link":"/reading-note/top-of-the-wave/"},{"title":"翟东升政经启翟系列视频","text":"翟老师与观视频合作的系列视频，内容可能较为广，此处总结要点便于回顾。 整个系列视频的时间为 2020-06 至 2021-02。 整个系列视频约有40节，内容涵盖政治经济教育文化等方面。 是翟老师其自身研究观点的整理与科普输出，值得思考。 2020-06-03 雄安，你就是中国的学园都市了！如何在新型举国体制下处理政府和市场关系 市场是政府的创造物，是一种基础性公共产品，目的是覆盖消费者。 政府要帮助新的商业模式探清道路。 科学由政府来做，技术由市场和企业来做 政府需要向国民提供安全。 政府可以为企业分担小部分的技术研发风险。 科技创新： 技术路线不能单主体决策 适当的远离市场，“养一批闲人”在雄安，创造一个良好的科研氛围。 2020-06-10 美国以后不让我们摸了，中国该怎么自己过河？创新人才基础存在（智商较高）。 中国文化抑制创新的因素： 重物而轻人，不愿意为人才买单只愿意为物品买单。 对说谎造假的惩罚和仇视不够，机会不能给到有真才实学的人。 好面子怕出丑怕红脸，尊老。 不喜辩论不喜批判。 教育考试体系求全，要求平衡发展。 人才之间跨国交流，目前被抑制。 2020-06-17 曾经的华尔街之狼，为什么在特朗普时代成了哈士奇？美国金融业对美国内政外交的影响力产生持续大规模的衰落。 1960年代，华尔街背后家族有多个族群，基本都是犹太人。 1970年代前，美国国家利益就是有利于通用公司的利益。 1980年代，什么有利于华尔街什么就有利于美国。 上位过程： 华尔街通过捐赠逐步上位。同时搞人员渗透（1960开始持续上升，最高达到60%）。 华尔街搞意识形态渗透，逐步上位。例如格林斯潘。 华尔街大而不能倒，绑架内政外交。 下位过程： 08年金融危机之后政治声望一落千丈 和其他医药协会、步枪协会比不占上风 世界资本主义利益分配结构： 美国产业通过股权债券外包给欧日韩 欧日韩FDI（外商直接投资）产业迁移到中国实现中国工业化（要付给外商15%） 中国买美国国债，和华尔街有紧密联系（3%收益率） 3%-15%是快速工业化的代价 中方和华尔街关系紧密（互利），2016年之后华尔街影响力急剧衰弱。 负利率时代，受伤最严重的就是金融系统，所以华尔街影响力可能还会持续下降。 2020-06-24 中国年轻人压力这么大，怎么才能让他们有钱消费？《美国真相》：政府市场关系的分析。 市场私人部门繁荣是因为政府能提供有效公共产品： 和平安全 基础设施 国际贸易条约 基础科研基础教育 知识产权保护 在保证公共产品的情况下，综合税率越低越好。 为何瑞典和北欧区域如此成功？不是因为税收高，企业就不干活，而是政府有钱可以转化为有效公共产品，导致私人部门的繁荣。 国内主要矛盾： 全球总需求本国总需求增速偏低，但是供给能力偏强，供给过剩。 所以： 内循环，让中国年轻人有钱去消费。 中国综合税率和美国差不太多，并不适合给企业减税，降税是增加了后代债务，给全球消费者输出了通货紧缩，加强了中国与外部的竞争力度。 鼓励年轻人生孩子。 2020-07-01 数据时代中国真正的对手还是只有一个：美国新技术带来的挑战：科技进步对中国对世界的冲击和影响。 科技进步利于再分配而不是创造新财富。 以更低成本以满足原有需求。 现象： 垄断性平台剥削生产商 商家相对消费者能力更大了 科技进步后果： 贫富分化进一步增大 大企业和政府谈判地位更强 负利率时代，人工智能发展时代，金融业能容纳的就业人数未来将会萎缩。 教师等细分领域的人员也会需求降低。 英语汉语西语对其他语言的文明的冲击可能十分明显。 如何建设新时代中国特色社会主义，不能刻舟求剑。 重新思考教育制度，终身教育。 退休制度，福利制度。 养老育儿制度。 2020-07-08 黑人运动在美国翻不了天，真正会使美国蛮族化的力量是…美国文化革命的根源： 种族的结构性变化 贫富分化 美版文革特点：种族党派贫富分化叠加一起。 共和党几乎全部白人。民主党五花八门。 美国文化革命发展缓慢，呈现周期性，需要社会动能去刺激。持续时间长。 运动的前景： 虽然以黑人为导火索，拉丁人真正动员起来才是真正的高潮。 平权的悖论：肤色，性别，年龄段。是要求机会的平等还是结果的平等？ 如何评判革命是否成功？（是否通过斗争达到团结？）美国正在出现古罗马的蛮族化。 对中国的启示： 要对中美竞争有信心。 信心源于，我专而敌分。 美国种族主义的原罪。 加强对拉丁裔文化的研究。什么样的新美国化？对美政策有的放矢。 2020-07-15 翟老师：下代霸主应该是个十亿人口规模的国家…印度：正是在下全球化的周期（贸易全球化指标：出口/GDP）： 1870-1914 上升 1914-1945 下降：英帝国的衰落 1945-1979 平行竞争，半球化时代 1979-2008 全球化时代，中国融入世界市场，英美提出新自由主义全球化。 2008-至今 全球化指标下行，逆全球化。 为什么全球化会有一个波动？全球化是历史的必然趋势吗？ 广义全球化：大航海突破地理隔阂。 狭义全球化： 霸权周期就是全球化周期。 几波全球化： 荷兰大航海，使商品跨境流通。人口大规模增长。百万人口。 英国工业文明诞生，FDI跨界流通。人均GDP跨境流通。千万人口。 美国，信息革命，货币跨界流动。亿级人口。 十亿人口规模？中国？印度？欧盟？ 2020-07-22 统治世界400年的新教，就要被我们“筷子教“打败了？看起来是霸权更替，但是荷兰英国美国都是新教文明。 12年前，指标上出现重大变化。2019年，东亚文化制造业总产出超过新教文明圈。 东亚文明圈的共同点，都在用筷子。 过去两百年形成的大众政治体系在逐渐过时。 小众政治时代，对全人类的政治稳定提出巨大挑战。 2020-07-29 欧盟又开始追求大一统了？中国先笑出了声……全球市场对欧洲一体化前景乐观。 需要中央政府对利益进行再分配。因为内部不同省份竞争力变化。 在欧盟下看，汇率对德国低谷，对南欧高估，所以强者愈强，弱者恒弱。 但是以前欧盟的转移支付能力有限。 新冠疫情下的转移支付法案，让他家看到了一体化的希望。 欧盟对中国的定位（疫情之前）： 系统性对手：政治和社会制度方面 谈判伙伴：国际问题 经济竞争 未来三角游戏： 军事：中美俄 经济：中美欧 欧盟体系：成员国太多，成事不足败事有余。 2020-08-05 眼看和平演变马上要成功..公知和特朗普：快！“救”中国黑格尔：重要的历史往往上演两次，第一次是悲剧，第二次是闹剧。 美国对华派系 老一代中国问题专家，知华派愿意接触中国，白人为主。 新一代中国问题专家，很多由中国去美国，对中国文化抱有仇恨态度，认为接触派是错的。坚持有原则的现实主义，施压中国。 进入美国体系，经济上的获益，是以独立性丧失为代价。 虽然特朗普政府短期带来的损失比较多，但是戳破了过去四十年美国对中国实施的和平演变政策，而且客观的太高了中国的国际地位。 两个时代的中美关系： 奥巴马时代：不平等，不对抗 特朗普时代：平等，对抗 2020-08-12 美国：我得癌症了，急需医生！美国医保：可肿瘤就是我美帝国为什么相对衰弱了？ 过度扩张穷兵黩武？这个观点站不住脚。国防占比占财务支出规模减少。 美帝国体系最大的问题是内耗。医疗医保支出持续扩大，医保体系存在巨大的系统性问题。 美国医疗产业越来越垄断，形成强大利益集团。 药品福利管理局也在合并垄断，推荐高价格药品。 保险业不受联邦反垄断法管理。 且这一系列链条向政府提供了很多的政治献金。 猫鼠沆瀣一气。 如何游说的？ 华盛顿K街，很多游说公司。 和国会议员助理沟通，再简短的和议员见面写支票，进入他下一次的竞选资金池。 2020-08-19 从小父母告诉我读书可以改变命运！美国学生：我酸了…两种教育思潮的撕扯： 自由主义教育思想：爱与自由，自然生长。重视文科艺术。个人价值本位。 非自由主义教育思想：调用本民族教育资源。从严要求约束。重视理工科。集体价值本位。 美国美国自由主义教育好不好？ 公立教育质量非常差。 很早放学。 体育娱乐活动多。 理工科难度比较低。 中小学老师待遇低，素质差。 读书改变命运的观念不存在。读书无用论。 美国高等教育存在巨大泡沫。 人文领域人才来自私立教育，理工人才来自欧亚大陆。 日本看到日本年轻人综合素质比较低下。举例： 中文专业无法用中文英文交流。 旅游专业无法安排行程。日语也无法讲解。 1980教育改革，让孩子自由生长。 造成平成废宅。 百年前英德自由主义和非自由主义教育理念之争，百年前英德已经完美演绎了 1870-1914： 英国 小学入学率偏低（50%），中学更低（25%） 着重于发现天赋 嘲讽对技能的培训 德国作为一个落后国家 重视教育，认为教育是义务 强调纪律和礼貌 重视理工科 中国不能学习英美的自由主义教育。 坚持为中华民族崛起而读书。 应该继续重视理工科。 在高考下，寒门仍能出贵子。 针对目前教育的问题，可以通过更实际的方式解决。 2020-08-26 压低老百姓的福利来发展本国经济，这不是必须滴！以德意志第二帝国的成功经验为例。 发达的公共产品： 市场经济 福利制度 全序列高质量教育 基础设施建设 立法保护创新 德意志帝国产业部门兼并联合。 真实世界中垄断不一定带来低效，可能会以全球消费者的福利损失来通过工资和福利分享给本国国民。 德国金融为实体产业服务，鼓励金融和实体产业股权人事上高度交叉。 对中国启示： 加大教育研发投入，保护知识产权 搞好社会再分配，实现社会团结 不必压低本国福利来谋求经济发展 2020-09-02 这个盛世帝国的灭亡，给中国复兴之路留下的5个经验教训以德意志第二帝国的灭亡为例。 二十世纪早期的德意志帝国和当今中国有相似之处： 后发的工业国，通过新一轮工业革命的基于实现对原有大国的赶超 与原有大国是盟友关系，但趋于反目 陆海复合型国家，且安于陆权 想过与原有大国重温旧梦（英德，中美G2） 新型大国韬光养晦 反对老大国的自由主义经济学，强调国情独特，强调个体服从整体，爱国主义价值观 国内搭建铁路，海外延伸，要实现大陆体系互联互通 模仿发达国家再技术反超 面临外国邻国的仇怨 工业化之后通过财富再分配缓解国内分化 认为老霸主是全球帝国，不会集中所有力量，对方会上门来打，自己有主场优势。把问题想象的相对简单。 会重蹈覆辙吗？有若干点关键不同。 政治制度不同。世袭封建和资本主义的嫁接vs民主集中制度。 名族个性不同。中国人实用唯物主义。德意志浪漫主义，内在的自杀倾向，悲剧之美。 人口规模不同。中国相当于美欧日只和。人口质量长期来看不同。 外交制度不同。中国不搞扩张主义。 中国不穷兵黩武。 对我们的教训： 不要过度刺激民族主义。 给本国民众提供平等发展空间。 不能热衷操纵民意。 联盟战略。 少搞存量博弈，多搞增量博弈。 军事力量建设，不能面面俱到要有专长。 在一两个维度有绝对优势。 充分战略规划和估计，不能太过乐观。 2020-09-09 讨论“中国GDP何时超美”没意义，这不是时间能决定的问题用线性外推推测中国GDP合适超过美国是存在很严重的错误的。 大国GDP相对力量变化背后结构性因素逻辑是什么。 中国经济超过美国不是客观趋势而是一种选择。 背后的逻辑结构： 过去100年绝大多数国家GDP难以长期超过美国。 中心国家比外围国家富有且稳定。 大国人均GDP占美国比例会不断上移趋进却无法达到美国经济。 美国的GDP和其他国家的GDP不一样。美国提供流动性，其经济是虚的。 美国经济的“虚”决定其他经济的“实”。 美国GDP占全球1/4： 非美经济规模应该等于美经济规模（生产=消费） 美元占全球货币市场份额50%多。 所以： 中国如果以美元储备为基础，永远无法超过美国GDP。 出口换美元存在局限 2020-09-16 美帝国自以为一切尽在掌握，不料失控而让这里再度崛起东亚供应链的政治经济学含义。 目前三大供应链：东亚，美加墨，欧洲。 日本是龙头，其成功之后低端产业转移到四小龙，之后再转移到中国，现在正在转移到越南。 为什么东亚能成功？龙腾文化。 东亚具有强政府，对于国民比较强势。工业化往往是由强政府推动。强政府一般是由战争而来。 东亚民众智商高。全球人均智商最高的区域。 东亚文化总体不信神。（升官发财生儿子。交易心态。）高储蓄率。 东亚模式缺陷：依附性的出口导向的发展，以本国民众的血汗换取别的国家的主权信用。 美国用投资收益顺差来支付贸易逆差。 经济表现不错，通胀低位。 但是以中国加入东亚供应链而转变。 于是美国提出TPP想把中国踢出，但是特朗普推出。 所以RCEP获得发展。 2020-09-23 中国工业化搞全产业链通吃？这是违背经济规律的幼稚做法！中国实现最大规模最快的工业化，主要原因1992年之后： 区别于日韩，中国欢迎别的国家来投资。（超国民待遇） 这一系列工业化浪潮，打乱了日本的雁行计划。 1994年人命币贬值，与美元非正式挂钩。 开放欢迎制造业外资。 中国角色的变化，从低端到终端。 本土品牌，本土供应链崛起。 目前全产业链都有。 中国要保持全产业链吗？不能。 汇率定价，要素价格配置不能同时适应低附加值和高附加值的商品。 通过产业升级改造，可以产生贸易顺差，人民币汇率会进一步升高。 此时劳动密集型产品没有生存空间。 中国劳动力总人数萎缩，劳动力成本越来越高（这是我们发展的目的。）。 我们不是要保持全产业链，而是要保留高附加值的产业。 不能有卡脖子的事情。 低端的污染的产业要让别人承担一部分。 低端产业也不能转移到一个国家，非东亚国家。 新冠会导致去中国化加速吗？会强化中国制造业的地位。 虽然东亚制造业先断裂再快速恢复，介入到了很多原先排斥我们的地方。 中国在东亚取得了中心地位。 政治影响之前日本经地区首要地位时，政治上十分软弱。 而中国的地区领导力令人刮目相看。 2020-09-30 日本究竟毁在哪里？日本政策界：我们也反思了35年…中国汇率提升会不会重蹈日本的覆辙？ 日本有什么问题和德国相比，日元升值很晚。 大企业财阀推动少升值晚升值。 导致产业升级得慢。 德国不靠汇率低估来获取竞争力。 所以德国央行不需要大规模放水导致泡沫。 同时日本生育率持续低迷。 广场协议日元升值。 广场协议之前日本大幅低息放债。 这种快速升值，伤害了其制造业。 日本通过资本项目放开和离岸中心来拉动经济。 导致资产泡沫化。 问题所在：开始拒绝升值，后来短期大幅升值。 对于中国2005年开始升值，每年6%左右。 政府给予纺织等行业补贴。 但是去除补贴他们的利益也创新高。 2020-10-07 中国低端制造业能不能转移到内陆？取决于这两个关键因素观点：不可行 云南行想要将江浙沪玩具产业转移到云南的园区。 但是价差很微小。 长期来看这些制造业难以生存。 人民币汇率长期看涨。 且老龄化形势严峻。 云南的要素价格难以与东南亚等国PK。 劳动力产业东南沿海的原因，运输成本低。 云南运输成本很高。 重庆黄奇帆引入其他地方的产业，补贴成本。 仍然没有提高重庆占国内的制造业占比。 我们发展的目的让人民过上好日子，而不是老板过上好日子。 不是为了拥有这些制造业。 一些无关国家安全的产业可以放弃。 2020-10-14 听说欧洲人有好为人师的习惯，那就给他个机会，顺便打击美国反华势力新时代中国应该向谁学习？欧洲。 过去的中国，先学习苏联，再进行中国特色社会主义探索，再学习美国及其盟友美日韩。 为什么一定要向他人学习呢？ 向自己学习不吉利（lol），中国之前向谁学习谁就变差了。 师心自用和我们传统文化相悖，我们虚心好学。 和世界态势有关。国外势力新冷战，中国也没有意愿推广中国模式。以美国为首对中国模式敌视。如果抬高欧洲有利于制造统一战线。 向欧洲学习： 我们依旧谦虚 不再向美国学习 寻求政治共识，孤立分化共和党反华 学欧洲什么？欧洲不是一个统一的模式： 莱茵模式，以德国为代表 斯堪的纳维亚半岛北欧模式 地中海模式 盎格鲁撒克逊模式（比较市场化与美国类似），出口导向 我们想要学的是前两种模式以及作为一个整体的模式 德国的资本主义：重视国有企业在命脉产业，大陆法系，发挥国家市场两种力量 利益相关者资本主义模式（不是stockholder） 欧洲各国在专利发展，人口阶级纵向流动，绿色发展等方面都十分领先。 欧洲最大的公共产品是和平，其实现了一个人类命运共同体。 学习欧洲需要避免的教训： 移民政策 福利政策，给老年人福利偏高 个人价值本位的人权政策，抽象人权。人权不能神圣化，我们要搞集体价值本位的人权。 2020-10-21 打破人口外流与经济衰退之间的恶性循环，振兴东北奥里给！东北有什么问题？东北为什么不行？ 并不是因为讲人情搞腐败，90年代的广东也有这些问题。不能用普遍因素解释区域性的特殊现象。 和地理区域有关，冬天工作时间少。但是这个观点也不合理，北欧和美国东海岸都冷。 全球化和老龄化带来的人口流动产业变迁才是根本问题。 全球化： 解放前是工业化最早的地区，解放后学苏联也是很领先全中国的。 1992年之后，中国成为美欧体系外围，东南沿海区位优势，人口优势处于这个体系中。东南沿海的枝叶嫁接在别人的根上，而放弃了自己的根东北。 东北军工科技和其他地区脱钩。 老龄化： 老龄化社会产出不会产少，可以用机器 但是老龄化社会消费萎缩，人的生命是有欲望周期，48岁是欲望高峰。 下岗潮和老龄化浪潮导致东北和其他地区拉开差距。 这两个因素导致人口自由流动，年轻人出去回不来，加剧了东北老龄化趋势。 本地的就业进一步下降。 人不是物以稀为贵。 人越密集不是竞争更激烈更难，而是每个年轻人发展机会越大。 认识群居动物，人聚在一起分工规模扩大，交易机会增加，合作成本降低。 为什么东北成为中国老龄化最快的区域？ 因为东北是现代化城市化工业化最早的区域。 现代化核心指标，女性的受教育提高。 女性个体意识觉醒，导致离婚率上升，生育率下降。 这个对女性是好事，但是长时间轴看，老龄化趋势加重。 如何振兴东北？寻找一个契机，是东北区域形成新的良性循环。 e.g. 图们江出海口打通，每年多几个月北冰洋到西欧的海上通道。 再做一个新城大城，提供特殊政策，吸引东北年轻人，甚至中国世界的年轻人。 2020-10-28 地方政府亟需从抢资本转向抢人：得年轻人者，得天下！中国地方政府操盘过程中从强资本转向抢年轻人的逻辑。 资本不再稀缺，人才稀缺人才是21世纪最重要的发展要素。 这个转变的原因： 全球需求的萎缩，产能过剩 别国更富的人变老了，在去杠杆 中国人也在变老，产能过剩 欧日长期处于负利率时代 所以目前是“资产荒”，有钱人不愿消费，年轻人没有钱。 蛋糕难以做大。 存量博弈时代，需求侧更加重要，才要抢人。 从别的地方抢年轻人，以邻为壑。 抢人是划算的。 极少部分人才是重要的，其他人只是为了保持基因多样性。 10000人中最优秀的人被吸引走了，那这10000人价值其实很少了。 人才的现金流十分可观。 年轻人的养老教育等都是将来时。 但是年轻人带来的效益是立竿见影的。 （深圳直呼内行。） 鼓励地方政府恶性竞争？这个从来不是新的东西。 以前一直都是抢资本的恶性竞争。 但是抢人会改变竞争的方式和重点。 把补贴外国资本的资金拿来补贴本国年轻人。 以人为中心，比倒贴资本来说，格局更有合理性。 先有人还是先有产业？蛋生鸡还是鸡生蛋？ 有人就有市场。 不一定是要人才，有需求的年轻人都是需要争取的对象。 BBC这种无良媒体，是媒体经济学家。 他们不懂底层逻辑，只拿数据对比。 人口流出地区，借很少的债都是在作孽。 人口流入地区，多借债是没问题的。 比如武汉区域位置很好，大学教育先进，长期来看，武汉的大学生一半左右留在武汉。 此处来看基础设施建设很必要。 如何抢人？ 要形成良性竞争。 年轻人在你这里生活很方便。 基础设施，医疗教育跟上。 抢人再着急也不能抢外国人。 中西部不适合抢人，抢人适合大城市。 政治经济学原理： 财富的源头是人而不是物。 发展的本质是人的能力的提升而不是物的堆积。 技术进步导致人的消费比劳动更重要。 所以年轻人比老人更重要。 自由主义经济学主张，善待企业家。 但是这个很容易变成，善待有钱人，厚待有钱人。 民本主义政治经济学要善待本地的年轻人。 2020-11-04 是什么让资源红利成了诅咒？政府需四大调控政策避免这些小城坐吃山空资源诅咒的问题当一个国家发现某个市场需求巨大的自然资源，最初会有经济热潮，之后回落，升值可能有长期的经济萧条。 资源反而是上天的诅咒。 这个行业挣钱不挣钱和人是否努力关系不大 国家的分类四种分类 中心美英：提供货币信用 次中心欧日韩：提供研发，高附加值产品 再外围中涂墨越等：卖血汗 最外围国家中东非洲拉美：卖资源 在国家内部也存在这样的分工。 发展不均衡。 发展不是物的堆积，发展是让人的能力不断提升。 资源诅咒的病理机制多重作用路径复杂交织。 市场角度，大宗商品波动大。给能源产地经济带来巨大冲击。 价格上升期，其他产业萎靡，储蓄资金都涌入能源 价格下行期，围绕资源的产业全部倒掉。所有人几乎同时失业。 （老百姓放高利贷，利滚利发大财） 政治和社会机制。 收益容易被当地有钱有势的人窃取。 采掘技术要和大公司合作，民选总统往往是大公司的傀儡。 最后留给人民的往往是一地鸡毛。 存量博弈，导致当地矛盾越来越大。 如何治理资源诅咒现象宏观调控，对资源采掘业抑制发展。 深度研发，生产下游产品。 控制总收入。价格低多卖，价格高少卖。 把资源采掘业获得的现金流培养本地年轻人。 搞储备，建立财富基金，维持平稳收入和经济波动。 2020-11-11 特朗普用贸易战拖住中国，之后拜登能“贡献”什么？日本：我有一计…大选的看点： 共和党已经抛弃特朗普，权贵阶层也抛弃。但是特朗普并没有输得很惨。拜登大赢没有出现。 一部分古巴裔年轻人背叛了民主党。公交车原理，上车之后希望后面人不要来。 投票和拜登关系不大，其实是对特朗普的表决。 提前投票，邮寄投票很利于民主党。 这次大选有可能严重伤害美国政治治理体系，戳破美国政治神话。 民主神话，撕裂美国，无论选谁，另一半都不服。 法制神话，如果诉诸司法，最高法院，大法官怎么判。 国会的归属，可能出现跛鸭。 民主党共和党分歧会扩大。 未来中美欧关系唯二能交流的话题： 医疗环保 公共卫生 拜登为人心胸宽广，情商高情绪稳定，可以谈成合作。 但是年纪比较大。 而且他其实也只是过客。 关税战一定会被拜登停掉。 即使加入TPP也对中国影响较小。 2020-11-18 它是决策者的参考也是投资者的僚机，请看今天的人口解读！如何从中国人口曲线研究中国经济？ 人口是慢变量，塑造的是长期趋势。 2014年那个时间节点，中国各年龄段的人口分布： 中国人口曲线的特点从曲线来看 50岁之后：自然规律，人的老化，曲线下降 50岁之前：社会规律，向左下方倾斜。总和生育率TFR下降（世界银行数据） 出生波动性很大。 回顾中国人口（联合国儿童基金会数据） 婴儿潮1963-1970中国，每年接近3000万。 回声潮1985-1989中国，每年近2500万。 2000年后新生儿掉到2000万之下。 单独二胎，2016全面二胎，2017反弹到1780万，然后迅速下跌。 人口的质量来看： 接受过高等教育的人数随代际变化很快。 婴儿潮（约30/3000） 今天（800/1500） 人口曲线对政策制定的影响2009年的人口数据，可以推断出2011年起中国人工工资将会出现大幅上涨。 后金融危机时代，中国60后人口退出低端劳动力市场（比高端劳动力退的更早）。 3000万人退休，但是只有不到1000万未接受高等教育的人接替，所以农民工数量大幅减少。 所以工资水平上升。 所以不担心农民工失业现象，即使没有经济增长，因为我们生不出孩子。 中国经济增速保GDP稳定增加意义何在？ 1%GDP能多创造约近100万非农就业岗位。 这是一种确保中国社会稳定的政治需要。 2012年之后，中国政府并不故意维持较高增速，我们低端劳动力就业压力降低。 即便经济增速2020年，2%中国东南沿海企业仍然招工难。 未来很长一段时间，真正面临的是大学生就业压力。 引入低端产业的同时，其实是把高端产业让给了西方国家。 新冠疫情加速了这样一种以内循环为主的方向的切换。 只有这样创造的岗位才是自主品牌设计，高端产业，研发等。 人口曲线对个人投资决策的影响需要配合另一条曲线。 人一生的消费曲线。 年轻人没钱有欲望，老年人有钱没欲望，所以消费的都少。 消费高峰期时是中年人。 从统计意义来看，人哪一年生孩子哪一年买车买房，哪一年孩子出国是一个定数，符合大数定理。 一般来看，大学毕业要结婚会买房。 42岁事业有成收入较高换大房子。 47岁左右消费高峰。 52岁买豪车。 60岁医药支出的高峰。 65岁养老度假高峰。 这样结合婴儿潮和回声潮： 2005-2012全中国会有一个房地产牛市（从90年代推断）。 2016-2021会有房地产小牛市，小户型为主，大城市远郊区。 85-89年婚房消费高峰。 2027-2030最后一次温和的牛市，好于2016-2021，弱于2005-2012。 集中在大户型，回声潮买房人生巅峰。婴儿潮养老房。 年轻人口持续聚居的区域依旧会上涨。 流失的区域则会下跌。 总结过去式，未来中国人口曲线波动性很少。 但是可以用到，一带一路沿线国家投资。 2020-11-25 美国：10年了，我都想把中国从全球经济里叉出去！RCEP：现在梦醒了吗？中国加入RCEP的影响短期内对中国的经济影响有限。 10-20年生效。 给各国国内一个缓冲阶段。 较发达国家使用负面清单，而中国需要6年从正面清单转向负面清单。 这个要求政府对经济运行体系管制效率题能力要求更高。 （来自朋友的数据）经济学量化模型，模拟对各国福利增长的影响，增长有限。 但是RCEP的价值更多的在于重塑东亚地区地缘经济体系。 RCEP对东亚供应链影响中日韩等经济链依附于美欧消费市场，是全球外围地区。 现在通过全世界最大的自由贸易协定，整个体系就能够构成一个较为完整的经济体。 但是还缺一个大市场，之后中国的再分配和经济升级，中国将成为全球最大市场。 RCEP对中日关系的影响日本在过去50年间难以扮演实质的东亚经济领头羊角色。 RCEP实质实现了中日之间的自由贸易协定。 可以理解为日本即将回归亚洲，以中国为中心的东亚贸易体系（经济上）。 只有在经济政治安全建立深度依赖的时候才能看到“亚洲是亚洲人的亚洲”。 RCEP对CPTPP的影响面对TPP中国需要做很多改革：工会、知识产权、环保、争端裁量权。 否则需要自绝于东亚。 所以中国加入RCEP谈判，这是一种对TPP的反制。 2017美国退出TPP，日本看摊CPTPP。 中国政策延续性很好，不会否定前任。 如果2021年初把中欧投资协定搞定，中日中欧大局已定。 美国排挤中国的图谋难以成功了。 RCEP看中印对比印度自绝于东亚经贸新体系。 印度国家能力较弱。 国家能力指可以强势的进行国内利益再分配。 （来自社会革命改造，人民的信任。） 印度的自治自立更多的是一种政治审美，而不是社会科学。 逃遁避世，拒绝先进，是一条自我毁灭之路。 穆迪想学邓小平，但是缺乏毛泽东帮他进行必要的社会改造。 2021-01-06 复盘2020年我的十个预判，2021年再做七个展望2020年初的判断 1月份，乐观面对疫情，认为5月能战胜病毒。 认为病毒对欧美伤害更严重。 是一次大考，考国家动员能力。 但是没有想到美欧如此惨烈。 西方社会不仅会面临金融波动，还会大规模失业，引起政治危机。 有些误判。之前支持桑德斯年轻人会打败建制派，预测错了。 预判对华舆情恶化。 人的本能，在困难的时候妒忌和指责。 2020，中国形象面临挑战。 反对美股会崩，预判标普跌破2300之后反弹，到冬季疫情重来之后再次下跌。 但是没有料到美元防水情况下市场特立独行。 认为人民币成为全球金融避险地之一。 成为逆周期货币。 预测对了，人民币升值。（虽然比多年前的预测弱了一些） 大宗商品的波动。现在来看只能说大体正确。 预测美国传统产业出现破产潮。 但是在美联储和政府的救助之下，没有出现。 这个预测是错的。 认为2020年制造业不会离开中国反而会有全球占比扩张和升级。 这方面说对了。 东亚复苏最早，相对占比升高。 预测全世界封锁中国到中国封锁全世界。 中国资本市场，会有一个小牛市。 我们需要一个牛市，预判破4000，甚至到4300. 局部牛市体现在科创板创业板。 做预测判断不是为了牟利。 是为了检验底层知识框架和对世界的理解和认识是否准确。 总体来看，方向大致正确，精确度有待提高。 2021年的预测 2021年，中美关系局部回暖，但是科技战，技术封锁持续。 中欧之间双边投资协定的签订和生效可能会有反复。 目前仅完成了草签。 部分国家议会可能难以支持。 中国内循环方面，解决卡脖子问题方面将会有多个好消息。 2021新冠疫苗生产效能竞争会成为热点。这里中国拥有优势。 2021全球多边主义有所回暖。 尤其是中美欧三大经济体讨论环保节能气候变化。 整治避税天堂。 2021美欧发达国家可能会使坏，怂恿重债发展中国家向中国减免债务。 面临外交压力。 人民币CFET指数仍热保持一定强势。 2021-01-13 美联储已经躺平乎？你我正在买单矣之前2020年三月专家会认为道指会掉到10000点之下。 但是却涨破30000点。 这就是无铆货币时代的印钱的魅力。 美联储量化宽松政策回顾和预测次贷危机之后 2008年9000亿 2014年4.2万亿 2019年8月中3.7万亿 2020年疫情之前4.2万亿 2020年12月7.2万亿 据说2022年下一位美联储主席可能是义务现代货币理论簇拥着的女性专家。 12现代货币理论认为，现代货币体系实际上是一种政府信用货币体系。主权货币不与任何商品与其他货币挂钩，只与未来税收债券相对应。 所以2025年拜登第一任期结束，美联储资产负债表可能是15-16万亿美元左右。 那么从2008到2025年年复合增速大约为18%（印钱速度，美元资产贬值速度）。 美元资产增值速度低于18%则是在为美国国民福利长治久安做贡献。 美联储资产负债表扩张，为什么美国没有严重通胀？第一大持有人是美联储，规模大于所有外国政府持有的总和。 两种通胀缓冲作用： 金融系统的冻结效应，弱化了信用创造过程，m1扩张但是m2、m3还没有扩张，推迟了通胀的表达。 美元作为全球储备货币，可以全球获得商品服务。通胀是全球性的。越外围通胀越高。在产能过剩，老龄化严重的经济体甚至可能出现通缩。 美元大放水流动性去哪里了？三个池子吸收流动性： 商品：可贸易品。全球产能过剩，美国也人口老化消费降低，所以价格不会涨得特别快。这个池子不会吸收太多流动性。 服务：增加理发餐饮价格，但是这些东西不可囤积。不能作为对抗通胀保值增值的手段。 资产：股市房地产债券。 美联储可以无限印钞而没有约束吗？理论上没有限制，只要全世界愿意用劳动换美元。 共和党强调小政府，控制赤字。 但是共和党总统债务扩张大幅加快。 现代货币理论学者，说债务大不是问题。 扩张没有边界，但是扩张速度还是有限制。 利息支出不能失控。 过去20多年美国国债存量巨幅增长，但是债务利息支出稳定。 美国国债利率一路降低。 如果未来通胀加剧，美联储不得不加息，导致美联邦财政借新还旧滚动存量债务利息翻倍。 全球和美国本土储蓄者敢不敢相信美元？ 哪些因素导致美国本土通胀率走高呢？（理论和政策研究的富矿，值得研究） 全球性防水带来的影响 操纵汇率。 美财政部将越南（操盘水平低）瑞士（货币受欢迎必须管控汇率）列为汇率操纵国。 中国连续三年是汇率操纵的警告对象。 资产价格飙升。 美国股市，房地产。数字货币。 全球资产配置者需要把小比例资金放入某种保险柜对冲。 老年人的保险柜（黄金），年轻人的保险柜（数字货币）。 小泡沫（黄金比特币）并不是大事。 汇率的波动。 美元指数年初100+到现在89。 美国贸易逆差大幅扩张。 中美金融体制重大差异中国m2和美国比较，中国GDP低但是m2大，不专业也不合理。 美国m2:m1+储蓄存款、小额定期、零售的货币市场基金 中国m2:m1+个人存款企业存款、人民币存款和外汇存款 美国m2不包含10w远以上的大额和外汇存款。美国m2统计口径小于中国。 真正可比的是m3 或者比较中国净主权信用货币（资产负债表-外汇占款），和美国的资产负债表。 中国信用增长率10.8%，美国18%。 中国不应该建立巨额的外汇储备。 2021-01-20 经济战该怎么打？拿破仑大陆封锁政策是最生动的反面教材！什么是经济战使用经济金融手段，打击削弱对方的实力和社会稳定性，改变对方政治政策和行为。 历史上： 法国拿破仑对英国的大陆封锁 1950苏伊士运河，美国对英法制裁，迫使从埃及撤兵。 特朗普政府： 技术禁运，华为等公司的制裁 人才回归审查 关税战（针对很多贸易伙伴），一般不能称为经济战，不是以政治为目的。 拿破仑的经济战军事成功，大战略层面失败。 打遍天下欧洲无敌手之后，发动对英国的经济战。 因为海军被摧毁，不能通过暴力摧毁英国。 拿破仑犯了很多战略错误： 对暴力管制迷信，对市场力量的轻视。管制内外形成巨大的价格落差。 不理解金融在国家间斗争的作用。没有把货币纳入考虑。 允许英镑在欧洲流通。 重商主义经济里面，拿破仑想把英国黄金挣完，使英国破产。 在金本位下，谁黄金多谁东西贵。 对自身的政治定位没有与时俱进。 控制欧洲之后仍然把法国利益作为首位。 中国古代经济战管仲，服帛降鲁。 倡导穿帛，但是不允许产丝，从鲁国进口丝，鲁国农民不种粮食了。 后来禁止穿帛，同时禁止粮食出口鲁国。 鲁国饥荒，通货膨胀，鲁王投降。 拿破仑应该低价出售粮食且高价收购英国棉布。 之后同时宣布对英国禁运粮食，对英国公司债作废。 如何经济战经济战不要想挣钱，是会赔钱的。 经济学家认为制裁要满足一致性原则、脆弱性原则（认为没有道理）。 经济战重要的不是压力的绝对值，而是压力的波动性。 受打击对象短期收到极大压力无法有效调整。 经济战要形成不利于对手，而操之在我的供求关系的巨幅波动。 2021-01-27 美元与黄金脱钩的50年：富国债务率在变高，而穷国外汇储备却变大自由主义右翼经济学家喜欢用宏观杠杆率来衡量经济体的健康程度。 中国宏观杠杆率高，变化太快。 探讨债务问题要和货币联系起来以di币为例。 用自己可以发行印制的钞票界定你我之间的债权债务关系，那我借的钱越多，我占你的便宜越大。 用黄金等自己不能印的货币借钱，哪怕借的少都是麻烦。 1971年之后债务情况富国债务率越来越高，穷国有越来越大外汇储备 毛轱辘的观点： 这个是民主制度中，政治家欺负两类没有选票的利益相关者。 外国人和后代。 美欧日这几十年的积累债务就是在欺负外国人和后代。 翟老师观点： 原因是金本位废除，主权债务的货币约束消失。 而不是民主制度。 1970年前美欧宏观杠杆率下行，之后持续上行。 1971年之前民主制度没有带来杠杆率上升。 黄金非货币化对世界运行方式的影响意义重大，影响深远。 1971年前，价值有黄金定义，像一个黄金铺满的平面，相对稳定。 1971年之后，就像一个美元铺满的球面，各国央行往里充气。 这个气是大国的主权债务（信用变成财富）。 个人和企业想挣到钱，宏观前提是全球主要大国愿意提供负债来支撑经济的信用。 华尔街的观点桥水基金Ray Dalio尝试用皇冠杠杆率解释金融市场经济波动国家兴衰。 量纲不对，GDP流量，债务存量 1971之前和之后混为一谈。技术分析的假设，过去现实未来内在一致性，这个假设不一定成立。 单因素论，尝试找到一个因素。 华尔街很多人把意识形态偏见当作定理科研学术。 追求深刻惊悚带节奏，不追求真理。 某些华尔街资产管理人和民科思想家说的和中国有关的东西可信度很低。 因为中国为大客户，他们要说一些讨喜或者耸人听闻的话。 就像“邹忌讽齐王纳谏”。 当今世界债务全球宏观风险宏观杠杆率逻辑关系，1980年以来： 发达国家债务率走高，发展中国家比较低。 哪些因素导致经济体债务率高不出问题，债务耐受度。 债务率高低和宏观风险无关，非本币计价的主权债务与宏观风险高度相关。 发展中国家用本币借不到债，导致发展中国家的原罪，金融体系中遭受某种先天制裁。 2021-02-03 建设后疫情时代的“一带一路”，输出我们强政府的经验与能力后疫情时代一带一路怎么搞。 面临一些变革的必要和升级换代的机遇。 民本主义政治经济学四组关系： 中心和外围：中心剥削，但是中心更重要，供养昂贵的高精尖项目 公共部门和私人部门的关系：公共部门提供公共产品，成本是综合税率 人和物的关系：财富不是物的堆积，人才是财富。人的生产消费创新才是财富的源头。 可贸易品和不可贸易品的关系：可贸易是财富的生产，不可贸易是财富的再分配。不可贸易品的定价区别来自于可贸易品的盈利能力。所以发展的时候是要发展可贸易品。 如何理解美国梦，第五个命题：我们目前处于主权信用的大泡沫中。 美国梦，他激动世界就繁荣，他低落世界就萧条。 中华民族伟大复兴，我们自己做梦。 一带一路背景下美国梦即将苏醒，中国将进入中心。 一般美国人民不愿意在进行全球化。 西方世界变老了变小气了，也消费不动这么多东西了。 不断印钱，不断稀释债权的游戏不可持续怎么办。 那我们和世界外围塑造一个新的泡泡，以前我们在别人的梦境，现在自己做梦。 中国人消费是全球最大市场。 旧泡泡我们是债主，但是用别人的货币定价。 新泡泡我们是加之基准的确定者，定价货币人民币。 外围国家为什么贫穷自由主义认为原因是缺少资本，但是其实不是，王公贵族很有钱。 原因在于当地缺少公共产品，因为没有强政府。 强政府指国家机器对社会的管控识别动员。 要帮他们发展起来，要输出如何如何建立强政府的经验和能力。 就是国家建设的现代化。 一带一路的目的是什么追逐的是一带一路年轻生命。 他们愿意消费但是没有钱。 我们借钱给他们，他们有了现金流购买我们的产品。 之后进一步投资借债，让这些年轻人进入我们的循环（但不是生活）。 多种族聚居会带来惨重教训（欧美，五胡乱华）。 一带一路三个倡议 多投资，少放债。用股权发展人民币计价的全球二级市场。 多搞集中投资，少搞分散投资（管控减弱）。 帮助他们获得可贸易部门的繁荣。设立特区，有主权没有治权，工业化新城，种族多元文化一元。塑造新中国人群体。 人类命运共同体实现过程就是漫长而曲折的。 2021-02-24 如何诱导国家间的合作与互信？推演博弈论在外交策略上的应用利用囚徒困境研究自私的人类何以可能合作和相互信任。 个人国家来讲什么处世之道是合理的。 囚徒困境关于囚徒困境： 什么是囚徒困境？ 两个囚徒是否招供。 纳什均衡。 人性假设，人是不是自私的？ 生物学来讲是自私的。保存基因。 但是也有例外，利他主义。蚂蚁蜜蜂，共享75%的基因 生活中的囚徒困境明知合作可以共赢，但是理性的自私信任缺乏带来背叛。 即使自己不贪心，但是难以相信对方不贪心。 熟人社会，互相之间是友善的。 是否存在最优的博弈策略在罗伯特·艾克斯洛德的实验中，他向各界征集了一些列博弈策略。 第一轮策略评比中，“tit for tat”（一报还一报）策略在实验中获得了第一。 第二轮评比中，告诉大家该策略最优，使大家改进。但是“tit for tat”策略仍旧获得了第一。 “tit for tat”： 第一步假设对方是好人，之后重复对方上一步的操作。 这个策略可以最有效的鼓励其他程序与自己长期合作。 这个简单的策略有以下品质： 善良。假设世界是对自己好的。 可激怒的。会产生报复。 宽容。如果对方弃恶从善可以原谅对方。 不嫉妒。不争取高于对手的分数。 建议阅读，《合作的进化》，被诺贝尔经济学家得主认为比其自身的研究更高明。 从博弈论中得到的人生信条 友善。 有原则。 宽容。 简单。 不妒忌朋友的成功。 一报还一报实操的问题朋友关系不一定是囚徒困境。 很多时候高成本低回报。 现实中很多时候回报的程度不确定。 如果双方都采用这个策略，误解可以导致以恶报恶。 利用圈子解决人际关系中的问题双边关系中的回报放到多边关系中进行操作，很多问题可以引刃而解。 用关系网分担了人心的自私带来的背叛的诱惑。 国与国之间，对外战略的启示多边主义外交，一个国家的背叛的潜在损失很大。 背负信用压力的大国，越是利用自己的信用，别人越敢相信。 中国礼让谦虚大度，捍卫原则根本利益，不羡慕别人的成功。 国与国之间应该把一次性的囚徒困境转化为重复博弈，诱导国与国之间的信任和合作，培育出信任与和平。","link":"/economy-finance/didongsheng/zjqd/"},{"title":"中国为什么有前途","text":"《中国为什么有前途》 翟东升老师 极简简介此处以两版的前言的节选来作简介。 第三版前言本书有2009年开始做了第一版，迄今为止已有第三版（2019年）。 本书的若干语言和预警都得以兑现： 2010年第一版提出逆全球化的风险 2015年第二版（撰写于2014年）预警美元强势周期导致人民币汇率下跌和资本外逃的恶性循环 第二版前言宏观来看，中国的国际地位提高。 但是微观来看，生活中的困境压的普通人喘不过气来。 看整体国势，中国好像很有前途；但是看自己身边，好像又问题很大。 写这本书是为了告诉读者，做一个21世纪的中国人，是艰辛却又令人骄傲的。 本书向读者指出一些基本事实和历史轨迹，包括： 世界历史演进的趋势 当代中国体制的特质 中国已经具备巨大潜能 个人看法极简评价这是一本全面的书，其以中国改革开放以来的政治经济为主线，涵盖货币，安全，投资，援助等方面，对认识中国当下经济政治政策的形成过程可以起到帮助作用。 这同样也是一本乐观的书，乐观地总结了中国从上世纪70年代以来的经济货币等政策变动，并乐观的展望了未来中国发展的趋势。 为何说这本书乐观？ 是因为其总能从正反两面考虑过去的政策，甚至是“不那么好的政策”，并提取出有价值的经验。 同时又通过分析历史和总结，在考虑到很多政策潜在弊端的情况下，对未来政策提出相对优良一些的改进或优化。 感想/看法本人从这本书收益良多，翟老师在自己的知识框架下对中国过去的走势和未来的发展做出了分析总结。 对我个人而言，由于之前知识体系尚未构建，所以对翟老师的观点以认同居多，或许将来在不断学习后，可以更加批判看待书中观点。 仅从书中总结的事实来看，我个人对中国近年来的发展持乐观态度。 可以从书中清晰地看到国内政策的调整与转变，尽管政策或许尚有不足，但是不断完善，向科学完备发展。 有一个明显有趣的点，就是可以看出翟老师的一些“双标”，总的来说就是我们察觉到了美国的货币霸权，不公平的优先发展权等，对其进行批判，但是我们的目标也是获得这一些权利。 还有就是一些政策在我们国家使用的时候可以明显看出利弊，但是宣传给其他国家使用相同政策时则强调好处。 这其实无可厚非，“人不为己，天诛地灭”，当然我还是更愿意相信当中国作为一个体系更中心的国家时，能更多的承担一些国际责任，可能要好于多数的民主大国。 在对外投资和援助上，我以前其实一直没搞清楚必要性重要性和利弊。 这里看来或许翟老师已经一一阐述，之后可以新开一个 post 来做一下总结。 但是总的来说，能说和不能说的好处都是存在的。 此时就有涉及到另一个也很有趣的话题——“师出有名”。 为自己的政策和潜在利益作出令人无法反驳的解释，还是很重要的。 还有一点就是这本书其实强化了一个我的认识。 就是金融的本质，或者说最重要的点，是为社会活动提供资金，尤其是优化资金配置。 以前我对这一认识并不清晰，觉得金融可能主要在于套利，或者保证市场有效（当然这个也很重要），所以一度觉得金融很虚，甚至有时会存在对其嗤之以鼻但又羡慕人家利润的看法。 尤其是之前觉得很多金融人士致力于杀猪盘割韭菜，更是让我这行业持轻视态度（可能是吃不到葡萄说葡萄酸）。 但是这本书在国家投资，主权基金，国企民企的陈述中，反复展现了优化资金的配置对于行业，对于国家的重要性，让我强化了金融这个行业负担起优化资本配置责任和义务的认知。 所以个人认为，金融，尤其是中国金融，脱虚向实还是很有必要的。 当然要能干的了实事，也得能抵御国外的金融攻击。 （外行人的见解。） 其实书中大部分内容，翟老师在政经启翟中都有提到。 这里只是在中国发展的维度进行展开。 但是总的来说跳不出“人本主义政治经济学”的框架。 本人虽然见识的社会比较少，但是对“人才是最重要的财富”深以为然。 最后以 Ray Dalio 最近说的一番话结束这番感想。 I encourage you to look at the trends and not misunderstand and over-focus on the wiggles. To understand what’s going on you need to understand that China is a state capitalist system which means that the state runs capitalism to serve the interests of most people and that policy makers won’t let the sensitivities of those in the capital markets and rich capitalists stand in the way of doing what they believe is best for the most people of the country. 按节讨论/摘录第一章：世界市场体系的中央与外围四类国家： 中央国家：依靠信心生活，美国 准中央国家：依靠理性生存，欧日 外围工业国家：依靠勤劳谋生，中国，印度，东亚，中东欧 原料提供国家：依靠运气生存，中东，拉美，非洲，东欧，中亚 四类国家间主要存在两种关系： 贸易关系：材料和产品的流动 融资关系：美元霸权 世界市场体系的基本特点： 无远弗届：很少有国家可以孤立于世界市场之外。 非常不公平的体系：中央国家比起外围国家更富有和稳定。 “中央国家掌握自己的货币政策，外围国家难以掌握自己的货币政策。”—索罗斯 铸币税：信用货币与真实价值；外围投资与美元贬值。 要素流动的选择性：不同要素流动程度不同，信息观念到商品服务到技术到人力资源流动程度递减。 自我强化功能：中心外围固化，强者恒强，弱者恒弱。（中短期效应） 周期的自我更新：伊利比亚半岛国家（葡萄牙和西班牙），荷兰，英国，美国 人口规模越来越大：当大家都掌握了新的财富与权势要素，规模（人口，幅员）重新变得重要。 短期内“劫贫济富”，长期“损有余而补不足”。 结构的鲁棒性： 利益分布不均衡但是博弈论意义均衡。 中央国家收益、代价和条件： 获得的收益： 提高人均财富拥有量。 提供对全球经济波动节奏的掌握和调控能力。 融资和负担转嫁能力。 语言、人才、政治自信、号召力。 获取和保有中央地位的条件： 货币霸权（参考如何成功开办和经营一家商业银行。） 资本实力 控制商业活动以自己货币进行 维系重要国家的关系获得支持 发行金融产品满足外围国家偏好 存款通过投资转化为资本 自己的跨国公司有明显经营优势 全球市场开放性 中央国家承担的成本： 产业外移，长期逆差，去工业化 控制战略地区的安全成本的 外围国家发展道路上的常见陷阱： 资源诅咒陷阱 欲速不达的赶超陷阱 发展过程中的政治陷阱 寥寥几个外围国家逆袭的成功者，都符合以下特征： 极度缺乏自然资源 人口增速适中 紧紧依附西方市场 政府对权力控制牢固 第二章：中国在体系中的足迹从人均角度来看，中国的比较优势不在于原材料而在于劳动力。 改革开放以来实现“三外路线”：对外贸易、引进外资（外国直接投资）、对外货币安排。 20世纪80年代，中国由外围原料出口国演变为外围工业国。 2019年中国即将从外围工业国转变为中央工业国的临界区域（地位等同欧日）。 “三外路线”的政策组合，形成了官方巨大的外汇储备： 人民币大幅贬值锚定美元 引进外资 鼓励出口 经常项目人民币可兑换 强制结售汇 “三外路线”的利弊： “三外路线”的利： 就业、工业化、资本积累、对改革的推动、和平。 “三外路线”的弊： 本土工业形成挤压、环境资源代价、对外部资本及市场依赖、地区部门阶层间分配失衡 对中央国家的实质性纳贡，外商利用外汇在中国收益10%以上的年华，而中国用外汇投资收益接近0。 不可持续性：中国生产美国消费，中国放贷美国借债的循环模式不可持续。 “三外路线”的修正： 科学发展观：以福利而不是 GDP 衡量政策成就。 外商回归国民待遇，招商引资到招商选资。 人民币渐进升值过程。 强调自主创新，培育自身多层次资本市场。 启动内需，减轻外部依赖。 对国有企业的扶持。 “三外路线”在中国何以实现： 工业化和现代化的历史使命：发挥比较优势，发展市场上有竞争力的产业，才能顺利实现工业化。 体制根源：国家权力彻底渗入动员和改造了中国社会，中央地方关系也同样重要。 思潮和意识形态因素：引入西方经济学，“科学发展观”。 要素禀赋，人口与资源：规模是竞争优势，也有着巨大的内需市场潜力。 “三外路线”兴衰对中国外交的影响： 20世纪90年代低调对外，“韬光养晦”。 2004年后，对西方依赖程度在逐步减轻，西方却更依赖中国。 2005年来，在联合国使用或声称使用否决权来迫使议案调整。 2012年以来，走出韬晦。 第三章：“三外路线”下的对外贸易 即将到来的中国对外产业转移，需要我们思考这样一个问题： 如何发展出一种独特的产业链编辑能力，以便尽可能按照我们的政治经济利益塑造地区乃至全球性的地缘经济和地缘政治格局。 中国对外贸易发展的轨迹： 外贸惊人增长： 贸易收支平衡方面持续顺差 外贸总量大幅升高和出口总量全球第一 外贸结构改善： 商品：原材料到劳动/资源密集型工业制成品到高新技术出口品 贸易性质：加工贸易比例先上升后下降。2005年之后，落后外资撤出，加工贸易下降，长期来看仍会下降。 出口目的地：从出口中央国家到出口目的地多元化。对外围地区的出口商由中资企业为主。 出口企业性质：外资比例稳步回落。 中国对外贸易发展的动能： 观念的变迁： 从使用苏联模式到接受西方自由主义经济学并成为主流。 进口替代型发展战略到出口导向型（不强调建立自己的完整产业体系），但是未极端化而是渐进融合。 比较优势和“竞争优势”。 制度的变迁： 外贸管理体制市场化、国际化、法律化。 企业内部治理模式改革。 外贸经营权放开和扩大。 鼓励出口的政策组合： 外汇留成制度和人民币贬值 出口退税政策促进出口 国际体系环境： 日本产业转移，四小龙诞生 美国限制进口增速 中国招商引资 所以对美出口绕行到中韩台地区。 贸易、产业与地缘政治经济回答此章开头的问题。 对外贸易是否可以塑造一个有力的地缘政治经济环境？ 基本思路在于培育“产业链编辑能力”。 所谓产业链编辑能力（capacity of industrial chain editing, CICE），是指大国依靠资深市场规模以及对某些关键性生产要素的掌控而获得的一种特殊能力，借此可以在一定程度上按照自身国家利益的需要来主动调整地区性的甚至全球性的产业地理分布，以便从国家间不对称的相互依赖中获得优势和权利。 今天，经济成长，吐故纳新，顺比较优势，我们之前的劳动力成本和环境成本不应该是优势了。 用国内消费取代外需和投资拉动经济成长的中国经济转型，既是应对全球金融危机和经济衰退的必要举措，也是建设国内和谐社会和可持续发展的内在要求。 所以产能向哪里转移？怎么转移？ 可以推动他国市场内在趋势加速，但是不能改变内在趋势。（斯里兰卡、缅甸、巴基斯坦、孟加拉转移纺织化工。） 集中力量深度介入少数经济体的经济建设和国家发展。 对周边单个国家依赖最小化，并让他们对我们依赖最大化。 中国已有的确保有效的编辑产业链的资源： 部分产业的控制权 技术资本积累 越来越大的本土市场规模 政府高调控管治能力 巨大国家资本积累 未来国际大国的国际信用 新问题：当替他大国也拥有上述资源和条件时，并试图运用这些资源实现与我们目标抵牾的地缘经济政治构造时，会发生什么情况？如何应对？ 第四章：外商直接投资与中国的经济安全招商引资与缺口论：中国为什么要吸引外资？ 资金缺口论：缺钱。这是没有解释力的，不论是外汇还是储蓄。 技术缺口论：资金本身不是资本，只有资金与特定行业的技术、管理、营销能力结合在一起的时候才成为资本。但是仍没有解释力，技术并未转移。 制度缺口轮：中国对民营资本的歧视，而对外资无影响，形成外资对内资明显的竞争优势。 中国吸引外资并非政策成就，而是中国经济体制缺陷的症状表现。 渐变的外资政策： 地缘上：点到线到面 行业上：出口加工到一般制造到服务业 引资方式：合资、合作、外资企业等方式 引资政策背后导向：进口替代到鼓励出口到促进产业结构升级和调整 外资的贡献： 协助中国转变在全球分工中的角色。 帮助缓解就业和大城市挑战。 帮助积累外汇。 提升中国的技术能力和产业层次。 竞争效应 示范模仿效应 联系效应 培训效应 关于外资的争论：“恐外症”“崇外症”及其本质本质是带有非理性的思维特征，对中国企业中国人的特质与潜能缺乏最起码的信心。 谈论这个问题首先要搞清楚：“谁是外资”、“什么是国家经济安全”、“什么是垄断”。 恐外症： 中国外资依存度过高（考虑实际使用外资规模总额/GDP） 我们只算了直接投资，算上间接投资的话，依赖程度并不高 压低人民币价格情况下计算的GDP是有问题的。 有根据的统计数据是外资的对外出口占中国出口量的比重。 “斩首策略”：独资企业或者控股合资，进入市场时，将中方品牌束之高阁取而代之。 市场的首席裁判员应该是价格而不是政治，被斩首说明产业出了问题。 诉诸情感和道德而不是理性和法理。 崇外症： 外资的优越性和价值被捧得太高，反过来支撑着这个优势。 对外资迷信的背后，是部分人不愿正视历史和现实，不愿向事实低头，不愿承认中国民营资本的实力和潜力。 外资政策的未来趋势及政治和战略潜力未来的调整方向： 取消外资超国民待遇 将国内市场吸引力而不是廉价要素作为吸引外国直接投资的核心竞争力：中国从全球生产基地转变为一个全球最大市场。 积极参与甚至推动国际多边投资规则的制定。 平衡的吸收外国直接投资和间接投资资金。 产业升级： 产业升级难以靠外资实现，高价值就业机会是世界各国政府努力竞争、想尽办法留在本国的东西，在国际市场上本来就稀缺。 政府及其资本触角在推动中国产业赶超和升级过程中扮演了很重要的角色，但是不同行业中政策绩效存在明显差别。 案例中的教益： 面向家庭个人的消费品，向民营资本开放越早越彻底，创新能力越强。国企改革适合扮演财务投资者角色，从管资产到管资本。 中国最大的优势在于中国的市场规模。我们应该把产业发展的希望主要寄托在别国国民和家庭（而非政府）能主导的行业中。 政府官员要相信本国私人企业家的潜能和潜力，要意识到自己能力有限。政府需要做的是意识到哪些产业出现问题，哪些地方资源配置效率低，并为生产力提升打开空间。一个好政府的主要任务是，确保没有人能不劳而获，让人民都劳有所得。 第五章：人民币汇率于人民币国际化汇率的政治经济学一般观点：货币即是权力，即使是和平时期，金融业也是受制于政治需要的。 不可能三角： 资本流动、固定汇率、货币政策独立性三者不能同时成立。 固定汇率制度下财政政策有效，浮动汇率政策下货币政策有效。 不同立场的汇率偏好： 出口海外投资型偏好汇率稳定，依靠国内市场的则更希望货币政策独立。 生产贸易品部门偏好固定汇率，生产非贸易品（金融业）部门偏爱浮动汇率。 中右翼政党更厌恶通货膨胀，左翼更关心失业率。 执政稳定政党汇率偏向弹性和高估（长远经济目标低通胀），执政不稳政党偏向干涉本币维持低估。 人民币汇率波动后的政治人民币汇率制度的改革： 1981-1984：盯住一揽子货币，双轨制，汇率下跌，扶持出口。 1985-1993：双轨外汇制度，管理浮动，汇率持续下跌，恶化国内通胀。 1994-2005：盯住美元，汇率长期稳定，促进经济发展。但是前期合理后期僵化，入世后应做调整。 2005-2013：盯住一揽子货币浮动管理，目标维持出口和就业稳定减少顺差，对美元稳定升值，导致套利行为，助长本土资本泡沫。 2013之后：参照一揽子货币双向波动，从汇率干预中逐步淡出，目标从被全球流动性绑架的货币政策中解放出来，波动性扩大但是保持强势和稳定，有助于打击套利，热钱流入减少。 对应的政治： 1972-1980：人民币高估不利于出口，贬值又会在非贸易项目吃亏，于是双轨制，贸易非贸易采用不同汇率。 1985之后：中央地方博弈，政府企业博弈，官方汇率和市场汇率并存。 1997 亚洲金融危机宣布不贬值，考虑国际形象和香港金融的稳定，同时当时外汇储备高，中央财政资源增强。这一行为奠定了中国在争取东亚地区领导地位的国家信用基础。 1998年之后盯住美元，不愿意恢复弹性，是在当下结构性失业背景下提出的。 2005年之后发现汇率低估不利于产业升级，所以开始稳步升值。 人民币国际化 一个经济强国的货币在实现了自由兑换后，被其他国家接受，成为国际支付和处置手段，我们称这种货币为国际货币。 人民币国际化的利弊衡量： 好处：铸币税、节约外汇储备成本、降低汇率风险、扩大贸易投资、可以对外转移宏观经济风险。 负面代价和风险： 低端产业挤出效应。 经济泡沫化风险（吸引巨量游资）。 放大汇率波动（大量流入流出）。 货币政策会收到海外存量货币的影响。 人民币国际化并不难，向外“送钱”即可。 难的是让它的负面代价最小化。 日本国际化就是我们的反面教训。 人民币国际化的机遇： 2008年之后全球货币结构的动荡。 中国积累了足够的经济实力，且有良好的工业基础。 美欧亚并驾齐驱，但是在亚洲日元难堪重任。 人民币国际化的进展： 2012年人民币国际化指数0.56到15年3.91，18年4.84，在震荡中增长。 政府管制方式重大变化，顺应市场需求政革放权。 2013开始跨境结算支付规模上升，2017年下降至2012年来最低水平。 涉外部门统计和管理中人民币计价。 海外人民币清算行。 人民币跨境支付系统 CIPS。 人民币国际化和资本项目放开的政策辩论央行支持主张放开。 一部分经济学家，包括社科院北大的学者提出批判质疑，反对放开。 两者展开了辩论： 从利弊上看： 支持者认为放开的好处： 更多融投资机会，优化资源配置 创造非政府部门资本流出途径，避免实体经济硬着陆 倒逼国内改革 反对者的焦虑： 资金外流对国内产业负面冲击 从开放和改革的顺序上看 央行认为可以同步审慎进行 一部分学者认为先改革再开放 开放倒逼改革的理念 反对者认为以外促内风险性高，历史上不乏失败案例 要不要公布开放时间表 反对者认为给出开放时间表不如给出推动国内结构性改革时间表 支持者认为时间表有必要。因为开放的实际利益受损者是权力部门，让受损者制定开放的方案推动起来会很难 辩论的三个特点： 对风险的认知和态度存在很大差异。 双方都是列举归纳，缺乏整体主义视角。不完全归纳无法说服对方。需要有整体视角，用演绎法而不是归纳法。 双方对“应然”和“实然”的侧重。 国际化的条件和战略阶段： 区域性国际化，东亚与部分发展中国家展开。 成为国际金融活动媒介和国际金融资产。可以使用美元当年“先挂钩后脱钩”的策略来获取信心。 步骤： 国内多层次资本市场和银行体系改革 升级产业结构和提高技术能力 全球资产投资和并购渠道的建设 第六章：外汇管理政策与外汇储备外汇储备实际上是替外资保管的金银细软。 但是也是一种前所未有的对外政策工具，政治上实现了“金融恐怖平衡”，大大弥补了中国整体国力尤其是军事能力方面的弱势。 外汇管理制度的改革与储备的形成 外汇储备：当局（不包含民间）能够有效控制并可随时动用的对外资产。 中国的外汇储备是在“强制结售汇政策”，“银行外汇周转头寸限制”，“盯住美元汇率政策”三位一体的制度安排下形成的。 以10%的复合收益率估算，我们外汇储备规模远远小于外资在中国经济体内部的资产积累。 之后外资撤出大量挤兑时怎么办？ 市场有风险，买卖自愿，人民币可以接受一定程度的本币贬值。 外汇储备的经济含义 为什么要积累巨额外汇储备？ 底部贸易赤字、保证偿付的外汇需求、维护汇率稳定、灾难时的战略储备。 到底应该如何看待中国的外汇储备？ 外汇储备的高速不平衡增长源于国内金融体系的功能缺陷，效率低下，无法将储蓄转化为有效投资。 只要我国国内金融体系与国际金融体系存在较大的效率差异，我国就只能通过这种“体外”资本循环方式支持国内经济增长。 导致了巨大的机会成本 中国积累多少外汇储备合适？外储如何实现保值升值？ 维持正常的需求，7000亿之内。 国债投资转为优质企业股权投资（例如淡马锡、阿布扎比投资局、挪威养老基金）。 如何控制外汇储备的过度增长？ 改善国内金融市场，民间金融合法化 拓宽对外直接投资的空间 加大人民币汇率弹性，减少游资套利空间 外汇储备与“中国-美国”中国明知持有的美元存在贬值倾向，但是不能卖出，因为如果卖出美元，那么美元将出现恐慌性贬值，导致自己手中剩余的美元资产缩水。 中国所能做的就是与美国政府谈判，警告他们不要做出背叛的行为，否则两败俱伤。 金融恐怖平衡：我们（美国）依赖的是他国不对美国赤字融资所需偿付的代价，（也即）他国一旦停止融资需要偿付的代价（是如此之大），确保了他国将继续融资。 中美之间数十年相对成熟稳定的主要原因是因为存在“核恐怖平衡”和“金融恐怖平衡”。 平衡中中国处于相对弱势。 随着中国新世纪出口多元化，对美依赖减轻，谈判地位强弱之势悄然异位。 中国意识到这种关系不可持续，所以逐步从“三外路线”循环中脱离。 中美关系少了重要的稳定器，如何维持一个较低成本的可持续和平环境？ 并且全球经常项目和资本项目顺差在中国周边大规模聚集是存在安全和政治前提的，就是东亚和平与稳定。 由于美国目前仍然主导着东亚地区的和平与安全格局，那就潜在的出现了一个巨大的套利机会：跳起某个地区性的冲突并放手升级之，结果将是东亚资本的极速外逃，美国的融资问题一夜间获得解决，尽管这是一种杀鸡取卵的不可持续的解决方法。 储备多元化及其政策后果中国持有大量美元，但其实是美元的空头，而不是多头。 华尔街跟庄，中国外汇储备增大时，他们就做空美元做多欧元，减小时就相反。 那常识为什么错了？ 外管局结售汇过程中收入的资产重新配置为美元资产和非美元资产。 就是买入外汇资金然后配置。 买入端美元比例极高（例如80:20）（由于美元可获得性高），然后配置时比例（50:50），其实是在卖出美元，买入非美货币。 所以当资产配置在美元上绝对数目多时，其实已经售出了大量的美元。 作为美元的空头，中国外汇储备面临何种风险？ 当然是美元上升周期。 外汇储备名义价值下降，欧日元资产贬值。 金融系统投入矿业和原油的信贷将面临风险。 不能陷入被逼空的状态。 美元的上行一般由以下因素驱动： 美国经济领先于日欧复苏 失业率下跌到6.5%以下 美国能源独立导致逆差减少 中国因素（空头力量其实在衰减） 被全面逼空的前提条件： 美元超级强势周期 巨额资本逃离 出现新制造业大国 国内通胀高企，人民币没有贬值空间 中国产业转型面临失败 第七章：中国对外资本输出中国正在从一个（产业）资本净输入国变成净输出国。 中国企业“走出去”中国企业“走出去”对提升巩固中国在世界市场体系中的地位具有重大意义。 政府也改革投资体制，鼓励本地企业国际直接投资。 “走出去”的动机： 获得市场 获得外部生产要素：能源、原材料、技术、研发能力、品牌 往返程投资：打扮成外资回归 “走出去”的问题和挑战： 国有企业的体制问题：监管不到位容易贪污和资本外逃 民营企业融资担保和保险问题 企业和个人要在行为方式上严格约束自己，要合规合法道德 企业对外直接投资的政策体系和服务体系不完善 自主品牌国际推广过程为中国国际公关提供政策杠杆： 成为西方媒体的大客户，则会拥有对他们施加压力的重要杠杆，他们在报道中国的时候会有所顾忌。 中国资本输出的现状和未来输出的现状： 1978-2000：建立经济特区吸引资本 2001-2012：加入 WTO 引入输出并重，央企海外并购 2013-至今：输出为主，兼顾引入 输出的主体： 国有企业为主：效率低下，动机模糊，意识形态上被怀疑 民营企业很少：天赋惊人，家族地缘为基础信用网络融资，但是目光短视 战略方向和地缘空间： 金砖体系、一带一路 西南西北产业聚集带 西部大开发升级 第八章：国际投资规则的制定权争夺主权财富基金的发展及其投资规则之争主权财富基金是一种政府金融投资工具。 与外汇储备不同，它偏好更高收益率。 从资金来源看，主权财富基金可以分为三类： 贸易外汇盈余 资源出口外汇盈余 国际援助 2007年开始，西方政界开始担心主权投资中钱的所有者之意图和潜能，担心出于政治和战略目的而不是商业盈利目的的大规模买卖。 所以下方一直主张制定一些“国际制度”来约束我们。 西方国家一方面想要这个资金，另一方面又害怕金融核武器。 目前来看，国际新规者制定主导权基本掌握在美欧手中。 但是其实美欧之间，各大经济体内部也存在分歧。 但是仍有一些共同诉求： 通过非正式国际立法来约束和规范主权基金，并将规则的制定权和监督过程掌握在美欧控制的 IMF 和 OECD 手中。 从道义上贬低、法律上禁止实践中防范各国主权基金出于政治与安全动机的投资行为。 要求主权基金提高透明度。 在此前提下，维持各国资本市场的开放。 于是美欧采取了一些明确的策略和措施： 以美欧主导的多边经济组织作为工具，制定规则，拉拢引诱世界各国接受作为未来谈判的基础。 制造国际舆论并炒作形成国际共识 分而治之，压迫小国主权基金满足规范，塑造国际惯例。 我们在贸易、裁军、气候变迁等各种全球治理问题谈判中，已经反复得到如下经验和教训： 参与到国际规则的制定中是维护国家利益的第一步，也往往是最关键的一步。 中方立场： 鼓励而不是禁止出于政治目的的投资 提高透明度应该是在坚持联系原则和自主原则两大基本前提下进行。 维持美欧资本市场开放下是对主权基金东道国的合理补偿，也是维持全球金融经济秩序稳定可持续的基本前提。 当越来越多的资产和股权被划为非卖品的时候，他们所支撑的货币将会不可挽回的越来越疲软。 当前谈判地位和可选策略当前逆来顺受，被动妥协。 措施和策略： 搞一个功能性同盟，把有共同利益的国家拉到一起，成立一个新的投资者俱乐部。团结有大量资本的依附性小国。 扶持急需外来投资的中小国，塑造我们的国际惯例。 自主研讨会，探讨全球主权基金投资规则。 假如能团结几大主权基金，则在谈判桌上能制定有利于资方的规则。 国际投资法与中国的选择世界范围投资法的发展： 多边国际投资法尝试，但是普遍失败 双边投资条约的发展 中国处于一个吸收投资和资本输出的双重身份中。 但是大背景是投资规模会超过吸纳投资： 所以中国新世纪以来的投资协定，采纳了类似美式范本的高标准投资保护机制。 第九章：超越“能源安全”不论是从人均还是未来前景看，中国都缺能源。 国际能源炒家以能源安全剥削中国。 但是高油价也许并非坏事，中国进口量远小于体系中央国家。 如果油价使中央财富流向边缘，那么可以通过对边缘国家的出口把钱赚回来。 所以要跳出有人营造的恐慌情绪，跳出那些直接的局部的利益得失，才能冷静的全面辩证看待能源在中国国家战略中的意义。 能源问题的若干基本常识 能源生产消费在地域分布上严重失衡。 能源结构走向多元化，化石能源仍是主体。 能源是世界上贸易规模最大的大宗商品。 能源问题被高度政治化。 中国的能源安全什么是能源安全？ 狭义：可以安全供应，是可以充分稳定经济的获取发展所需要的能源。 中国的能源安全可以分解为四个环节： 能源供给安全：有时是非卖品，所以需要同时使用市场和外交两种手段 能源价格安全：并不是越廉价越好 能源运输安全：防范被截断 能源消费安全：环境气候危害 中国的能源缺口是常见的夸大之处，其实缺口不大，有时甚至供大于求。 能源安全运输也被夸大，马六甲海峡其实可以替代。 高油价利于中国崛起 为了中国整体国家利益的最大化，中国在适当的时候有必要托起石油价格。 较高油价可以维持有利于中国的全球战略平衡。 我们希望确保美国在任何时候都无法集中足够的力量、意志和盟友资源对华实施战略摊牌。 能源价格越高，中东地区财政力量越强，越有能力为美国及其盟友制造麻烦。 如果能源价格暴跌，很多国家包括俄罗斯都会陷入财政困境。 托举油价在经济上也是合算的 油价维持较高位置，有利于中国扩大对外围体系国家出口投资，将托市成本赚回来。 适度托举油价有利于提升中国在国际能源市场上的定价权。 拥有巨额石油储备之后，可以避免逼空。 熨平能源价格波动是大国的国际责任 帮助发展中国家应对能源诅咒 我们的战略储备基地已经竣工 第十章：原材料市场中国的定价权原材料市场的三个关键问题 市场结构与定价权。 即便我们在生产规模和莫呕血消费规模上是最大的，但是如果市场分散无序，在国际市场上仍不会有定价权。 所以对国内市场整合，用国内的联合协作对付国际上跨国公司的联合垄断，才能扭转地位。 资源配置到底应该以价格还是权力为杠杆？ 以价格为杠杆可以让整体福利和效率最大化。 但是国际市场非充分开放。 中国应该用现实主义确保自己的安全利益，再用理想主义去推动开放。 资源能源与金融和货币关系。 一个国家在拥有国际影响力的大宗交易产品市场，那么该国在争夺相关商品全球定价权时便获得了不可小视的技术型便利。 开放经济环境下的中国粮食安全粮食具有作为重要战略资源的政治经济特性。 其需求刚性生产周期性，价格弹性小。 当前分工：外围国家从美国及其盟友进口粮食，自己则专门生产经济作物。 当年管仲所用的战略模式，今天在体系大国和外围小国之间悄悄重演。 对中国人来说，中国人什么都吃，是这个民族历史上所受苦难的遗迹和明证。 目前来看，从供求关系来看，中国粮食自给有余，基本平衡；从战略态势来看，中国是在勉强防守，态势堪忧。 当前的平衡能维持，是因为中央政府比较富裕。 如果不能维持，则要内部深度挖掘潜力，等待人口总量下降。 但是未来三十年，我们通过内部挖潜维持平衡的难度将越来越大： 人民币汇率上涨，粮食美元计价成本高，维持粮食产量则需要增大补贴（杠杆成倍放大补贴）。 工业化城市化，精耕细作失去其劳动力基础。 水资源匮乏限制粮食增长。 对于全球粮食格局的三个基本判断： 全球人口增加将为粮食安全带来巨大挑战，为美国国家权势反弹埋下伏笔。 世界不缺耕地，也不缺潜能，缺的是耕地与高科技农业劳动力的错配。 粮食安全不能单纯依赖市场，必须有大国出来平抑粮价。 中国未来粮食安全的长期解决出路是大规模农业资本输出： 政府出面包租购买土地。出资本管理技术，粮食按比例分成。 合同由政府签，经营由市场吸引国内农业企业承包。 控制中方农技人员数量与东道国结合。 中方企业粮食分成由中国官方投资机构保价收购，返销国内，或者在其他地区建立储备库用于平抑波动。 国内建立更大规模粮食储备能力。 疯狂的石头：铁矿石进口及其谈判乌克兰中国铁矿石储量大，但是贫矿多富矿少，含铁量低。 从铁产量上来看，澳大利亚和巴西是最主要的两个铁矿石生产大国。 世界最大的铁矿石厂商位于巴西的淡水河谷，86亿吨高品位铁矿石储量。 铁矿石主要进口方是中日韩欧。 在铁矿石价格谈判中，中方2003年开始参与。 宝钢作为代表参与谈判。 宝钢在谈判中没有考虑到其他钢企的利益。 2009年宝钢退出谈判，中钢协接手。 他们站在维护国家整体利益的立场谈判，“宁可谈判破裂也不妥协”，但是对于企业实际需求和市场谈判经验不足。 2010年，宝钢重新占据谈判主导权。 2012年初，铁矿石现货交易平台启动。 2012年以来进口地区经济增长放缓，铁矿石产量却屡创新高，价格走低，转变为买方市场。 2014年，我国开始涉足铁矿石期货的金融服务，旨在推动人名币计价、清算和结算铁矿石，是金融市场的创新，也满足实体经济套期保值的需求。 中国决策者意识到美元存款的不可靠和矿藏价格低迷的机遇。 于是鼓励国企央企走出国门收购海外矿山资源。 2009中国铝业收购力拓赔了夫人又折兵。 五矿收购 OZ Minerals 旗下核心资产则获得良好收益。 有专家认为，如果要改善目前处境则需要在内部贸易和公平性下功夫，改变国内钢企一盘散沙的局面。 也有学者提出取消钢铁产品出口退税。 稀土我国稀土储量占全球71.1%，产量95%。 却不能像石油和铁矿一样控制全球价格，这是因为企业互相杀价，导致价格低位运行。 （其实国外也有大量的未开采稀土矿，因为其相对于中国稀土不经济所以仍未开采。） 中国政府试图实现稀土行业全行业整合。 2009年对我国储量产量第一的三种矿保护性开采。 并鼓励企业走出去收购国际重要稀土矿资源。 对保护性开采，西方国家强烈反对。 但是这并没有违背国际贸易准则，且符合国际惯例。 联合国《建立新的国际秩序宣言》： 每个国家对自己的自然资源和一切经济活动拥有充分的永久主权。为了保卫这些资源，每个国家都有权采取适合自己的手段，对本国资源及其开发实行有效控制…… 任何一国都不应遭受经济、政治或其他形式的协迫，以致不能自由地和充分地行使这一不容剥夺的权利。 中国稀土政策的若干选项： 提高关税，控制出口配额：有利于内外价差，和新能源产业的先手。 对矿企征收高额环境税和资源税：利益留在中国，抬高全球价格，但是没有保护国内科技企业利益。 稀土金融化、货币化、储备化（像黄金一样）：财富增值，但是降低其使用价值。 第十一章：中国的对外援助中国援外的历史和现实 新中国成立后30年： 中国对外经济技术援助八项原则 大力援外并不是过度承担国际义务，影响自身发展，而要与当时冷战的背景结合理解。（但是这里也没说怎么理解。） 第一代领导人，世界革命的背景 谋势，用较小成本获得国家声望 “三外路线”下的援外 随着人民币贬值，受援国感到援助力度减小 自身经济扩张，援助占本国财政份额下降 第二代领导人，优先本国经济发展 无偿援助变贴息贷款，大型项目变中小型项目等 谋利，不求改变体系而是融入体系 援外工作新高潮 为了确保能源原材料供给，重新活跃 与西方国家口惠而不实形成对比 强调双赢 美欧认为中国援助： 涉及一些对专制政权的援助 不干涉内政不利于改善全球治理水平 利用援助做为争夺原材料的辅助手段 很少雇佣当地人，不利于当地经济发展 构建中国特色援外理论自利利人的审慎道德原则：重塑对外援助的政治伦理基础 援助宣传中的道德主义是一种陷阱，大家的援助都被诟病的原因在于，大家普遍使用很高的道德标准来要求和评价援助行为。 援助和意识形态的捆绑，无益于国家间的稳定和良性互动。 国家的对外援助不是慈善活动，而是理性的广义的国家利益拓展的需要。 审慎性道德准则：用人的标准来要求而不是神的标准来要求人，不通过损害他人以自利。 审慎道德标准低调可行可持续。 中国援助行为的特点：心理上平等，事理上互利。 中国人明白：对于落后者而言，尊严往往比五斗米更值得珍惜。 在符合审慎道德原则下，国际援助的必要性： 增进国家人民间信任友谊和认同，利于体系持久和平。 有殖民原罪的国家，有义务提供援助：假如先辈犯罪而获得好处至今仍为当代人享用，那么赎罪是援助不单是消除负罪感，更是一种政治和道德的必须。 全球市场中心的大国，发达国家从风险收益不对称获利巨大，代价由外围国家承担，补贴外围国家是平衡体系内生的不公平。 贫困和混乱会溢出到其他国家和地区，影响富国的长期利益。 干涉与良治：援助中的主权问题： 用财力逼他人就范，实际也是一种专制 任何人手中都不拥有绝对真理 在一国经济技术水平和社会结构非常落后情况下，强求发展民主可能是拔苗助长的做法。 援助者不应随意对他国家庭内部事务发表见解。 改进中国的援外政策 重视人的作用 争取当地民心，不能只惠政府 重视人员交流，服务民生 改革援外决策和管理机制 缺乏协调统一的高于各部委的决策部门 随意性较大缺乏论证 权利竞争，责任推诿 未纳入法制化轨道 企业和非政府组织的高效对外援助还有很大空间 应该争取本国民众理解 集中援助树“典型” “撒胡椒面”难以产生强大的初始动能 评估上来说如果该国更依赖中国市场而不是中国政府，则援助成功 对其他国家对华政策产生诱导作用 安哥拉模式就是很好的例子 援外和资本输出结合 形成合力，良性循环 滚动发展，使对外援助在中国内部具有经济效益上的可持续 这种援助某种意义是一种投资 在和美元脱钩后，我们需要使对外援助在中国对外经济关系中具有不可替代性，要让其真实成本降低。 援助形成的友好氛围，基础设施可以为投资创造条件和抬高收益率 土地矿权股权有获得资本增值机会 金融危机救援，救急不救穷，别人做不了的事情往往最有利可图 改进对援外事物的宣传 拿出少量美元储备以贷款方式援助是出于对国家利益的整体长远考虑 民众认为政府拿着本来属于自己的财富白送给别人，这反映出： 货币知识的缺失：美元不能用在国内，不用掉只能躺在美元债券，不能支付国内贫困学生的学费或者为苦难群众造房子。 民生问题关注度高 宣传技巧有进一步提升空间 第十二章：结论：中国对外经济关系的潜能和风险关于两组关系的探讨 国家与市场间的关系 国家很重要，但是如果光靠国家控制，没有市场来配置资源，那么经济必然缺乏活力，最终国家力量也会衰竭。市场的确很高效，但是离开了国家力量的规范和约束，市场秩序和市场结构难以自我维持。 为了在全球市场获取超额利润，内部非市场化往往是一种必须，有选择有分寸的搞些政府管制，尽管会不可避免的导致资源配置效率损失，但是由于这种管制获得了外部博弈的优势，那么超额利润可能足够弥补内部损失。 中国与体系间的关系 哪个国家越能够全面彻底有效地把人类整合到全球性的交易分工和要素流动中来，就越能得到“天命”的资助。（世界市场体系对各地区各文明的吞噬消化和整合。） 我们将用什么办法来实现比美国时代更加全面深入有效的整合全球的经济要素？ 我们如何在主导权更替的过程中尽可能实现和平禅让而不是物理决斗？ “三外路线”是否可以向外围国家推广 \b“三外路线”的普适性： 体系外围国家：资本缺乏，工业技术能力落后，层次较低，人才匮乏，制度落后，国内治理混乱。互为因果互相牵制。 三外路线使中国一下子摆脱了资本匮乏，也让其他方面逐步改善。同时快虚积累外汇储备，增强了抵御系统性风险的能力。 三位路线可以看成一种融资模式，向他国（产业资本）融资，向未来融资，从而在特定地域和时间点上集中足够密集的资本规模，实现发展的突破，开启发展的正循环。 \b“三外路线”的推广： 向一部分发展中国家推荐，对中国有以下好处：加速产业升级和对外转移趋势，减少美欧贸易顺差但是不会损减整体福利，出口数量损失换来的是质量提升，在国家间形成不对称依赖，符合人民币国际化内在需求。 美欧应该不会反对此路线 适用性前提： 资源富集劳动力匮乏就不适用 要求政府对社会有很强的渗透控制能力 出口加工工业的地理要求 民族文化对勤劳节俭的鼓励（较容易改变，例如70年代被认为懒惰，90年代勤奋。） 竟然制度变迁可以大幅度改造中国民众的生活方式，为什么对非洲或者其他地区人民持悲观态度呢？ 时机问题，目前中国资源密集型产业向外转移 中国对外经济关系战略潜能 中国已积累了罕见的经济实力，但是执政者还不善于用这种实力寻求国际抱负。 对各章探讨的对外经济关系各个方面潜能加以总结： 参与国际经济分工时，\b可以使用自身各方面优势塑造周边中小国对中国的不对称依赖。 集中了世界最大的外汇官方储备可以通过耐心大胆巧妙的长期投资把手中外汇变成世界主要跨国企业的控制权，形成网络化的非正式权力。 推动人民币国际化，修正货币格局，让东亚在世界经济体系获得更有利的位置，并对美国国际权势基础形成瓦解作用。 中国企业走出去创建品牌的时候，中国的外宣系统应当加以支持和利用，获得世界各地主流媒体的影响力。 借助自身资金和市场规模，应该有选择的调控全球基础原材料的价格，拥有部分定价权。 对外援助和资本输出结合起来，在体系外围国家扮演金融和货币危机救援者的角色，形成中国援助品牌。 这些构想的实现需要中国的决策和执行体系做必要的改革，尤其是将对外经济事务同涉外政治事务之间的制度性藩篱消除掉。 比较现实的政策建议，通过边际性改革促进对外战略和对外经济政策之间实现若干功能性的结合。 对外经济关系的风险2010-2020: 货币政策与泡沫化风险： 这里不是说价格高得让人难以理解就是泡沫，而是符合某种特点的价格运动过程。（自我增长自我毁灭的循环。） 外资的投资行为，造成了人民币升值的事实，又使外资“理性决策”，继续投资。 全球化退潮风险 对外投资效率风险 国有企业体制缺陷，对人激励约束不充分 民营企业规模小，资本有限 2020-2030: 逆全球化纵深发展，中美经济持续脱钩，并各自组团将全球市场再次变成相互平行而竞争的两个体系。 美国国内的债务周期将迎来巨幅调整阶段。 2021年前后巨额的债券到期，借新还旧不可持续成本提高，除非大幅降息。 一带一路面临的挑战，没有很好的回答以下问题： 发展中国家缺什么发展不起来？ 可能并不是钱，而是现代主权国家治理能力和治理体系。 我们通过一带一路究竟在追求什么要素？ 不是能源资源，甚至也不是劳动力，而是巨量年轻消费者。 当发展中国家还不起债的时候，我们如何追求回报？","link":"/reading-note/china-bold-future/"}],"tags":[{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"search","slug":"search","link":"/tags/search/"},{"name":"logic","slug":"logic","link":"/tags/logic/"},{"name":"cs-seminar","slug":"cs-seminar","link":"/tags/cs-seminar/"},{"name":"sorting","slug":"sorting","link":"/tags/sorting/"},{"name":"algorithms","slug":"algorithms","link":"/tags/algorithms/"},{"name":"interesting","slug":"interesting","link":"/tags/interesting/"},{"name":"感悟总结","slug":"感悟总结","link":"/tags/%E6%84%9F%E6%82%9F%E6%80%BB%E7%BB%93/"},{"name":"neural-network","slug":"neural-network","link":"/tags/neural-network/"},{"name":"machine-learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"attention","slug":"attention","link":"/tags/attention/"},{"name":"deep-learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"high-performance-computing","slug":"high-performance-computing","link":"/tags/high-performance-computing/"},{"name":"contrastive-learning","slug":"contrastive-learning","link":"/tags/contrastive-learning/"},{"name":"self-supervised-learning","slug":"self-supervised-learning","link":"/tags/self-supervised-learning/"},{"name":"loss","slug":"loss","link":"/tags/loss/"},{"name":"entropy","slug":"entropy","link":"/tags/entropy/"},{"name":"info-theory","slug":"info-theory","link":"/tags/info-theory/"},{"name":"loss-function","slug":"loss-function","link":"/tags/loss-function/"},{"name":"gaussian-process","slug":"gaussian-process","link":"/tags/gaussian-process/"},{"name":"theory","slug":"theory","link":"/tags/theory/"},{"name":"gradient","slug":"gradient","link":"/tags/gradient/"},{"name":"rnn","slug":"rnn","link":"/tags/rnn/"},{"name":"lstm","slug":"lstm","link":"/tags/lstm/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"few-shot-learning","slug":"few-shot-learning","link":"/tags/few-shot-learning/"},{"name":"reinforcement-learning","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"imitation-learning","slug":"imitation-learning","link":"/tags/imitation-learning/"},{"name":"distribution","slug":"distribution","link":"/tags/distribution/"},{"name":"KL-divergence","slug":"KL-divergence","link":"/tags/KL-divergence/"},{"name":"JS-divergence","slug":"JS-divergence","link":"/tags/JS-divergence/"},{"name":"active-learning","slug":"active-learning","link":"/tags/active-learning/"},{"name":"survey","slug":"survey","link":"/tags/survey/"},{"name":"EA","slug":"EA","link":"/tags/EA/"},{"name":"Cuda","slug":"Cuda","link":"/tags/Cuda/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"latex","slug":"latex","link":"/tags/latex/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"Matplotlib","slug":"Matplotlib","link":"/tags/Matplotlib/"},{"name":"Conda","slug":"Conda","link":"/tags/Conda/"},{"name":"Pytorch","slug":"Pytorch","link":"/tags/Pytorch/"},{"name":"numpy","slug":"numpy","link":"/tags/numpy/"},{"name":"Vim","slug":"Vim","link":"/tags/Vim/"},{"name":"转载","slug":"转载","link":"/tags/%E8%BD%AC%E8%BD%BD/"},{"name":"读书","slug":"读书","link":"/tags/%E8%AF%BB%E4%B9%A6/"},{"name":"历史","slug":"历史","link":"/tags/%E5%8E%86%E5%8F%B2/"},{"name":"Research","slug":"Research","link":"/tags/Research/"},{"name":"翟东升","slug":"翟东升","link":"/tags/%E7%BF%9F%E4%B8%9C%E5%8D%87/"},{"name":"辩论","slug":"辩论","link":"/tags/%E8%BE%A9%E8%AE%BA/"},{"name":"Icarus","slug":"Icarus","link":"/tags/Icarus/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"Master Ma","slug":"Master-Ma","link":"/tags/Master-Ma/"},{"name":"cloud","slug":"cloud","link":"/tags/cloud/"},{"name":"MacOS","slug":"MacOS","link":"/tags/MacOS/"},{"name":"Hackintosh","slug":"Hackintosh","link":"/tags/Hackintosh/"},{"name":"Ray-Dalio","slug":"Ray-Dalio","link":"/tags/Ray-Dalio/"}],"categories":[{"name":"Artificial Intelligence","slug":"Artificial-Intelligence","link":"/categories/Artificial-Intelligence/"},{"name":"Computer Science and Engineering","slug":"computer-science-engineering","link":"/categories/computer-science-engineering/"},{"name":"Interesting Stuff","slug":"Interesting-Stuff","link":"/categories/Interesting-Stuff/"},{"name":"Knowledge from Growth","slug":"knowledge-from-growth","link":"/categories/knowledge-from-growth/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"},{"name":"Math","slug":"Math","link":"/categories/Math/"},{"name":"Paper Reading","slug":"paper-reading","link":"/categories/paper-reading/"},{"name":"Programming","slug":"programming","link":"/categories/programming/"},{"name":"Reading Note","slug":"reading-note","link":"/categories/reading-note/"},{"name":"Research","slug":"research","link":"/categories/research/"},{"name":"Economy &amp; Finance","slug":"economy-finance","link":"/categories/economy-finance/"},{"name":"Software Tools","slug":"software-tools","link":"/categories/software-tools/"}]}